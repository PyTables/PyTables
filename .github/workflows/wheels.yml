name: Wheels

concurrency:
  group: ${{ github.workflow }}-${{ github.event.number }}-${{ github.event.ref }}
  cancel-in-progress: true

# Publish when a (published) GitHub Release is created.
on:
  push:
    tags:
      - v*
  release:
    types:
      - published
  # TODO:
  # The scheduled version of this job should really download and install NumPy
  # from scientific-python-nightly-wheels
  schedule:
    - cron:  '12 13 * * 0'
  # Allow manual triggering of the workflow (which will release the wheels to nightly)
  workflow_dispatch:

permissions:
  contents: read

jobs:
  build_sdist:
    name: Build source distribution
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
        with:
          fetch-depth: 0
          submodules: true

      - uses: actions/setup-python@a309ff8b426b58ec0e2a45f0f869d46889d02405 # v6.2.0
        name: Install Python
        with:
          python-version: 3.x

      - name: Install APT packages
        if: ${{ contains(matrix.os, 'ubuntu') }}
        run: |
          sudo apt update
          # Keep in sync with "Prerequisites" in User's Guide.
          sudo apt install libbz2-dev libhdf5-serial-dev
          sudo apt install latexmk texlive-fonts-recommended texlive-latex-recommended texlive-latex-extra texlive-plain-generic

      - name: Install dependencies
        run: |
          # Keep in sync with ``build-system.requires`` in ``pyproject.toml``.
          python -m pip install --require-hashes -r .github/workflows/requirements/build-requirements.txt
          python -m pip install --require-hashes -r requirements.txt
          python -m pip install --require-hashes -r requirements-docs.txt

      - name: Build dist (sdist and docs)
        run: make PYTHON=python dist

      - uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          path: dist/*
          name: dist

  build_wheels:
    name: Build ${{ matrix.os }} wheels
    runs-on: ${{ matrix.os }}
    env:
      # Keep in sync with "Prerequisites" in User's Guide.
      HDF5_VERSION: 1.14.6  # H5Dchunk_iter needs at least 1.14.1
      MACOSX_DEPLOYMENT_TARGET: "10.9"
    strategy:
      fail-fast: false
      matrix:
        os: ['ubuntu-latest', 'ubuntu-24.04-arm', 'macos-latest', 'windows-latest', 'macos-15-intel']

    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
        with:
          fetch-depth: 0
          submodules: true

      - uses: actions/setup-python@a309ff8b426b58ec0e2a45f0f869d46889d02405 # v6.2.0
        name: Install Python on Unix
        with:
          python-version: '3.14'
        if: runner.os != 'Windows'

      - name: Install Python and dependencies on Windows
        uses: mamba-org/setup-micromamba@add3a49764cedee8ee24e82dfde87f5bc2914462 # v2.0.7
        with:
          environment-name: build
          create-args: >
            python=3.14 blosc c-blosc2 bzip2 hdf5 lz4 snappy zstd zlib pkgconfig
          init-shell: bash powershell
        if: runner.os == 'Windows'

      - uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3
        id: deps-cache
        with:
          path: hdf5_build
          # Instead of a hashFiles('**/wheels.yml') appended here, just use a counter.
          # It allows us to modify this file without having to constantly rebuild HDF5
          key: ${{ matrix.os }}-native-deps-cache-${{ hashFiles('**/get_hdf5.sh') }}-${{ env.HDF5_VERSION }}
        if: runner.os != 'Windows'

      - name: Build dependencies
        env:
          CFLAGS: -g0
          HDF5_DIR: ${{ github.workspace }}/hdf5_build
          MACOSX_DEPLOYMENT_TARGET: "10.9"
        if: runner.os != 'Windows' && steps.deps-cache.outputs.cache-hit != 'true'
        run: |
          mkdir hdf5_build
          if [[ "${{ runner.os }}" = 'Linux' ]]; then
            ARCH=$(uname -m)
            docker run --rm -e HDF5_DIR=/io/hdf5_build -e CFLAGS="$CFLAGS" -e HDF5_VERSION="$HDF5_VERSION" -v `pwd`:/io:rw quay.io/pypa/manylinux2014_${ARCH} /io/ci/github/get_hdf5.sh
          else
            HDF5_DIR=`pwd`/hdf5_build ci/github/get_hdf5.sh
          fi

      - name: Install patchelf
        if: runner.os != 'Windows'
        run: |
          python -m pip install patchelf

      - name: Install cibuildwheel
        run: |
          python -m pip install --require-hashes -r ./.github/workflows/requirements/wheels-requirements.txt

      - name: Build wheels
        run: |
          python -m cibuildwheel --output-dir wheelhouse

      - name: Copy requirements.txt
        run: |
          cp requirements.txt ./wheelhouse/

      - uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          path: |
            ./wheelhouse/*.whl
            ./wheelhouse/*.txt
          name: cibw-wheels-${{ matrix.os }}-${{ strategy.job-index }}

  # TODO someday add a run that downloads the artifact and tests them (e.g., on a new
  # 3.13 or 3.14 install), but abi3audit should in theory be sufficient

  twine_check:
    needs: [ build_sdist, build_wheels ]
    name: Twine check
    runs-on: 'ubuntu-latest'

    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
        with:
          fetch-depth: 0
          submodules: true

      - uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7.0.0
        with:
          pattern: '*'
          merge-multiple: true
          path: wheelhouse

      - name: List downloaded artifacts
        run: ls -alR ./wheelhouse/

      - name: Install Python
        uses: actions/setup-python@a309ff8b426b58ec0e2a45f0f869d46889d02405 # v6.2.0
        with:
          python-version: '3.x'

      - name: Install patchelf
        if: runner.os != 'Windows'
        run: |
          python -m pip install patchelf

      - name: Install twine
        run: |
          python -m pip install --require-hashes -r ./.github/workflows/requirements/wheels-requirements.txt

      - name: Check sdist and wheels
        run: |
          python -m twine check *.whl tables-*.tar.gz
        working-directory: wheelhouse

      - name: Upload wheel
        uses: scientific-python/upload-nightly-action@5748273c71e2d8d3a61f3a11a16421c8954f9ecf # 0.6.3
        if: github.repository == 'PyTables/PyTables' && (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch')
        with:
          artifacts_path: wheelhouse
          anaconda_nightly_upload_token: ${{secrets.ANACONDA_ORG_UPLOAD_TOKEN}}
