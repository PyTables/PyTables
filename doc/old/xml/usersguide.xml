<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<book lang="en" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title><literal>PyTables</literal> User's Guide</title>

  <subtitle>Hierarchical datasets in Python - Release
  <xi:include href="../../VERSION" parse="text"/>
  </subtitle>

  <bookinfo>
    <authorgroup>
      <author>
        <firstname>Francesc</firstname>

        <surname>Alted</surname>
      </author>

      <author>
        <firstname>Ivan</firstname>

        <surname>Vilata</surname>
      </author>

      <author>
        <firstname>Scott</firstname>

        <surname>Prater</surname>
      </author>

      <author>
        <firstname>Vicent</firstname>

        <surname>Mas</surname>
      </author>

      <author>
        <firstname>Tom</firstname>

        <surname>Hedley</surname>
      </author>

      <author>
        <firstname>Antonio</firstname>

        <surname>Valentino</surname>
      </author>

      <author>
        <firstname>Jeffrey</firstname>

        <surname>Whitaker</surname>
      </author>
    </authorgroup>

    <pubdate>$LastChangedDate$
    </pubdate>

    <mediaobject>
      <imageobject>
        <imagedata align="center" fileref="pytables-pro.svg" format="SVG" />
      </imageobject>
    </mediaobject>

    <copyright>
      <year>2002, 2003, 2004</year>

      <holder>Francesc Alted</holder>
    </copyright>

    <copyright>
      <year>2005, 2006, 2007</year>

      <holder>Cárabos Coop. V.</holder>
    </copyright>

    <copyright>
      <year>2008, 2009, 2010</year>

      <holder>Francesc Alted</holder>
    </copyright>

    <copyright>
      <year>2011</year>

      <holder>PyTables maintainers</holder>
    </copyright>

    <legalnotice>
      <para><emphasis role="bold">Copyright Notice and Statement for
      <literal>PyTables</literal> User's Guide.
      </emphasis></para>

      <para>Redistribution and use in source and binary forms, with or without
      modification, are permitted provided that the following conditions are
      met:</para>

      <para>a. Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.</para>

      <para>b. Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following disclaimer
      in the documentation and/or other materials provided with the
      distribution.</para>

      <para>c. Neither the name of Francesc Alted nor the names of its
      contributors may be used to endorse or promote products derived from
      this software without specific prior written permission.</para>

      <para>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
      CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
      BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
      FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
      COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
      INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
      NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF
      USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
      ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
      (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
      THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</para>
    </legalnotice>
  </bookinfo>

  <part label="I">
    <title>The PyTables Core Library</title>

    <chapter>
      <title>Introduction</title>

      <epigraph>
        <attribution>Gabriel García Márquez, <citetitle>A wise Catalan in
        "Cien años de soledad"</citetitle></attribution>

        <literallayout>
      La sabiduría no vale la pena si no es posible servirse de ella para
      inventar una nueva manera de preparar los garbanzos.

      [Wisdom isn't worth anything if you can't use it to come up with a new
      way to cook garbanzos.]
      </literallayout>
      </epigraph>

      <para></para>

      <para>The goal of PyTables is to enable the end user to manipulate
      easily data <emphasis>tables</emphasis> and <emphasis>array</emphasis>
      objects in a hierarchical structure. The foundation of the underlying
      hierarchical data organization is the excellent <literal>HDF5</literal>
      library (see <biblioref linkend="HDFWhatIs" />).</para>

      <para>It should be noted that this package is not intended to serve as a
      complete wrapper for the entire HDF5 API, but only to provide a
      flexible, <emphasis>very pythonic</emphasis> tool to deal with
      (arbitrarily) large amounts of data (typically bigger than available
      memory) in tables and arrays organized in a hierarchical and persistent
      disk storage structure.</para>

      <para>A table is defined as a collection of records whose values are
      stored in <emphasis>fixed-length</emphasis> fields. All records have the
      same structure and all values in each field have the same <emphasis>data
      type</emphasis>. The terms <emphasis>fixed-length</emphasis> and strict
      <emphasis>data types</emphasis> may seem to be a strange requirement for
      an interpreted language like Python, but they serve a useful function if
      the goal is to save very large quantities of data (such as is generated
      by many data acquisition systems, Internet services or scientific
      applications, for example) in an efficient manner that reduces demand on
      CPU time and I/O.</para>

      <para>In order to emulate in Python records mapped to HDF5 C structs
      PyTables implements a special class so as to easily define all its
      fields and other properties. PyTables also provides a powerful interface
      to mine data in tables. Records in tables are also known in the HDF5
      naming scheme as <emphasis>compound</emphasis> data types.</para>

      <para>For example, you can define arbitrary tables in Python simply by
      declaring a class with named fields and type information, such as in the
      following example:</para>

      <programlisting width="72">class Particle(IsDescription):
    name      = StringCol(16)   # 16-character String
    idnumber  = Int64Col()      # signed 64-bit integer
    ADCcount  = UInt16Col()     # unsigned short integer
    TDCcount  = UInt8Col()      # unsigned byte
    grid_i    = Int32Col()      # integer
    grid_j    = Int32Col()      # integer
    class Properties(IsDescription):  # A sub-structure (nested data-type)
        pressure = Float32Col(shape=(2,3)) # 2-D float array (single-precision)
        energy   = Float64Col(shape=(2,3,4)) # 3-D float array (double-precision)</programlisting>

      <para>You then pass this class to the table constructor, fill its rows
      with your values, and save (arbitrarily large) collections of them to a
      file for persistent storage. After that, the data can be retrieved and
      post-processed quite easily with PyTables or even with another HDF5
      application (in C, Fortran, Java or whatever language that provides a
      library to interface with HDF5).</para>

      <para>Other important entities in PyTables are
      <emphasis>array</emphasis> objects, which are analogous to tables with
      the difference that all of their components are homogeneous. They come
      in different flavors, like <emphasis>generic</emphasis> (they provide a
      quick and fast way to deal with for numerical arrays),
      <emphasis>enlargeable</emphasis> (arrays can be extended along a single
      dimension) and <emphasis>variable length</emphasis> (each row in the
      array can have a different number of elements).</para>

      <para>The next section describes the most interesting capabilities of
      PyTables.</para>

      <section>
        <title>Main Features</title>

        <para>PyTables takes advantage of the object orientation and
        introspection capabilities offered by Python, the powerful data
        management features of HDF5, and NumPy's flexibility and Numexpr's
        high-performance manipulation of large sets of objects organized in a
        grid-like fashion to provide these features:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis>Support for table entities:</emphasis> You can
            tailor your data adding or deleting records in your tables. Large
            numbers of rows (up to 2**63, much more than will fit into memory)
            are supported as well.</para>
          </listitem>

          <listitem>
            <para><emphasis>Multidimensional and nested table
            cells:</emphasis> You can declare a column to consist of values
            having any number of dimensions besides scalars, which is the only
            dimensionality allowed by the majority of relational databases.
            You can even declare columns that are made of other columns (of
            different types).</para>
          </listitem>

          <listitem>
            <para><emphasis>Indexing support for columns of tables:</emphasis>
            Very useful if you have large tables and you want to quickly look
            up for values in columns satisfying some criteria.</para>
          </listitem>

          <listitem>
            <para><emphasis>Support for numerical arrays:</emphasis>
            <literal>NumPy</literal> (see <biblioref linkend="NumPy" />)
            arrays can be used as a useful complement of tables to store
            homogeneous data.</para>
          </listitem>

          <listitem>
            <para><emphasis>Enlargeable arrays:</emphasis> You can add new
            elements to existing arrays on disk in any dimension you want (but
            only one). Besides, you are able to access just a slice of your
            datasets by using the powerful extended slicing mechanism, without
            need to load all your complete dataset in memory.</para>
          </listitem>

          <listitem>
            <para><emphasis>Variable length arrays:</emphasis> The number of
            elements in these arrays can vary from row to row. This provides a
            lot of flexibility when dealing with complex data.</para>
          </listitem>

          <listitem>
            <para><emphasis>Supports a hierarchical data model:</emphasis>
            Allows the user to clearly structure all data. PyTables builds up
            an <emphasis>object tree</emphasis> in memory that replicates the
            underlying file data structure. Access to objects in the file is
            achieved by walking through and manipulating this object tree.
            Besides, this object tree is built in a lazy way, for efficiency
            purposes.</para>
          </listitem>

          <listitem>
            <para><emphasis>User defined metadata:</emphasis> Besides
            supporting system metadata (like the number of rows of a table,
            shape, flavor, etc.) the user may specify arbitrary metadata (as
            for example, room temperature, or protocol for IP traffic that was
            collected) that complement the meaning of actual data.</para>
          </listitem>

          <listitem>
            <para><emphasis>Ability to read/modify generic HDF5
            files:</emphasis> PyTables can access a wide range of objects in
            generic HDF5 files, like compound type datasets (that can be
            mapped to <literal>Table</literal> objects), homogeneous datasets
            (that can be mapped to <literal>Array</literal> objects) or
            variable length record datasets (that can be mapped to
            <literal>VLArray</literal> objects). Besides, if a dataset is not
            supported, it will be mapped to a special
            <literal>UnImplemented</literal> class (see <xref
            linkend="UnImplementedClassDescr" xrefstyle="select: label" />),
            that will let the user see that the data is there, although it
            will be unreachable (still, you will be able to access the
            attributes and some metadata in the dataset). With that, PyTables
            probably can access and <emphasis>modify</emphasis> most of the
            HDF5 files out there.</para>
          </listitem>

          <listitem>
            <para><emphasis>Data compression:</emphasis> Supports data
            compression (using the <emphasis>Zlib</emphasis>,
            <emphasis>LZO</emphasis>, <emphasis>bzip2</emphasis>
            and <emphasis>Blosc</emphasis> compression libraries) out of the
            box. This is important when you have repetitive data patterns and
            don't want to spend time searching for an optimized way to store
            them (saving you time spent analyzing your data
            organization).</para>
          </listitem>

          <listitem>
            <para><emphasis>High performance I/O:</emphasis> On modern systems
            storing large amounts of data, tables and array objects can be
            read and written at a speed only limited by the performance of the
            underlying I/O subsystem. Moreover, if your data is compressible,
            even that limit is surmountable!</para>
          </listitem>

          <listitem>
            <para><emphasis>Support of files bigger than 2 GB:</emphasis>
            PyTables automatically inherits this capability from the
            underlying HDF5 library (assuming your platform supports the C
            long long integer, or, on Windows, __int64).</para>
          </listitem>

          <listitem>
            <para><emphasis>Architecture-independent:</emphasis> PyTables has
            been carefully coded (as HDF5 itself) with
            little-endian/big-endian byte ordering issues in mind. So, you can
            write a file on a big-endian machine (like a Sparc or MIPS) and
            read it on other little-endian machine (like an Intel or Alpha)
            without problems. In addition, it has been tested successfully
            with 64 bit platforms (Intel-64, AMD-64, PowerPC-G5, MIPS,
            UltraSparc) using code generated with 64 bit aware
            compilers.</para>
          </listitem>
        </itemizedlist>
      </section>

      <section id="ObjectTreeSection">
        <title>The Object Tree</title>

        <para>The hierarchical model of the underlying HDF5 library allows
        PyTables to manage tables and arrays in a tree-like structure. In
        order to achieve this, an <emphasis>object tree</emphasis> entity is
        <emphasis>dynamically</emphasis> created imitating the HDF5 structure
        on disk. The HDF5 objects are read by walking through this object
        tree. You can get a good picture of what kind of data is kept in the
        object by examining the <emphasis>metadata</emphasis> nodes.</para>

        <para>The different nodes in the object tree are instances of PyTables
        classes. There are several types of classes, but the most important
        ones are the <literal>Node</literal>, <literal>Group</literal> and
        <literal>Leaf</literal> classes. All nodes in a PyTables tree are
        instances of the <literal>Node</literal> class. The
        <literal>Group</literal> and <literal>Leaf</literal> classes are
        descendants of <literal>Node</literal>. <literal>Group</literal>
        instances (referred to as <emphasis>groups</emphasis> from now on) are
        a grouping structure containing instances of zero or more groups or
        leaves, together with supplementary metadata. <literal>Leaf</literal>
        instances (referred to as <emphasis>leaves</emphasis>) are containers
        for actual data and can not contain further groups or leaves. The
        <literal>Table</literal>, <literal>Array</literal>,
        <literal>CArray</literal>, <literal>EArray</literal>,
        <literal>VLArray</literal> and <literal>UnImplemented</literal>
        classes are descendants of <literal>Leaf</literal>, and inherit all
        its properties.</para>

        <para>Working with groups and leaves is similar in many ways to
        working with directories and files on a Unix filesystem, i.e. a node
        (file or directory) is always a <emphasis>child</emphasis> of one and
        only one group (directory), its <emphasis>parent group</emphasis>
        <footnote>
            <para>PyTables does not support hard links – for the
            moment.</para>
          </footnote>. Inside of that group, the node is accessed by its
        <emphasis>name</emphasis>. As is the case with Unix directories and
        files, objects in the object tree are often referenced by giving their
        full (absolute) path names. In PyTables this full path can be
        specified either as string (such as
        <literal>'/subgroup2/table3'</literal>, using <literal>/</literal> as
        a parent/child separator) or as a complete object path written in a
        format known as the <emphasis>natural name</emphasis> schema (such as
        <literal>file.root.subgroup2.table3</literal>).</para>

        <para>Support for <emphasis>natural naming</emphasis> is a key aspect
        of PyTables. It means that the names of instance variables of the node
        objects are the same as the names of its children<footnote>
            <para>I got this simple but powerful idea from the excellent
            <literal>Objectify</literal> module by David Mertz (see <biblioref
            linkend="Objectify" />)</para>
          </footnote>. This is very <emphasis>Pythonic</emphasis> and
        intuitive in many cases. Check the tutorial <xref
        linkend="readingAndSelectingUsage" xrefstyle="select: label" /> for
        usage examples.</para>

        <para>You should also be aware that not all the data present in a file
        is loaded into the object tree. The <emphasis>metadata</emphasis>
        (i.e. special data that describes the structure of the actual data) is
        loaded only when the user want to access to it (see later). Moreover,
        the actual data is not read until she request it (by calling a method
        on a particular node). Using the object tree (the metadata) you can
        retrieve information about the objects on disk such as table names,
        titles, column names, data types in columns, numbers of rows, or, in
        the case of arrays, their shapes, typecodes, etc. You can also search
        through the tree for specific kinds of data then read it and process
        it. In a certain sense, you can think of PyTables as a tool that
        applies the same introspection capabilities of Python objects to large
        amounts of data in persistent storage.</para>

        <para>It is worth noting that PyTables sports a <emphasis>metadata
        cache system</emphasis> that loads nodes <emphasis>lazily</emphasis>
        (i.e. on-demand), and unloads nodes that have not been used for some
        time (following a <emphasis>Least Recently Used</emphasis> schema). It
        is important to stress out that the nodes enter the cache after they
        have been unreferenced (in the sense of Python reference counting),
        and that they can be revived (by referencing them again) directly from
        the cache without performing the de-serialization process from
        disk. This feature allows dealing with files with large hierarchies
        very quickly and with low memory consumption, while retaining all the
        powerful browsing capabilities of the previous implementation of the
        object tree. See <biblioref linkend="NewObjectTreeCacheRef" /> for
        more facts about the advantages introduced by this new metadata cache
        system.</para>

        <para>To better understand the dynamic nature of this object tree
        entity, let's start with a sample PyTables script (which you can find
        in <literal>examples/objecttree.py</literal>) to create an HDF5
        file:</para>

        <screen>from tables import *

class Particle(IsDescription):
    identity = StringCol(itemsize=22, dflt=" ", pos=0)  # character String
    idnumber = Int16Col(dflt=1, pos = 1)  # short integer
    speed    = Float32Col(dflt=1, pos = 1)  # single-precision

# Open a file in "w"rite mode
fileh = openFile("objecttree.h5", mode = "w")
# Get the HDF5 root group
root = fileh.root

# Create the groups:
group1 = fileh.createGroup(root, "group1")
group2 = fileh.createGroup(root, "group2")

# Now, create an array in root group
array1 = fileh.createArray(root, "array1", ["string", "array"], "String array")
# Create 2 new tables in group1
table1 = fileh.createTable(group1, "table1", Particle)
table2 = fileh.createTable("/group2", "table2", Particle)
# Create the last table in group2
array2 = fileh.createArray("/group1", "array2", [1,2,3,4])

# Now, fill the tables:
for table in (table1, table2):
    # Get the record object associated with the table:
    row = table.row
    # Fill the table with 10 records
    for i in xrange(10):
        # First, assign the values to the Particle record
        row['identity']  = 'This is particle: %2d' % (i)
        row['idnumber'] = i
        row['speed']  = i * 2.
        # This injects the Record values
        row.append()

    # Flush the table buffers
    table.flush()

# Finally, close the file (this also will flush all the remaining buffers!)
fileh.close()</screen>

        <para>This small program creates a simple HDF5 file called
        <literal>objecttree.h5</literal> with the structure that appears in
        <xref linkend="objecttree-h5" xrefstyle="select: label" />
        <footnote>
            <para>We have used ViTables (see <biblioref
            linkend="ViTablesRef" />) in order to create this snapshot.</para>
        </footnote>
        . When the file is created, the metadata in the object tree is updated
        in memory while the actual data is saved to disk. When you close the
        file the object tree is no longer available. However, when you reopen
        this file the object tree will be reconstructed in memory from the
        metadata on disk (this is done in a lazy way, in order to load only
        the objects that are required by the user), allowing you to work with
        it in exactly the same way as when you originally created it.

        <figure id="objecttree-h5">
          <title>An HDF5 example with 2 subgroups, 2 tables and 1
          array.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center" fileref="objecttree-h5.png"
                         format="PNG" scale="75" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center" fileref="objecttree-h5.png"
                         format="PNG" scale="100" />
            </imageobject>
          </mediaobject>
        </figure>

        In <xref linkend="objecttree" xrefstyle="select: label" /> you
        can see an example of the object tree created when the above
        <literal>objecttree.h5</literal> file is read (in fact, such an object
        tree is always created when reading any supported generic HDF5 file).
        It is worthwhile to take your time to understand it
        <footnote>
            <para>Bear in mind, however, that this diagram is
            <emphasis>not</emphasis> a standard UML class diagram; it is
            rather meant to show the connections between the PyTables objects
            and some of its most important attributes and methods.</para>
        </footnote>
        . It will help you understand the relationships of in-memory PyTables
        objects.

        <figure id="objecttree">
          <title>A PyTables object tree example.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center" fileref="objecttree.svg" format="SVG"
                        scale="40"/>
            </imageobject>

            <imageobject role="html">
              <imagedata align="center" fileref="objecttree.png" format="PNG"
                         scale="100" />
            </imageobject>
          </mediaobject>
        </figure>
        </para>
      </section>
    </chapter>

    <chapter>
      <title>Installation</title>

      <epigraph>
        <attribution>Albert Einstein</attribution>

        <literallayout>
      Make things as simple as possible, but not any simpler.
      </literallayout>
      </epigraph>

      <para></para>

      <para>The Python <literal>Distutils</literal> are used to build and
      install PyTables, so it is fairly simple to get the application up and
      running. If you want to install the package from sources you can go on
      reading to the next section.</para>

      <para>However, if you are running Windows and want to install
      precompiled binaries, you can jump straight to <xref
      linkend="binaryInstallationDescr" xrefstyle="select: label" />. In
      addition, binary packages are available for many different Linux
      distributions, MacOSX and other Unices.  Just check the package
      repository for your preferred operating system.</para>

      <section id="sourceInstallationDescr">
        <title>Installation from source</title>

        <para>These instructions are for both Unix/MacOS X and Windows
        systems. If you are using Windows, it is assumed that you have a
        recent version of <literal>MS Visual C++</literal> compiler installed.
        A <literal>GCC</literal> compiler is assumed for Unix, but other
        compilers should work as well.</para>

        <para>Extensions in PyTables have been developed in Cython (see
        <biblioref linkend="Cython" />) and the C language. You can rebuild
        everything from scratch if you have Cython installed, but this is not
        necessary, as the Cython compiled source is included in the source
        distribution.</para>

        <para>To compile PyTables you will need a recent version of Python,
        the HDF5 (C flavor) library from <ulink url="http://hdfgroup.org/"/>,
        and the <literal>NumPy</literal> (see <biblioref linkend="NumPy" />)
        and <literal>Numexpr</literal> (see <biblioref linkend="Numexpr" />)
        packages. Although you won't need <literal>numarray</literal> (see
        <biblioref linkend="Numarray" />) or <literal>Numeric</literal> (see
        <biblioref linkend="Numeric" />) in order to compile PyTables, they
        are supported; you only need a reasonably recent version of them
        (&gt;= 1.5.2 for numarray and &gt;= 24.2 for Numeric) if you plan on
        using them in your applications. If you already have
        <literal>numarray</literal> and/or <literal>Numeric</literal>
        installed, the test driver module will detect them and will run the
        tests for <literal>numarray</literal> and/or
        <literal>Numeric</literal> automatically.</para>

        <warning>
          <para>The use of <literal>numarray</literal> and
          <literal>Numeric</literal> in PyTables is now deprecated.
          Support for these packages will be dropped in future versions.
          </para>
        </warning>

        <section id="PrerequisitesSourceDescr">
          <title>Prerequisites</title>

          <para>First, make sure that you have at least Python 2.4, HDF5
          1.6.10, NumPy 1.4.1, Numexpr 1.4.1 and Cython 0.13 or higher
          installed (for testing purposes, we are using HDF5 1.6.10/1.8.5,
          NumPy 1.5 and Numexpr 1.4.1 currently). If you don't, fetch and
          install them before proceeding.</para>

          <para>Compile and install these packages (but see <xref
          linkend="prerequisitesBinInst" xrefstyle="select: label" /> for
          instructions on how to install precompiled binaries if you are not
          willing to compile the prerequisites on Windows systems).</para>

          <para>For compression (and possibly improved performance), you will
          need to install the <literal>Zlib</literal> (see
          <biblioref linkend="zlibRef" />), which is also required by HDF5 as
          well. You may also optionally install the
          excellent <literal>LZO</literal> compression library (see
          <biblioref linkend="lzoRef" /> and <xref linkend="compressionIssues"
          xrefstyle="select: label" />). The high-performance bzip2
          compression library can also be used with PyTables (see
          <biblioref linkend="bzip2Ref" />).  The <literal>Blosc</literal>
          (see <biblioref linkend="BloscRef" />) compression library is
          embedded in PyTables, so you don't need to install it
          separately.</para>

          <variablelist>
            <varlistentry>
              <term><emphasis role="bold">Unix</emphasis></term>

              <listitem>
                <para><literal>setup.py</literal> will detect
                <literal>HDF5</literal>,
                <literal>LZO</literal>, or <literal>bzip2</literal> libraries
                and include files under <literal>/usr</literal>
                or <literal>/usr/local</literal>; this will cover most manual
                installations as well as installations from
                packages. If <literal>setup.py</literal> can not find
                <literal>libhdf5</literal>, <literal>libhdf5</literal>
                (or <literal>liblzo</literal>, or <literal>libbz2</literal>
                that you may wish to use) or if you have several versions of a
                library installed and want to use a particular one, then you
                can set the path to the resource in the environment, by
                setting the values of the <literal>HDF5_DIR</literal>,
                <literal>LZO_DIR</literal>,
                or <literal>BZIP2_DIR</literal> environment variables to the
                path to the particular resource. You may also specify the
                locations of the resource root directories on the
                <literal>setup.py</literal> command line. For example:</para>

                <screen>--hdf5=/stuff/hdf5-1.8.5
--lzo=/stuff/lzo-2.02
--bzip2=/stuff/bzip2-1.0.5</screen>

                <para>If your HDF5 library was built as a shared library not
                in the runtime load path, then you can specify the additional
                linker flags needed to find the shared library on the command
                line as well. For example:</para>

                <screen>--lflags="-Xlinker -rpath -Xlinker /stuff/hdf5-1.8.5/lib"</screen>

                <para>You may also want to try setting the LD_LIBRARY_PATH
                environment variable to point to the directory where the
                shared libraries can be found. Check your compiler and linker
                documentation as well as the Python
                <literal>Distutils</literal> documentation for the correct
                syntax or environment variable names.</para>

                <para>It is also possible to link with specific libraries by
                setting the <literal>LIBS</literal> environment
                variable:</para>

                <screen>LIBS="hdf5-1.8.5 nsl"</screen>

                <para>Finally, you can give additional flags to your compiler
                by passing them to the <literal>--cflags</literal>
                flag:</para>

                <screen>--cflags="-w -O3 -msse2"</screen>

                <para>In the above case, a <literal>gcc</literal> compiler is
                used and you instructed it to suppress all the warnings and
                set the level 3 of optimization.  Finally, if you are running
                Linux in 32-bit mode, and you know that your CPU has support
                for SSE2 vector instructions, you may want to pass
                the <literal>-msse2</literal> flag that will accelerate Blosc
                operation.</para>
              </listitem>
            </varlistentry>

            <varlistentry>
              <term><emphasis role="bold">Windows</emphasis></term>

              <listitem>
                <para>You can get ready-to-use Windows binaries and other
                development files for most of the following libraries from the
                GnuWin32 project (see <biblioref linkend="GnuWin32" />).  In
                case you cannot find the LZO binaries in the GnuWin32
                repository, you can find them at
                <ulink url="http://www.pytables.org/download/lzo-win"/>.
                </para>

                <para>Once you have installed the prerequisites,
                <literal>setup.py</literal> needs to know where the necessary
                library <emphasis>stub</emphasis> (<literal>.lib</literal>)
                and <emphasis>header</emphasis> (<literal>.h</literal>) files
                are installed. You can set the path to the
                <literal>include</literal> and <literal>dll</literal>
                directories for the HDF5 (mandatory) and
                LZO or BZIP2 (optional) libraries in the environment, by
                setting the values of the <literal>HDF5_DIR</literal>,
                <literal>LZO_DIR</literal>,
                or <literal>BZIP2_DIR</literal> environment variables to the
                path to the particular resource.  For example:</para>

                <screen>set HDF5_DIR=c:\stuff\hdf5-1.8.5-32bit-VS2008-IVF101\release
set LZO_DIR=c:\Program Files (x86)\GnuWin32
set BZIP2_DIR=c:\Program Files (x86)\GnuWin32</screen>

                <para>You may also specify the locations of the resource root
                directories on the <literal>setup.py</literal> command line.
                For example:</para>

                <screen>--hdf5=c:\stuff\hdf5-1.8.5-32bit-VS2008-IVF101\release
--lzo=c:\Program Files (x86)\GnuWin32
--bzip2=c:\Program Files (x86)\GnuWin32</screen>
              </listitem>
            </varlistentry>
          </variablelist>
        </section>

        <section id="PyTablesSourceInstallationDescr">
          <title>PyTables package installation</title>

          <para>Once you have installed the HDF5 library and the NumPy and
          Numexpr packages, you can proceed with the PyTables package
          itself:</para>

          <orderedlist>
            <listitem>
              <para>Run this command from the main PyTables distribution
              directory, including any extra command line arguments as
              discussed above:</para>

              <screen>python setup.py build_ext --inplace</screen>

            </listitem>

            <listitem>
              <para>To run the test suite, execute any of these
              commands:</para>

              <variablelist>
                <varlistentry>
                  <term><emphasis role="bold">Unix</emphasis></term>

                  <listitem>
                    <para>In the <literal>sh</literal> shell and its
                    variants:</para>

                    <screen>PYTHONPATH=.:$PYTHONPATH  python tables/tests/test_all.py</screen>

                    <para>or, if you prefer:</para>

                    <screen>PYTHONPATH=.:$PYTHONPATH  python -c "import tables; tables.test()"</screen>

                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term><emphasis role="bold">Windows</emphasis></term>

                  <listitem>
                    <para>Open the command prompt (<literal>cmd.exe</literal>
                    or <literal>command.com</literal>) and type:</para>

                    <screen>set PYTHONPATH=.;%PYTHONPATH%
python tables\tests\test_all.py</screen>

                    <para>or:</para>

                    <screen>set PYTHONPATH=.;%PYTHONPATH%
python -c "import tables; tables.test()"</screen>
                  </listitem>
                </varlistentry>
              </variablelist>

              <para>Both commands do the same thing, but the latter still
              works on an already installed PyTables (so, there is no need to
              set the PYTHONPATH variable for this case).  However, before
              installation, the former is recommended because it is more
              flexible, as you can see below.</para>

              <para>If you would like to see verbose output from the tests
              simply add the <literal>-v</literal> flag and/or the word
              <literal>verbose</literal> to the first of the command lines
              above. You can also run only the tests in a particular test
              module. For example, to execute just the
              <literal>test_types</literal> test suite, you only have to
              specify it:</para>

              <screen>python tables/tests/test_types.py -v  # change to backslashes for win</screen>

              <para>You have other options to pass to the
              <literal>test_all.py</literal> driver: <screen>python tables/tests/test_all.py --heavy  # change to backslashes for win</screen>
              The command above runs every test in the test unit. Beware, it
              can take a lot of time, CPU and memory resources to complete.
              <screen>python tables/tests/test_all.py --print-versions  # change to backslashes for win</screen>
              The command above shows the versions for all the packages that
              PyTables relies on. Please be sure to include this when
              reporting bugs. <screen>python tables/tests/test_all.py --show-memory  # only under Linux 2.6.x</screen>
              The command above prints out the evolution of the memory
              consumption after each test module completion. It's useful for
              locating memory leaks in PyTables (or packages behind it). Only
              valid for Linux 2.6.x kernels.</para>

              <para>And last, but not least, in case a test fails, please run
              the failing test module again and enable the verbose output:
              <screen>python tables/tests/test_&lt;module&gt;.py -v verbose</screen>
              and, very important, obtain your PyTables version information by
              using the <literal>--print-versions</literal> flag (see above)
              and send back both outputs to developers so that we may continue
              improving PyTables.</para>

              <para>If you run into problems because Python can not load the
              HDF5 library or other shared libraries:</para>

              <variablelist>
                <varlistentry>
                  <term><emphasis role="bold">Unix</emphasis></term>

                  <listitem>
                    <para>Try setting the LD_LIBRARY_PATH or equivalent
                    environment variable to point to the directory where the
                    missing libraries can be found.</para>
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term><emphasis role="bold">Windows</emphasis></term>

                  <listitem>
                    <para>Put the DLL libraries
                    (<literal>hdf5dll.dll</literal> and, optionally,
                    <literal>lzo1.dll</literal> and
                    <literal>bzip2.dll</literal>) in a directory listed in
                    your <literal>PATH</literal> environment variable. The
                    <literal>setup.py</literal> installation program will
                    print out a warning to that effect if the libraries can
                    not be found.</para>
                  </listitem>
                </varlistentry>
              </variablelist>
            </listitem>

            <listitem>
              <para>To install the entire PyTables Python package, change back
              to the root distribution directory and run the following command
              (make sure you have sufficient permissions to write to the
              directories where the PyTables files will be installed):</para>

              <screen>python setup.py install</screen>

              <para>Of course, you will need super-user privileges if you want
              to install PyTables on a system-protected area. You can select,
              though, a different place to install the package using the
              <literal>--prefix</literal> flag:</para>

              <screen>python setup.py install --prefix="/home/myuser/mystuff"</screen>

              <para>Have in mind, however, that if you use the
              <literal>--prefix</literal> flag to install in a non-standard
              place, you should properly setup your
              <literal>PYTHONPATH</literal> environment variable, so that the
              Python interpreter would be able to find your new PyTables
              installation.</para>

              <para>You have more installation options available in the
              Distutils package. Issue a: <screen>python setup.py install --help</screen>
              for more information on that subject.</para>
            </listitem>
          </orderedlist>

          <para>That's it! Now you can skip to the next chapter to learn how
          to use PyTables.</para>
        </section>
      </section>

      <section id="binaryInstallationDescr">
        <title>Binary installation (Windows)</title>

        <para>This section is intended for installing precompiled binaries on
        Windows platforms. You may also find it useful for instructions on how
        to install <emphasis>binary prerequisites</emphasis> even if you want
        to compile PyTables itself on Windows.</para>

        <warning>
          <para>Since PyTables 2.2b3, Windows binaries are distributed with
          SSE2 instructions enabled.  If your processor does not have support
          for SSE2, then you will not be able to use these binaries.</para>
        </warning>

        <section id="prerequisitesBinInst">
          <title>Windows prerequisites</title>

          <para>First, make sure that you have Python 2.4, NumPy 1.4.1 and
          Numexpr 1.4.1 or higher installed (PyTables binaries have been built
          using NumPy 1.5 and Numexpr 1.4.1).  The binaries already include
          DLLs for HDF5 (1.6.10, 1.8.5), zlib1 (1.2.3), szlib (2.0,
          uncompression support only) and bzip2 (1.0.5) for Windows (2.8.0).
          The LZO DLL can't be included because of license issues (but read
          below for directives to install it if you want so).</para>

          <para>To enable compression with the optional LZO library (see the
          <xref linkend="compressionIssues" xrefstyle="select: label" /> for
          hints about how it may be used to improve performance), fetch and
          install the <literal>LZO</literal> from
          <ulink url="http://www.pytables.org/download/lzo-win" /> (choose
          v1.x for Windows 32-bit and v2.x for Windows 64-bit). Normally, you
          will only need to fetch that package and copy the
          included <literal>lzo1.dll/lzo2.dll</literal> file in a directory in
          the <literal>PATH</literal> environment variable (for example
          <literal>C:\WINDOWS\SYSTEM</literal>) or
          <literal>python_installation_path\Lib\site-packages\tables</literal>
          (the last directory may not exist yet, so if you want to install the
          DLL there, you should do so <emphasis>after</emphasis> installing
          the PyTables package), so that it can be found by the PyTables
          extensions.</para>

          <para>Please note that PyTables has internal machinery for dealing
          with uninstalled optional compression libraries, so, you don't need
          to install the LZO dynamic library if you don't want to.</para>
        </section>

        <section id="PyTablesBinInstallDescr">
          <title>PyTables package installation</title>

          <para>Download the
          <literal>tables-&lt;version&gt;.win32-py&lt;version&gt;.exe</literal>
          file and execute it.</para>

          <para>You can (and <emphasis>you should</emphasis>) test your
          installation by running the next commands: <screen>>>> import tables
>>> tables.test()</screen> on your favorite python shell. If all the
          tests pass (possibly with a few warnings, related to the potential
          unavailability of LZO lib) you already have a working,
          well-tested copy of PyTables installed! If any test fails, please
          copy the output of the error messages as well as the output of:
          <screen>>>> tables.print_versions()</screen> and mail them
          to the developers so that the problem can be fixed in future
          releases.</para>

          <para>You can proceed now to the next chapter to see how to use
          PyTables.</para>
        </section>
      </section>
    </chapter>

    <chapter id="usage">
      <title>Tutorials</title>

      <epigraph>
        <attribution>Lyrics: Vicent Andrés i Estellés. Music: Ovidi Montllor,
        Toti Soler, <citetitle>M'aclame a tu</citetitle></attribution>

        <literallayout>
      Seràs la clau que obre tots els panys,
      seràs la llum, la llum il.limitada,
      seràs confí on l'aurora comença,
      seràs forment, escala il.luminada!
      </literallayout>
      </epigraph>

      <para></para>

      <para>This chapter consists of a series of simple yet comprehensive
      tutorials that will enable you to understand PyTables' main features. If
      you would like more information about some particular instance variable,
      global function, or method, look at the doc strings or go to the library
      reference in <xref linkend="libraryReference"
      xrefstyle="select: label" />. If you are reading this in PDF or HTML
      formats, follow the corresponding hyperlink near each newly introduced
      entity.</para>

      <para>Please note that throughout this document the terms
      <emphasis>column</emphasis> and <emphasis>field</emphasis> will be used
      interchangeably, as will the terms <emphasis>row</emphasis> and
      <emphasis>record</emphasis>.</para>

      <section>
        <title>Getting started</title>

        <para>In this section, we will see how to define our own records in
        Python and save collections of them (i.e. a
        <emphasis>table</emphasis>) into a file. Then we will select some of
        the data in the table using Python cuts and create NumPy arrays to
        store this selection as separate objects in a tree.</para>

        <para>In <emphasis>examples/tutorial1-1.py</emphasis> you will find
        the working version of all the code in this section. Nonetheless, this
        tutorial series has been written to allow you reproduce it in a Python
        interactive console. I encourage you to do parallel testing and
        inspect the created objects (variables, docs, children objects, etc.)
        during the course of the tutorial!</para>

        <section>
          <title>Importing <literal>tables</literal> objects</title>

          <para>Before starting you need to import the public objects in the
          <literal>tables</literal> package. You normally do that by
          executing:</para>

          <screen>>>> import tables</screen>

          <para>This is the recommended way to import
          <literal>tables</literal> if you don't want to pollute your
          namespace. However, PyTables has a contained set of first-level
          primitives, so you may consider using the alternative:</para>

          <screen>>>> from tables import *</screen>

          <para>If you are going to work with <literal>NumPy</literal> arrays
          (and normally, you will) you will also need to import functions from
          the <literal>numpy</literal> package. So most PyTables programs
          begin with:</para>

          <screen>>>> import tables        # but in this tutorial we use "from tables import *"
>>> import numpy</screen>
        </section>

        <section>
          <title>Declaring a Column Descriptor</title>

          <para>Now, imagine that we have a particle detector and we want to
          create a table object in order to save data retrieved from it. You
          need first to define the table, the number of columns it has, what
          kind of object is contained in each column, and so on.</para>

          <para>Our particle detector has a TDC (Time to Digital Converter)
          counter with a dynamic range of 8 bits and an ADC (Analogical to
          Digital Converter) with a range of 16 bits. For these values, we
          will define 2 fields in our record object called
          <literal>TDCcount</literal> and <literal>ADCcount</literal>. We also
          want to save the grid position in which the particle has been
          detected, so we will add two new fields called
          <literal>grid_i</literal> and <literal>grid_j</literal>. Our
          instrumentation also can obtain the pressure and energy of the
          particle. The resolution of the pressure-gauge allows us to use a
          single-precision float to store <literal>pressure</literal>
          readings, while the <literal>energy</literal> value will need a
          double-precision float. Finally, to track the particle we want to
          assign it a name to identify the kind of the particle it is and a
          unique numeric identifier. So we will add two more fields:
          <literal>name</literal> will be a string of up to 16 characters, and
          <literal>idnumber</literal> will be an integer of 64 bits (to allow
          us to store records for extremely large numbers of
          particles).</para>

          <para>Having determined our columns and their types, we can now
          declare a new <literal>Particle</literal> class that will contain
          all this information:</para>

          <screen>>>> from tables import *
>>> class Particle(IsDescription):
      name      = StringCol(16)   # 16-character String
      idnumber  = Int64Col()      # Signed 64-bit integer
      ADCcount  = UInt16Col()     # Unsigned short integer
      TDCcount  = UInt8Col()      # unsigned byte
      grid_i    = Int32Col()      # 32-bit integer
      grid_j    = Int32Col()      # 32-bit integer
      pressure  = Float32Col()    # float  (single-precision)
      energy    = Float64Col()    # double (double-precision)

>>> </screen>

          <para>This definition class is self-explanatory. Basically, you
          declare a class variable for each field you need. As its value you
          assign an instance of the appropriate <literal>Col</literal>
          subclass, according to the kind of column defined (the data type,
          the length, the shape, etc). See the <xref linkend="ColClassDescr"
          xrefstyle="select: label" /> for a complete description of these
          subclasses. See also <xref linkend="datatypesSupported"
          xrefstyle="select: label" /> for a list of data types supported by
          the <literal>Col</literal> constructor.</para>

          <para>From now on, we can use <literal>Particle</literal> instances
          as a descriptor for our detector data table. We will see later on
          how to pass this object to construct the table. But first, we must
          create a file where all the actual data pushed into our table will
          be saved.</para>
        </section>

        <section>
          <title>Creating a PyTables file from scratch</title>

          <para>Use the first-level <literal>openFile</literal> function (see
          <xref linkend="openFileDescr" />) to create a PyTables file:</para>

          <screen>>>> h5file = openFile("tutorial1.h5", mode = "w", title = "Test file")</screen>

          <para><literal>openFile()</literal> (see <xref
          linkend="openFileDescr" />) is one of the objects imported by the
          "<literal>from tables import *</literal>" statement. Here, we are
          saying that we want to create a new file in the current working
          directory called "<literal>tutorial1.h5</literal>" in
          "<literal>w</literal>"rite mode and with an descriptive title string
          ("<literal>Test file</literal>"). This function attempts to open the
          file, and if successful, returns the <literal>File</literal> (see
          <xref linkend="FileClassDescr" xrefstyle="select: label" />) object
          instance <literal>h5file</literal>. The root of the object tree is
          specified in the instance's <literal>root</literal>
          attribute.</para>
        </section>

        <section>
          <title>Creating a new group</title>

          <para>Now, to better organize our data, we will create a group
          called <emphasis>detector</emphasis> that branches from the root
          node. We will save our particle data table in this group.</para>

          <screen>>>> group = h5file.createGroup("/", 'detector', 'Detector information')</screen>

          <para>Here, we have taken the <literal>File</literal> instance
          <literal>h5file</literal> and invoked its
          <literal>createGroup()</literal> method (see <xref
          linkend="createGroupDescr" xrefstyle="select: label" />) to create a
          new group called <emphasis>detector</emphasis> branching from
          "<emphasis>/</emphasis>" (another way to refer to the
          <literal>h5file.root</literal> object we mentioned above). This will
          create a new <literal>Group</literal> (see <xref
          linkend="GroupClassDescr" xrefstyle="select: label" />) object
          instance that will be assigned to the variable
          <literal>group</literal>.</para>
        </section>

        <section>
          <title>Creating a new table</title>

          <para>Let's now create a <literal>Table</literal> (see <xref
          linkend="TableClassDescr" xrefstyle="select: label" />) object as a
          branch off the newly-created group. We do that by calling the
          <literal>createTable</literal> (see <xref linkend="createTableDescr"
          />) method of the <literal>h5file</literal> object:</para>

          <screen>>>> table = h5file.createTable(group, 'readout', Particle, "Readout example")</screen>

          <para>We create the <literal>Table</literal> instance under
          <literal>group</literal>. We assign this table the node name
          "<emphasis>readout</emphasis>". The <literal>Particle</literal>
          class declared before is the <emphasis>description</emphasis>
          parameter (to define the columns of the table) and finally we set
          "<emphasis>Readout example</emphasis>" as the
          <literal>Table</literal> title. With all this information, a new
          <literal>Table</literal> instance is created and assigned to the
          variable <emphasis>table</emphasis>.</para>

          <para>If you are curious about how the object tree looks right now,
          simply <literal>print</literal> the <literal>File</literal> instance
          variable <emphasis>h5file</emphasis>, and examine the output:</para>

          <screen>>>> print h5file
tutorial1.h5 (File) 'Test file'
Last modif.: 'Wed Mar  7 11:06:12 2007'
Object Tree:
/ (RootGroup) 'Test file'
/detector (Group) 'Detector information'
/detector/readout (Table(0,)) 'Readout example'</screen>

          <para>As you can see, a dump of the object tree is displayed. It's
          easy to see the <literal>Group</literal> and
          <literal>Table</literal> objects we have just created. If you want
          more information, just type the variable containing the
          <literal>File</literal> instance:</para>

          <screen>>>> h5file
File(filename='tutorial1.h5', title='Test file', mode='w', rootUEP='/', filters=Filters(complevel=0, shuffle=False, fletcher32=False))
/ (RootGroup) 'Test file'
/detector (Group) 'Detector information'
/detector/readout (Table(0,)) 'Readout example'
  description := {
  "ADCcount": UInt16Col(shape=(), dflt=0, pos=0),
  "TDCcount": UInt8Col(shape=(), dflt=0, pos=1),
  "energy": Float64Col(shape=(), dflt=0.0, pos=2),
  "grid_i": Int32Col(shape=(), dflt=0, pos=3),
  "grid_j": Int32Col(shape=(), dflt=0, pos=4),
  "idnumber": Int64Col(shape=(), dflt=0, pos=5),
  "name": StringCol(itemsize=16, shape=(), dflt='', pos=6),
  "pressure": Float32Col(shape=(), dflt=0.0, pos=7)}
  byteorder := 'little'
  chunkshape := (87,)</screen>

          <para>More detailed information is displayed about each object in
          the tree. Note how <literal>Particle</literal>, our table descriptor
          class, is printed as part of the <emphasis>readout</emphasis> table
          description information. In general, you can obtain much more
          information about the objects and their children by just printing
          them. That introspection capability is very useful, and I recommend
          that you use it extensively.</para>

          <para>The time has come to fill this table with some values. First
          we will get a pointer to the <literal>Row</literal> (see <xref
          linkend="RowClassDescr" xrefstyle="select: label" />) instance of
          this <literal>table</literal> instance:</para>

          <screen>>>> particle = table.row</screen>

          <para>The <literal>row</literal> attribute of
          <literal>table</literal> points to the <literal>Row</literal>
          instance that will be used to write data rows into the table. We
          write data simply by assigning the <literal>Row</literal> instance
          the values for each row as if it were a dictionary (although it is
          actually an <emphasis>extension class</emphasis>), using the column
          names as keys.</para>

          <para>Below is an example of how to write rows: <screen>>>> for i in xrange(10):
      particle['name']  = 'Particle: %6d' % (i)
      particle['TDCcount'] = i % 256
      particle['ADCcount'] = (i * 256) % (1 &lt;&lt; 16)
      particle['grid_i'] = i
      particle['grid_j'] = 10 - i
      particle['pressure'] = float(i*i)
      particle['energy'] = float(particle['pressure'] ** 4)
      particle['idnumber'] = i * (2 ** 34)
      # Insert a new particle record
      particle.append()

>>> </screen></para>

          <para>This code should be easy to understand. The lines inside the
          loop just assign values to the different columns in the Row instance
          <literal>particle</literal> (see <xref linkend="RowClassDescr"
          xrefstyle="select: label" />). A call to its
          <literal>append()</literal> method writes this information to the
          <literal>table</literal> I/O buffer.</para>

          <para>After we have processed all our data, we should flush the
          table's I/O buffer if we want to write all this data to disk. We
          achieve that by calling the <literal>table.flush()</literal>
          method.</para>

          <screen>>>> table.flush()</screen>

          <para>Remember, flushing a table is a <emphasis>very
          important</emphasis> step as it will not only help to maintain the
          integrity of your file, but also will free valuable memory resources
          (i.e. internal buffers) that your program may need for other
          things.</para>
        </section>

        <section id="readingAndSelectingUsage">
          <title>Reading (and selecting) data in a table</title>

          <para>Ok. We have our data on disk, and now we need to access it and
          select from specific columns the values we are interested in. See
          the example below:</para>

          <screen>>>> table = h5file.root.detector.readout
>>> pressure = [ x['pressure'] for x in table.iterrows()
               if x['TDCcount'] &gt; 3 and 20 &lt;= x['pressure'] &lt; 50 ]
>>> pressure
[25.0, 36.0, 49.0]</screen>

          <para>The first line creates a "shortcut" to the
          <emphasis>readout</emphasis> table deeper on the object tree. As you
          can see, we use the <emphasis>natural naming</emphasis> schema to
          access it. We also could have used the
          <literal>h5file.getNode()</literal> method, as we will do later
          on.</para>

          <para>You will recognize the last two lines as a Python list
          comprehension. It loops over the rows in <emphasis>table</emphasis>
          as they are provided by the <literal>table.iterrows()</literal>
          iterator (see <xref linkend="Table.iterrows" />). The iterator
          returns values until all the data in table is exhausted. These rows
          are filtered using the expression: <screen>x['TDCcount'] &gt; 3 and 20 &lt;= x['pressure'] &lt; 50</screen>So,
          we are selecting the values of the <literal>pressure</literal>
          column from filtered records to create the final list and assign it
          to <literal>pressure</literal> variable.</para>

          <para>We could have used a normal <literal>for</literal> loop to
          accomplish the same purpose, but I find comprehension syntax to be
          more compact and elegant.</para>

          <para>PyTables do offer other, more powerful ways of performing
          selections which may be more suitable if you have very large tables
          or if you need very high query speeds. They are called
          <emphasis>in-kernel</emphasis> and <emphasis>indexed</emphasis>
          queries, and you can use them
          through <literal>Table.where()</literal> (see
          <xref linkend="Table.where" />) and other related methods.</para>

          <para>Let's use an in-kernel selection to query
          the <literal>name</literal> column for the same set of cuts:</para>

          <screen>>>> names = [ x['name'] for x in table.where(
              """(TDCcount &gt; 3) &amp; (20 &lt;= pressure) &amp; (pressure &lt; 50)""") ]
>>> names
['Particle:      5', 'Particle:      6', 'Particle:      7']</screen>

          <para>In-kernel and indexed queries are not only much faster, but as
          you can see, they also look more compact, and are among the
          greatests features for PyTables, so be sure that you use them a
          lot. See <xref linkend="conditionSyntax" xrefstyle="select: label"
          /> and <xref linkend="searchOptim" xrefstyle="select: label" /> for
          more information on in-kernel and indexed selections.</para>

          <para>That's enough about selections for now. The next section will
          show you how to save these selected results to a file.</para>
        </section>

        <section>
          <title>Creating new array objects</title>

          <para>In order to separate the selected data from the mass of
          detector data, we will create a new group <literal>columns</literal>
          branching off the root group. Afterwards, under this group, we will
          create two arrays that will contain the selected data. First, we
          create the group:</para>

          <screen>>>> gcolumns = h5file.createGroup(h5file.root, "columns", "Pressure and Name")</screen>

          <para>Note that this time we have specified the first parameter
          using <emphasis>natural naming</emphasis>
          (<literal>h5file.root</literal>) instead of with an absolute path
          string ("/").</para>

          <para>Now, create the first of the two <literal>Array</literal>
          objects we've just mentioned:</para>

          <screen>>>> h5file.createArray(gcolumns, 'pressure', array(pressure),
                    "Pressure column selection")
/columns/pressure (Array(3,)) 'Pressure column selection'
  atom := Float64Atom(shape=(), dflt=0.0)
  maindim := 0
  flavor := 'numpy'
  byteorder := 'little'
  chunkshape := None</screen>

          <para>We already know the first two parameters of the
          <literal>createArray</literal> (see <xref
          linkend="createArrayDescr" />) methods (these are the same as the
          first two in <literal>createTable</literal>): they are the parent
          group <emphasis>where</emphasis> <literal>Array</literal> will be
          created and the <literal>Array</literal> instance
          <emphasis>name</emphasis>. The third parameter is the
          <emphasis>object</emphasis> we want to save to disk. In this case,
          it is a <literal>NumPy</literal> array that is built from the
          selection list we created before. The fourth parameter is the
          <emphasis>title</emphasis>.</para>

          <para>Now, we will save the second array. It contains the list of
          strings we selected before: we save this object as-is, with no
          further conversion.</para>

          <screen>>>> h5file.createArray(gcolumns, 'name', names, "Name column selection")
/columns/name (Array(3,)) 'Name column selection'
  atom := StringAtom(itemsize=16, shape=(), dflt='')
  maindim := 0
  flavor := 'python'
  byteorder := 'irrelevant'
  chunkshape := None</screen>

          <para>As you can see, <literal>createArray()</literal> accepts
          <emphasis>names</emphasis> (which is a regular Python list) as an
          <emphasis>object</emphasis> parameter. Actually, it accepts a
          variety of different regular objects (see <xref
          linkend="createArrayDescr" />) as parameters. The
          <literal>flavor</literal> attribute (see the output above) saves the
          original kind of object that was saved. Based on this
          <emphasis>flavor</emphasis>, PyTables will be able to retrieve
          exactly the same object from disk later on.</para>

          <para>Note that in these examples, the
          <literal>createArray</literal> method returns an
          <literal>Array</literal> instance that is not assigned to any
          variable. Don't worry, this is intentional to show the kind of
          object we have created by displaying its representation. The
          <literal>Array</literal> objects have been attached to the object
          tree and saved to disk, as you can see if you print the complete
          object tree:</para>

          <screen>>>> print h5file
tutorial1.h5 (File) 'Test file'
Last modif.: 'Wed Mar  7 19:40:44 2007'
Object Tree:
/ (RootGroup) 'Test file'
/columns (Group) 'Pressure and Name'
/columns/name (Array(3,)) 'Name column selection'
/columns/pressure (Array(3,)) 'Pressure column selection'
/detector (Group) 'Detector information'
/detector/readout (Table(10,)) 'Readout example'</screen>
        </section>

        <section>
          <title>Closing the file and looking at its content</title>

          <para>To finish this first tutorial, we use the
          <literal>close</literal> method of the h5file
          <literal>File</literal> object to close the file before exiting
          Python:</para>

          <screen>>>> h5file.close()
>>> ^D
$ </screen>

          <para>You have now created your first PyTables file with a table and
          two arrays. You can examine it with any generic HDF5 tool, such as
          <literal>h5dump</literal> or <literal>h5ls</literal>. Here is what
          the <literal>tutorial1.h5</literal> looks like when read with the
          <literal>h5ls</literal> program:</para>

          <screen>$ h5ls -rd tutorial1.h5
/columns                 Group
/columns/name            Dataset {3}
    Data:
        (0) "Particle:      5", "Particle:      6", "Particle:      7"
/columns/pressure        Dataset {3}
    Data:
        (0) 25, 36, 49
/detector                Group
/detector/readout        Dataset {10/Inf}
    Data:
        (0) {0, 0, 0, 0, 10, 0, "Particle:      0", 0},
        (1) {256, 1, 1, 1, 9, 17179869184, "Particle:      1", 1},
        (2) {512, 2, 256, 2, 8, 34359738368, "Particle:      2", 4},
        (3) {768, 3, 6561, 3, 7, 51539607552, "Particle:      3", 9},
        (4) {1024, 4, 65536, 4, 6, 68719476736, "Particle:      4", 16},
        (5) {1280, 5, 390625, 5, 5, 85899345920, "Particle:      5", 25},
        (6) {1536, 6, 1679616, 6, 4, 103079215104, "Particle:      6", 36},
        (7) {1792, 7, 5764801, 7, 3, 120259084288, "Particle:      7", 49},
        (8) {2048, 8, 16777216, 8, 2, 137438953472, "Particle:      8", 64},
        (9) {2304, 9, 43046721, 9, 1, 154618822656, "Particle:      9", 81}</screen>

          <para>Here's the output as displayed by the "ptdump" PyTables
          utility (located in <literal>utils/</literal> directory):</para>

          <screen>$ ptdump tutorial1.h5
/ (RootGroup) 'Test file'
/columns (Group) 'Pressure and Name'
/columns/name (Array(3,)) 'Name column selection'
/columns/pressure (Array(3,)) 'Pressure column selection'
/detector (Group) 'Detector information'
/detector/readout (Table(10,)) 'Readout example'</screen>

          <para>You can pass the <literal>-v</literal> or
          <literal>-d</literal> options to <literal>ptdump</literal> if you
          want more verbosity. Try them out!</para>

          <para>Also, in <xref linkend="tutorial1-1-tableview"
          xrefstyle="select: label" />, you can admire how the
          <literal>tutorial1.h5</literal> looks like using the <ulink
          url="http://www.vitables.org">ViTables </ulink> graphical interface
          .</para>

          <figure id="tutorial1-1-tableview">
            <title>The initial version of the data file for tutorial 1, with a
            view of the data objects.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" fileref="tutorial1-1-tableview.png"
                           format="PNG" scale="75" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" fileref="tutorial1-1-tableview.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>
      </section>

      <!-- -->

      <section>
        <title>Browsing the <emphasis>object tree</emphasis></title>

        <para>In this section, we will learn how to browse the tree and
        retrieve data and also meta-information about the actual data.</para>

        <para>In <emphasis>examples/tutorial1-2.py</emphasis> you will find
        the working version of all the code in this section. As before, you
        are encouraged to use a python shell and inspect the object tree
        during the course of the tutorial.</para>

        <section>
          <title>Traversing the object tree</title>

          <para>Let's start by opening the file we created in last tutorial
          section.</para>

          <screen>>>> h5file = openFile("tutorial1.h5", "a")</screen>

          <para>This time, we have opened the file in "a"ppend mode. We use
          this mode to add more information to the file.</para>

          <para>PyTables, following the Python tradition, offers powerful
          introspection capabilities, i.e. you can easily ask information
          about any component of the object tree as well as search the
          tree.</para>

          <para>To start with, you can get a preliminary overview of the
          object tree by simply printing the existing <literal>File</literal>
          instance:</para>

          <screen>>>> print h5file
tutorial1.h5 (File) 'Test file'
Last modif.: 'Wed Mar  7 19:50:57 2007'
Object Tree:
/ (RootGroup) 'Test file'
/columns (Group) 'Pressure and Name'
/columns/name (Array(3,)) 'Name column selection'
/columns/pressure (Array(3,)) 'Pressure column selection'
/detector (Group) 'Detector information'
/detector/readout (Table(10,)) 'Readout example'</screen>

          <para>It looks like all of our objects are there. Now let's make use
          of the <literal>File</literal> iterator to see how to list all the
          nodes in the object tree:</para>

          <screen>>>> for node in h5file:
      print node

/ (RootGroup) 'Test file'
/columns (Group) 'Pressure and Name'
/detector (Group) 'Detector information'
/columns/name (Array(3,)) 'Name column selection'
/columns/pressure (Array(3,)) 'Pressure column selection'
/detector/readout (Table(10,)) 'Readout example'</screen>

          <para>We can use the <literal>walkGroups</literal> method (see <xref
          linkend="walkGroupsDescr" />) of the <literal>File</literal> class
          to list only the <emphasis>groups</emphasis> on tree:</para>

          <screen>>>> for group in h5file.walkGroups():
      print group

/ (RootGroup) 'Test file'
/columns (Group) 'Pressure and Name'
/detector (Group) 'Detector information'</screen>

          <para>Note that <literal>walkGroups()</literal> actually returns an
          <emphasis>iterator</emphasis>, not a list of objects. Using this
          iterator with the <literal>listNodes()</literal> method is a
          powerful combination. Let's see an example listing of all the arrays
          in the tree:</para>

          <screen>>>> for group in h5file.walkGroups("/"):
      for array in h5file.listNodes(group, classname='Array'):
          print array

/columns/name (Array(3,)) 'Name column selection'
/columns/pressure (Array(3,)) 'Pressure column selection'</screen>

          <para><literal>listNodes()</literal> (see <xref
          linkend="File.listNodes" />) returns a list containing all the nodes
          hanging off a specific <literal>Group</literal>. If the
          <emphasis>classname</emphasis> keyword is specified, the method will
          filter out all instances which are not descendants of the class. We
          have asked for only <literal>Array</literal> instances. There exist
          also an iterator counterpart called <literal>iterNodes()</literal>
          (see <xref linkend="File.iterNodes" />) that might be handy is some
          situations, like for example when dealing with groups with a large
          number of nodes behind it.</para>

          <para>We can combine both calls by using the
          <literal>walkNodes(where, classname)</literal> special method of the
          <literal>File</literal> object (see <xref
          linkend="File.walkNodes" />). For example:</para>

          <screen>>>> for array in h5file.walkNodes("/", "Array"):
      print array

/columns/name (Array(3,)) 'Name column selection'
/columns/pressure (Array(3,)) 'Pressure column selection'</screen>

          <para>This is a nice shortcut when working interactively.</para>

          <para>Finally, we will list all the <literal>Leaf</literal>, i.e.
          <literal>Table</literal> and <literal>Array</literal> instances (see
          <xref linkend="LeafClassDescr" xrefstyle="select: label" />
          for detailed information on <literal>Leaf</literal> class), in the
          <literal>/detector</literal> group. Note that only one instance of
          the <literal>Table</literal> class (i.e. <literal>readout</literal>)
          will be selected in this group (as should be the case):</para>

          <screen>>>> for leaf in h5file.root.detector._f_walkNodes('Leaf'):
      print leaf

/detector/readout (Table(10,)) 'Readout example'</screen>

          <para>We have used a call to the
          <literal>Group._f_walkNodes(classname)</literal> method (see <xref
          linkend="Group._f_walkNodes" />), using the <emphasis>natural
          naming</emphasis> path specification.</para>

          <para>Of course you can do more sophisticated node selections using
          these powerful methods. But first, let's take a look at some
          important PyTables object instance variables.</para>
        </section>

        <section>
          <title>Setting and getting user attributes</title>

          <para>PyTables provides an easy and concise way to complement the
          meaning of your node objects on the tree by using the
          <literal>AttributeSet</literal> class (see <xref
          linkend="AttributeSetClassDescr" xrefstyle="select: label" />). You
          can access this object through the standard attribute
          <literal>attrs</literal> in <literal>Leaf</literal> nodes and
          <literal>_v_attrs</literal> in <literal>Group</literal>
          nodes.</para>

          <para>For example, let's imagine that we want to save the date
          indicating when the data in <literal>/detector/readout</literal>
          table has been acquired, as well as the temperature during the
          gathering process:</para>

          <screen>>>> table = h5file.root.detector.readout
>>> table.attrs.gath_date = "Wed, 06/12/2003 18:33"
>>> table.attrs.temperature = 18.4
>>> table.attrs.temp_scale = "Celsius"</screen>

          <para>Now, let's set a somewhat more complex attribute in the
          <literal>/detector</literal> group:</para>

          <screen>>>> detector = h5file.root.detector
>>> detector._v_attrs.stuff = [5, (2.3, 4.5), "Integer and tuple"]</screen>

          <para>Note how the AttributeSet instance is accessed with the
          <literal>_v_attrs</literal> attribute because detector is a
          <literal>Group</literal> node. In general, you can save any standard
          Python data structure as an attribute node. See <xref
          linkend="AttributeSetClassDescr" xrefstyle="select: label" /> for a
          more detailed explanation of how they are serialized for export to
          disk.</para>

          <para>Retrieving the attributes is equally simple:</para>

          <screen>>>> table.attrs.gath_date
'Wed, 06/12/2003 18:33'
>>> table.attrs.temperature
18.399999999999999
>>> table.attrs.temp_scale
'Celsius'
>>> detector._v_attrs.stuff
[5, (2.2999999999999998, 4.5), 'Integer and tuple']</screen>

          <para>You can probably guess how to delete attributes:</para>

          <screen>>>> del table.attrs.gath_date</screen>

          <para>If you want to examine the current user attribute set of
          <literal>/detector/table</literal>, you can print its representation
          (try hitting the <literal>TAB</literal> key twice if you are on a
          Unix Python console with the <literal>rlcompleter</literal> module
          active):</para>

          <screen>>>> table.attrs
/detector/readout._v_attrs (AttributeSet), 23 attributes:
   [CLASS := 'TABLE',
    FIELD_0_FILL := 0,
    FIELD_0_NAME := 'ADCcount',
    FIELD_1_FILL := 0,
    FIELD_1_NAME := 'TDCcount',
    FIELD_2_FILL := 0.0,
    FIELD_2_NAME := 'energy',
    FIELD_3_FILL := 0,
    FIELD_3_NAME := 'grid_i',
    FIELD_4_FILL := 0,
    FIELD_4_NAME := 'grid_j',
    FIELD_5_FILL := 0,
    FIELD_5_NAME := 'idnumber',
    FIELD_6_FILL := '',
    FIELD_6_NAME := 'name',
    FIELD_7_FILL := 0.0,
    FIELD_7_NAME := 'pressure',
    FLAVOR := 'numpy',
    NROWS := 10,
    TITLE := 'Readout example',
    VERSION := '2.6',
    temp_scale := 'Celsius',
    temperature := 18.399999999999999]</screen>

          <para>We've got all the attributes (including the
          <emphasis>system</emphasis> attributes). You can get a list of
          <emphasis>all</emphasis> attributes or only the
          <emphasis>user</emphasis> or <emphasis>system</emphasis> attributes
          with the <literal>_f_list()</literal> method.</para>

          <screen>>>> print table.attrs._f_list("all")
['CLASS', 'FIELD_0_FILL', 'FIELD_0_NAME', 'FIELD_1_FILL', 'FIELD_1_NAME',
 'FIELD_2_FILL', 'FIELD_2_NAME', 'FIELD_3_FILL', 'FIELD_3_NAME', 'FIELD_4_FILL',
 'FIELD_4_NAME', 'FIELD_5_FILL', 'FIELD_5_NAME', 'FIELD_6_FILL', 'FIELD_6_NAME',
 'FIELD_7_FILL', 'FIELD_7_NAME', 'FLAVOR', 'NROWS', 'TITLE', 'VERSION',
 'temp_scale', 'temperature']
>>> print table.attrs._f_list("user")
['temp_scale', 'temperature']
>>> print table.attrs._f_list("sys")
['CLASS', 'FIELD_0_FILL', 'FIELD_0_NAME', 'FIELD_1_FILL', 'FIELD_1_NAME',
 'FIELD_2_FILL', 'FIELD_2_NAME', 'FIELD_3_FILL', 'FIELD_3_NAME', 'FIELD_4_FILL',
 'FIELD_4_NAME', 'FIELD_5_FILL', 'FIELD_5_NAME', 'FIELD_6_FILL', 'FIELD_6_NAME',
 'FIELD_7_FILL', 'FIELD_7_NAME', 'FLAVOR', 'NROWS', 'TITLE', 'VERSION']</screen>

          <para>You can also rename attributes:</para>

          <screen>>>> table.attrs._f_rename("temp_scale","tempScale")
>>> print table.attrs._f_list()
['tempScale', 'temperature']</screen>

          <para>And, from PyTables 2.0 on, you are allowed also to set, delete
          or rename system attributes:</para>

          <screen>>>> table.attrs._f_rename("VERSION", "version")
>>> table.attrs.VERSION
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "tables/attributeset.py", line 222, in __getattr__
    (name, self._v__nodePath)
AttributeError: Attribute 'VERSION' does not exist in node: '/detector/readout'
>>> table.attrs.version
'2.6'</screen>

          <para><emphasis role="bold">Caveat emptor:</emphasis> you must be
          careful when modifying system attributes because you may end fooling
          PyTables and ultimately getting unwanted behaviour. Use this only if
          you know what are you doing.</para>

          <para>So, given the caveat above, we will proceed to restore the
          original name of VERSION attribute:<screen>>>> table.attrs._f_rename("version", "VERSION")
>>> table.attrs.VERSION
'2.6'</screen></para>

          <para>Ok. that's better. If you would terminate your session now,
          you would be able to use the <literal>h5ls</literal> command to read
          the <literal>/detector/readout</literal> attributes from the file
          written to disk:</para>

          <screen>$ h5ls -vr tutorial1.h5/detector/readout
Opened "tutorial1.h5" with sec2 driver.
/detector/readout        Dataset {10/Inf}
    Attribute: CLASS     scalar
        Type:      6-byte null-terminated ASCII string
        Data:  "TABLE"
    Attribute: VERSION   scalar
        Type:      4-byte null-terminated ASCII string
        Data:  "2.6"
    Attribute: TITLE     scalar
        Type:      16-byte null-terminated ASCII string
        Data:  "Readout example"
    Attribute: NROWS     scalar
        Type:      native long long
        Data:  10
    Attribute: FIELD_0_NAME scalar
        Type:      9-byte null-terminated ASCII string
        Data:  "ADCcount"
    Attribute: FIELD_1_NAME scalar
        Type:      9-byte null-terminated ASCII string
        Data:  "TDCcount"
    Attribute: FIELD_2_NAME scalar
        Type:      7-byte null-terminated ASCII string
        Data:  "energy"
    Attribute: FIELD_3_NAME scalar
        Type:      7-byte null-terminated ASCII string
        Data:  "grid_i"
    Attribute: FIELD_4_NAME scalar
        Type:      7-byte null-terminated ASCII string
        Data:  "grid_j"
    Attribute: FIELD_5_NAME scalar
        Type:      9-byte null-terminated ASCII string
        Data:  "idnumber"
    Attribute: FIELD_6_NAME scalar
        Type:      5-byte null-terminated ASCII string
        Data:  "name"
    Attribute: FIELD_7_NAME scalar
        Type:      9-byte null-terminated ASCII string
        Data:  "pressure"
    Attribute: FLAVOR    scalar
        Type:      5-byte null-terminated ASCII string
        Data:  "numpy"
    Attribute: tempScale scalar
        Type:      7-byte null-terminated ASCII string
        Data:  "Celsius"
    Attribute: temperature scalar
        Type:      native double
        Data:  18.4
    Location:  0:1:0:1952
    Links:     1
    Modified:  2006-12-11 10:35:13 CET
    Chunks:    {85} 3995 bytes
    Storage:   470 logical bytes, 3995 allocated bytes, 11.76% utilization
    Type:      struct {
                   "ADCcount"         +0    native unsigned short
                   "TDCcount"         +2    native unsigned char
                   "energy"           +3    native double
                   "grid_i"           +11   native int
                   "grid_j"           +15   native int
                   "idnumber"         +19   native long long
                   "name"             +27   16-byte null-terminated ASCII string
                   "pressure"         +43   native float
               } 47 bytes</screen>

          <para>Attributes are a useful mechanism to add persistent (meta)
          information to your data.</para>
        </section>

        <section>
          <title>Getting object metadata</title>

          <para>Each object in PyTables has <emphasis>metadata</emphasis>
          information about the data in the file. Normally this
          <emphasis>meta-information</emphasis> is accessible through the node
          instance variables. Let's take a look at some examples:</para>

          <screen>>>> print "Object:", table
Object: /detector/readout (Table(10,)) 'Readout example'
>>> print "Table name:", table.name
Table name: readout
>>> print "Table title:", table.title
Table title: Readout example
>>> print "Number of rows in table:", table.nrows
Number of rows in table: 10
>>> print "Table variable names with their type and shape:"
Table variable names with their type and shape:
>>> for name in table.colnames:
      print name, ':= %s, %s' % (table.coldtypes[name],
                                 table.coldtypes[name].shape)

ADCcount := uint16, ()
TDCcount := uint8, ()
energy := float64, ()
grid_i := int32, ()
grid_j := int32, ()
idnumber := int64, ()
name := |S16, ()
pressure := float32, ()</screen>

          <para>Here, the <literal>name</literal>, <literal>title</literal>,
          <literal>nrows</literal>, <literal>colnames</literal> and
          <literal>coldtypes</literal> attributes (see <xref
          linkend="TableInstanceVariablesDescr" xrefstyle="select: label" />
          for a complete attribute list) of the <literal>Table</literal>
          object gives us quite a bit of information about the table
          data.</para>

          <para>You can interactively retrieve general information about the
          public objects in PyTables by asking for help:</para>

          <screen>>>> help(table)
Help on Table in module tables.table:

class Table(tableExtension.Table, tables.leaf.Leaf)
 |  This class represents heterogeneous datasets in an HDF5 file.
 |
 |  Tables are leaves (see the `Leaf` class) whose data consists of a
 |  unidimensional sequence of *rows*, where each row contains one or
 |  more *fields*.  Fields have an associated unique *name* and
 |  *position*, with the first field having position 0.  All rows have
 |  the same fields, which are arranged in *columns*.
[snip]
 |
 |  Instance variables
 |  ------------------
 |
 |  The following instance variables are provided in addition to those
 |  in `Leaf`.  Please note that there are several ``col*`` dictionaries
 |  to ease retrieving information about a column directly by its path
 |  name, avoiding the need to walk through `Table.description` or
 |  `Table.cols`.
 |
 |  autoIndex
 |      Automatically keep column indexes up to date?
 |
 |      Setting this value states whether existing indexes should be
 |      automatically updated after an append operation or recomputed
 |      after an index-invalidating operation (i.e. removal and
 |      modification of rows).  The default is true.
[snip]
 |  rowsize
 |      The size in bytes of each row in the table.
 |
 |  Public methods -- reading
 |  -------------------------
 |
 |  * col(name)
 |  * iterrows([start][, stop][, step])
 |  * itersequence(sequence)
    * itersorted(sortby[, checkCSI][, start][, stop][, step])
 |  * read([start][, stop][, step][, field][, coords])
 |  * readCoordinates(coords[, field])
    * readSorted(sortby[, checkCSI][, field,][, start][, stop][, step])
 |  * __getitem__(key)
 |  * __iter__()
 |
 |  Public methods -- writing
 |  -------------------------
 |
 |  * append(rows)
 |  * modifyColumn([start][, stop][, step][, column][, colname])
[snip]</screen>

          <para>Try getting help with other object docs by yourself:</para>

          <screen>>>> help(h5file)
>>> help(table.removeRows)</screen>

          <para>To examine metadata in the
          <emphasis>/columns/pressure</emphasis> <literal>Array</literal>
          object:</para>

          <screen>>>> pressureObject = h5file.getNode("/columns", "pressure")
>>> print "Info on the object:", repr(pressureObject)
Info on the object: /columns/pressure (Array(3,)) 'Pressure column selection'
  atom := Float64Atom(shape=(), dflt=0.0)
  maindim := 0
  flavor := 'numpy'
  byteorder := 'little'
  chunkshape := None
>>> print "  shape: ==&gt;", pressureObject.shape
  shape: ==&gt; (3,)
>>> print "  title: ==&gt;", pressureObject.title
  title: ==&gt; Pressure column selection
>>> print "  atom: ==&gt;", pressureObject.atom
  atom: ==&gt; Float64Atom(shape=(), dflt=0.0)</screen>

          <para>Observe that we have used the <literal>getNode()</literal>
          method of the <literal>File</literal> class to access a node in the
          tree, instead of the natural naming method. Both are useful, and
          depending on the context you will prefer one or the other.
          <literal>getNode()</literal> has the advantage that it can get a
          node from the pathname string (as in this example) and can also act
          as a filter to show only nodes in a particular location that are
          instances of class <emphasis>classname</emphasis>. In general,
          however, I consider natural naming to be more elegant and easier to
          use, especially if you are using the name completion capability
          present in interactive console. Try this powerful combination of
          natural naming and completion capabilities present in most Python
          consoles, and see how pleasant it is to browse the object tree
          (well, as pleasant as such an activity can be).</para>

          <para>If you look at the <literal>type</literal> attribute of the
          <literal>pressureObject</literal> object, you can verify that it is
          a "<emphasis>float64</emphasis>" array. By looking at its
          <literal>shape</literal> attribute, you can deduce that the array on
          disk is unidimensional and has 3 elements. See <xref
          linkend="ArrayClassInstanceVariables" xrefstyle="select: label" />
          or the internal doc strings for the complete
          <literal>Array</literal> attribute list.</para>
        </section>

        <section>
          <title>Reading data from <literal>Array</literal> objects</title>

          <para>Once you have found the desired <literal>Array</literal>, use
          the <literal>read()</literal> method of the <literal>Array</literal>
          object to retrieve its data:</para>

          <screen>>>> pressureArray = pressureObject.read()
>>> pressureArray
array([ 25.,  36.,  49.])
>>> print "pressureArray is an object of type:", type(pressureArray)
pressureArray is an object of type: &lt;type 'numpy.ndarray'&gt;
>>> nameArray = h5file.root.columns.name.read()
>>> print "nameArray is an object of type:", type(nameArray)
nameArray is an object of type: &lt;type 'list'&gt;
>>>
>>> print "Data on arrays nameArray and pressureArray:"
Data on arrays nameArray and pressureArray:
>>> for i in range(pressureObject.shape[0]):
      print nameArray[i], "--&gt;", pressureArray[i]

Particle:      5 --&gt; 25.0
Particle:      6 --&gt; 36.0
Particle:      7 --&gt; 49.0</screen>

          <para>You can see that the <literal>read()</literal> method (see
          <xref linkend="Array.read" xrefstyle="select: label" />) returns
          an authentic <literal>NumPy</literal> object for the
          <literal>pressureObject</literal> instance by looking at the output
          of the <literal>type()</literal> call. A <literal>read()</literal>
          of the <literal>nameArray</literal> object instance returns a native
          Python list (of strings). The type of the object saved is stored as
          an HDF5 attribute (named <literal>FLAVOR</literal>) for objects on
          disk. This attribute is then read as <literal>Array</literal>
          meta-information (accessible through in the
          <literal>Array.attrs.FLAVOR</literal> variable), enabling the read
          array to be converted into the original object. This provides a
          means to save a large variety of objects as arrays with the
          guarantee that you will be able to later recover them in their
          original form. See <xref linkend="createArrayDescr" /> for a
          complete list of supported objects for the <literal>Array</literal>
          object class.</para>
        </section>
      </section>

      <section>
        <title>Commiting data to tables and arrays</title>

        <para>We have seen how to create tables and arrays and how to browse
        both data and metadata in the object tree. Let's examine more closely
        now one of the most powerful capabilities of PyTables, namely, how to
        modify already created tables and arrays<footnote>
            <para>Appending data to arrays is also supported, but you need to
            create special objects called <literal>EArray</literal> (see <xref
            linkend="EArrayClassDescr" xrefstyle="select: label" /> for more
            info).</para>
          </footnote>.</para>

        <section>
          <title>Appending data to an existing table</title>

          <para>Now, let's have a look at how we can add records to an
          existing table on disk. Let's use our well-known
          <emphasis>readout</emphasis> <literal>Table</literal> object and
          append some new values to it:</para>

          <screen>>>> table = h5file.root.detector.readout
>>> particle = table.row
>>> for i in xrange(10, 15):
      particle['name']  = 'Particle: %6d' % (i)
      particle['TDCcount'] = i % 256
      particle['ADCcount'] = (i * 256) % (1 &lt;&lt; 16)
      particle['grid_i'] = i
      particle['grid_j'] = 10 - i
      particle['pressure'] = float(i*i)
      particle['energy'] = float(particle['pressure'] ** 4)
      particle['idnumber'] = i * (2 ** 34)
      particle.append()

>>> table.flush()</screen>

          <para>It's the same method we used to fill a new table. PyTables
          knows that this table is on disk, and when you add new records, they
          are appended to the end of the table<footnote>
              <para>Note that you can append not only scalar values to tables,
              but also fully multidimensional array objects.</para>
            </footnote>.</para>

          <para>If you look carefully at the code you will see that we have
          used the <literal>table.row</literal> attribute to create a table
          row and fill it with the new values. Each time that its
          <literal>append()</literal> method is called, the actual row is
          committed to the output buffer and the row pointer is incremented to
          point to the next table record. When the buffer is full, the data is
          saved on disk, and the buffer is reused again for the next
          cycle.</para>

          <para><emphasis>Caveat emptor</emphasis>: Do not forget to always
          call the <literal>flush()</literal> method after a write operation,
          or else your tables will not be updated!</para>

          <para>Let's have a look at some rows in the modified table and
          verify that our new data has been appended:</para>

          <screen>>>> for r in table.iterrows():
     print "%-16s | %11.1f | %11.4g | %6d | %6d | %8d |" % \
         (r['name'], r['pressure'], r['energy'], r['grid_i'], r['grid_j'],
          r['TDCcount'])

Particle:      0 |         0.0 |           0 |      0 |     10 |        0 |
Particle:      1 |         1.0 |           1 |      1 |      9 |        1 |
Particle:      2 |         4.0 |         256 |      2 |      8 |        2 |
Particle:      3 |         9.0 |        6561 |      3 |      7 |        3 |
Particle:      4 |        16.0 |   6.554e+04 |      4 |      6 |        4 |
Particle:      5 |        25.0 |   3.906e+05 |      5 |      5 |        5 |
Particle:      6 |        36.0 |    1.68e+06 |      6 |      4 |        6 |
Particle:      7 |        49.0 |   5.765e+06 |      7 |      3 |        7 |
Particle:      8 |        64.0 |   1.678e+07 |      8 |      2 |        8 |
Particle:      9 |        81.0 |   4.305e+07 |      9 |      1 |        9 |
Particle:     10 |       100.0 |       1e+08 |     10 |      0 |       10 |
Particle:     11 |       121.0 |   2.144e+08 |     11 |     -1 |       11 |
Particle:     12 |       144.0 |     4.3e+08 |     12 |     -2 |       12 |
Particle:     13 |       169.0 |   8.157e+08 |     13 |     -3 |       13 |
Particle:     14 |       196.0 |   1.476e+09 |     14 |     -4 |       14 |</screen>
        </section>

        <section id="modifyingTableUsage">
          <title>Modifying data in tables</title>

          <para>Ok, until now, we've been only reading and writing (appending)
          values to our tables. But there are times that you need to modify
          your data once you have saved it on disk (this is specially true
          when you need to modify the real world data to adapt your goals ;).
          Let's see how we can modify the values that were saved in our
          existing tables. We will start modifying single cells in the first
          row of the <literal>Particle</literal> table:</para>

          <screen>>>> print "Before modif--&gt;", table[0]
Before modif--&gt; (0, 0, 0.0, 0, 10, 0L, 'Particle:      0', 0.0)
>>> table.cols.TDCcount[0] = 1
>>> print "After modifying first row of ADCcount--&gt;", table[0]
After modifying first row of ADCcount--&gt; (0, 1, 0.0, 0, 10, 0L, 'Particle:      0', 0.0)
>>> table.cols.energy[0] = 2
>>> print "After modifying first row of energy--&gt;", table[0]
After modifying first row of energy--&gt; (0, 1, 2.0, 0, 10, 0L, 'Particle:      0', 0.0)</screen>

          <para>We can modify complete ranges of columns as well:</para>

          <screen>>>> table.cols.TDCcount[2:5] = [2,3,4]
>>> print "After modifying slice [2:5] of TDCcount--&gt;", table[0:5]
After modifying slice [2:5] of TDCcount--&gt;
[(0, 1, 2.0, 0, 10, 0L, 'Particle:      0', 0.0)
 (256, 1, 1.0, 1, 9, 17179869184L, 'Particle:      1', 1.0)
 (512, 2, 256.0, 2, 8, 34359738368L, 'Particle:      2', 4.0)
 (768, 3, 6561.0, 3, 7, 51539607552L, 'Particle:      3', 9.0)
 (1024, 4, 65536.0, 4, 6, 68719476736L, 'Particle:      4', 16.0)]
>>> table.cols.energy[1:9:3] = [2,3,4]
>>> print "After modifying slice [1:9:3] of energy--&gt;", table[0:9]
After modifying slice [1:9:3] of energy--&gt;
[(0, 1, 2.0, 0, 10, 0L, 'Particle:      0', 0.0)
 (256, 1, 2.0, 1, 9, 17179869184L, 'Particle:      1', 1.0)
 (512, 2, 256.0, 2, 8, 34359738368L, 'Particle:      2', 4.0)
 (768, 3, 6561.0, 3, 7, 51539607552L, 'Particle:      3', 9.0)
 (1024, 4, 3.0, 4, 6, 68719476736L, 'Particle:      4', 16.0)
 (1280, 5, 390625.0, 5, 5, 85899345920L, 'Particle:      5', 25.0)
 (1536, 6, 1679616.0, 6, 4, 103079215104L, 'Particle:      6', 36.0)
 (1792, 7, 4.0, 7, 3, 120259084288L, 'Particle:      7', 49.0)
 (2048, 8, 16777216.0, 8, 2, 137438953472L, 'Particle:      8', 64.0)]</screen>

          <para>Check that the values have been correctly modified!
          <emphasis>Hint:</emphasis> remember that column
          <literal>TDCcount</literal> is the second one, and that
          <literal>energy</literal> is the third. Look for more info on
          modifying columns in <xref linkend="Column.__setitem__"
          xrefstyle="select: label" />.</para>

          <para>PyTables also lets you modify complete sets of rows at the
          same time. As a demonstration of these capability, see the next
          example:</para>

          <screen>>>> table.modifyRows(start=1, step=3,
                   rows=[(1, 2, 3.0, 4, 5, 6L, 'Particle:   None', 8.0),
                         (2, 4, 6.0, 8, 10, 12L, 'Particle: None*2', 16.0)])
2
>>> print "After modifying the complete third row--&gt;", table[0:5]
After modifying the complete third row--&gt;
[(0, 1, 2.0, 0, 10, 0L, 'Particle:      0', 0.0)
 (1, 2, 3.0, 4, 5, 6L, 'Particle:   None', 8.0)
 (512, 2, 256.0, 2, 8, 34359738368L, 'Particle:      2', 4.0)
 (768, 3, 6561.0, 3, 7, 51539607552L, 'Particle:      3', 9.0)
 (2, 4, 6.0, 8, 10, 12L, 'Particle: None*2', 16.0)]</screen>

          <para>As you can see, the <literal>modifyRows()</literal> call has
          modified the rows second and fifth, and it returned the number of
          modified rows.</para>

          <para>Apart of <literal>modifyRows()</literal>, there exists another
          method, called <literal>modifyColumn()</literal> to modify specific
          columns as well. Please check sections <xref
          linkend="Table.modifyRows" /> and <xref
          linkend="Table.modifyColumn" /> for a more in-depth description of
          them.</para>

          <para>Finally, it exists another way of modifying tables that is
          generally more handy than the described above. This new way uses the
          method <literal>update()</literal> (see <xref
          linkend="Row.update" />) of the <literal>Row</literal> instance that
          is attached to every table, so it is meant to be used in table
          iterators. Look at the next example:</para>

          <screen>>>> for row in table.where('TDCcount &lt;= 2'):
      row['energy'] = row['TDCcount']*2
      row.update()

>>> print "After modifying energy column (where TDCcount &lt;=2)--&gt;", table[0:4]
After modifying energy column (where TDCcount &lt;=2)--&gt;
[(0, 1, 2.0, 0, 10, 0L, 'Particle:      0', 0.0)
 (1, 2, 4.0, 4, 5, 6L, 'Particle:   None', 8.0)
 (512, 2, 4.0, 2, 8, 34359738368L, 'Particle:      2', 4.0)
 (768, 3, 6561.0, 3, 7, 51539607552L, 'Particle:      3', 9.0)]</screen>

          <para><emphasis>Note:</emphasis>The authors find this way of
          updating tables (i.e. using <literal>Row.update()</literal>) to be
          both convenient and efficient. Please make sure to use it
          extensively.</para>
        </section>

        <section id="modifyingArrayUsage">
          <title>Modifying data in arrays</title>

          <para>We are going now to see how to modify data in array objects.
          The basic way to do this is through the use of
          <literal>__setitem__</literal> special method (see <xref
          linkend="Array.__setitem__" />). Let's see at how modify data on the
          <literal>pressureObject</literal> array:</para>

          <screen>>>> pressureObject = h5file.root.columns.pressure
>>> print "Before modif--&gt;", pressureObject[:]
Before modif--&gt; [ 25.  36.  49.]
>>> pressureObject[0] = 2
>>> print "First modif--&gt;", pressureObject[:]
First modif--&gt; [  2.  36.  49.]
>>> pressureObject[1:3] = [2.1, 3.5]
>>> print "Second modif--&gt;", pressureObject[:]
Second modif--&gt; [ 2.   2.1  3.5]
>>> pressureObject[::2] = [1,2]
>>> print "Third modif--&gt;", pressureObject[:]
Third modif--&gt; [ 1.   2.1  2. ]</screen>

          <para>So, in general, you can use any combination of
          (multidimensional) extended slicing<footnote>
              <para>With the sole exception that you cannot use negative
              values for <literal>step</literal>.</para>
            </footnote> to refer to indexes that you want to modify. See <xref
          linkend="Array.__getitem__" xrefstyle="select: label" /> for
          more examples on how to use extended slicing in PyTables
          objects.</para>

          <para>Similarly, with and array of strings:</para>

          <screen>>>> nameObject = h5file.root.columns.name
>>> print "Before modif--&gt;", nameObject[:]
Before modif--&gt; ['Particle:      5', 'Particle:      6', 'Particle:      7']
>>> nameObject[0] = 'Particle:   None'
>>> print "First modif--&gt;", nameObject[:]
First modif--&gt; ['Particle:   None', 'Particle:      6', 'Particle:      7']
>>> nameObject[1:3] = ['Particle:      0', 'Particle:      1']
>>> print "Second modif--&gt;", nameObject[:]
Second modif--&gt; ['Particle:   None', 'Particle:      0', 'Particle:      1']
>>> nameObject[::2] = ['Particle:     -3', 'Particle:     -5']
>>> print "Third modif--&gt;", nameObject[:]
Third modif--&gt; ['Particle:     -3', 'Particle:      0', 'Particle:     -5']</screen>
        </section>

        <section>
          <title>And finally... how to delete rows from a table</title>

          <para>We'll finish this tutorial by deleting some rows from the
          table we have. Suppose that we want to delete the the 5th to 9th
          rows (inclusive):</para>

          <screen>>>> table.removeRows(5,10)
5</screen>

          <para><literal>removeRows(start, stop)</literal> (see <xref
          linkend="Table.removeRows" />) deletes the rows in the range (start,
          stop). It returns the number of rows effectively removed.</para>

          <para>We have reached the end of this first tutorial. Don't forget
          to close the file when you finish:</para>

          <screen>>>> h5file.close()
>>> ^D
$ </screen>

          <para>In <xref linkend="tutorial1-2-tableview"
          xrefstyle="select: label" /> you can see a graphical view of the
          PyTables file with the datasets we have just created. In <xref
          linkend="tutorial1-general" xrefstyle="select: label" /> are
          displayed the general properties of the table
          <literal>/detector/readout</literal>.</para>

          <figure id="tutorial1-2-tableview">
            <title>The final version of the data file for tutorial 1.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" fileref="tutorial1-2-tableview.png"
                           format="PNG" scale="75" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" fileref="tutorial1-2-tableview.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>

          <figure id="tutorial1-general">
            <title>General properties of the
            <literal>/detector/readout</literal> table.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" fileref="tutorial1-general.png"
                           format="PNG" scale="50" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" fileref="tutorial1-general.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>
      </section>

      <section id="secondExample">
        <title>Multidimensional table cells and automatic sanity
        checks</title>

        <para>Now it's time for a more real-life example (i.e. with errors in
        the code). We will create two groups that branch directly from the
        <literal>root</literal> node, <literal>Particles</literal> and
        <literal>Events</literal>. Then, we will put three tables in each
        group. In <literal>Particles</literal> we will put tables based on the
        <literal>Particle</literal> descriptor and in
        <literal>Events</literal>, the tables based the
        <literal>Event</literal> descriptor.</para>

        <para>Afterwards, we will provision the tables with a number of
        records. Finally, we will read the newly-created table
        <literal>/Events/TEvent3</literal> and select some values from it,
        using a comprehension list.</para>

        <para>Look at the next script (you can find it in
        <literal>examples/tutorial2.py</literal>). It appears to do all of the
        above, but it contains some small bugs. Note that this
        <literal>Particle</literal> class is not directly related to the one
        defined in last tutorial; this class is simpler (note, however, the
        <emphasis>multidimensional</emphasis> columns called
        <literal>pressure</literal> and
        <literal>temperature</literal>).</para>

        <para>We also introduce a new manner to describe a
        <literal>Table</literal> as a structured NumPy dtype (or even as a
        dictionary), as you can see in the <literal>Event</literal>
        description. See <xref linkend="createTableDescr" xrefstyle="select:
        label" /> about the different kinds of descriptor objects that can be
        passed to the <literal>createTable()</literal> method.</para>

        <screen>from tables import *
from numpy import *

# Describe a particle record
class Particle(IsDescription):
    name        = StringCol(itemsize=16)  # 16-character string
    lati        = Int32Col()              # integer
    longi       = Int32Col()              # integer
    pressure    = Float32Col(shape=(2,3)) # array of floats (single-precision)
    temperature = Float64Col(shape=(2,3)) # array of doubles (double-precision)

# Native NumPy dtype instances are also accepted
Event = dtype([
    ("name"     , "S16"),
    ("TDCcount" , uint8),
    ("ADCcount" , uint16),
    ("xcoord"   , float32),
    ("ycoord"   , float32)
    ])

# And dictionaries too (this defines the same structure as above)
# Event = {
#     "name"     : StringCol(itemsize=16),
#     "TDCcount" : UInt8Col(),
#     "ADCcount" : UInt16Col(),
#     "xcoord"   : Float32Col(),
#     "ycoord"   : Float32Col(),
#     }

# Open a file in "w"rite mode
fileh = openFile("tutorial2.h5", mode = "w")
# Get the HDF5 root group
root = fileh.root
# Create the groups:
for groupname in ("Particles", "Events"):
    group = fileh.createGroup(root, groupname)
# Now, create and fill the tables in Particles group
gparticles = root.Particles
# Create 3 new tables
for tablename in ("TParticle1", "TParticle2", "TParticle3"):
    # Create a table
    table = fileh.createTable("/Particles", tablename, Particle,
                              "Particles: "+tablename)
    # Get the record object associated with the table:
    particle = table.row
    # Fill the table with 257 particles
    for i in xrange(257):
        # First, assign the values to the Particle record
        particle['name'] = 'Particle: %6d' % (i)
        particle['lati'] = i
        particle['longi'] = 10 - i
        ########### Detectable errors start here. Play with them!
        particle['pressure'] = array(i*arange(2*3)).reshape((2,4))  # Incorrect
        #particle['pressure'] = array(i*arange(2*3)).reshape((2,3))  # Correct
        ########### End of errors
        particle['temperature'] = (i**2)     # Broadcasting
        # This injects the Record values
        particle.append()
    # Flush the table buffers
    table.flush()

# Now, go for Events:
for tablename in ("TEvent1", "TEvent2", "TEvent3"):
    # Create a table in Events group
    table = fileh.createTable(root.Events, tablename, Event,
                              "Events: "+tablename)
    # Get the record object associated with the table:
    event = table.row
    # Fill the table with 257 events
    for i in xrange(257):
        # First, assign the values to the Event record
        event['name']  = 'Event: %6d' % (i)
        event['TDCcount'] = i % (1&lt;&lt;8)   # Correct range
        ########### Detectable errors start here. Play with them!
        event['xcoor'] = float(i**2)     # Wrong spelling
        #event['xcoord'] = float(i**2)   # Correct spelling
        event['ADCcount'] = "sss"          # Wrong type
        #event['ADCcount'] = i * 2        # Correct type
        ########### End of errors
        event['ycoord'] = float(i)**4
        # This injects the Record values
        event.append()
    # Flush the buffers
    table.flush()

# Read the records from table "/Events/TEvent3" and select some
table = root.Events.TEvent3
e = [ p['TDCcount'] for p in table
      if p['ADCcount'] &lt; 20 and 4 &lt;= p['TDCcount'] &lt; 15 ]
print "Last record ==&gt;", p
print "Selected values ==&gt;", e
print "Total selected records ==&gt; ", len(e)
# Finally, close the file (this also will flush all the remaining buffers!)
fileh.close()</screen>

        <section>
          <title>Shape checking</title>

          <para>If you look at the code carefully, you'll see that it won't
          work. You will get the following error:</para>

          <screen>$ python tutorial2.py
Traceback (most recent call last):
  File "tutorial2.py", line 60, in ?
    particle['pressure'] = array(i*arange(2*3)).reshape((2,4))  # Incorrect
ValueError: total size of new array must be unchanged</screen>

          <para>This error indicates that you are trying to assign an array
          with an incompatible shape to a table cell. Looking at the source,
          we see that we were trying to assign an array of shape
          <literal>(2,4)</literal> to a <literal>pressure</literal> element,
          which was defined with the shape <literal>(2,3)</literal>.</para>

          <para>In general, these kinds of operations are forbidden, with one
          valid exception: when you assign a <emphasis>scalar</emphasis> value
          to a multidimensional column cell, all the cell elements are
          populated with the value of the scalar. For example:</para>

          <screen>particle['temperature'] = (i**2)    # Broadcasting</screen>

          <para>The value <literal>i**2</literal> is assigned to all the
          elements of the <literal>temperature</literal> table cell. This
          capability is provided by the <literal>NumPy</literal> package and
          is known as <emphasis>broadcasting</emphasis>.</para>
        </section>

        <section>
          <title>Field name checking</title>

          <para>After fixing the previous error and rerunning the program, we
          encounter another error:</para>

          <screen>$ python tutorial2.py
Traceback (most recent call last):
  File "tutorial2.py", line 73, in ?
    event['xcoor'] = float(i**2)     # Wrong spelling
  File "tableExtension.pyx", line 1094, in tableExtension.Row.__setitem__
  File "tableExtension.pyx", line 127, in tableExtension.getNestedFieldCache
  File "utilsExtension.pyx", line 331, in utilsExtension.getNestedField
KeyError: 'no such column: xcoor'</screen>

          <para>This error indicates that we are attempting to assign a value
          to a non-existent field in the <emphasis>event</emphasis> table
          object. By looking carefully at the <literal>Event</literal> class
          attributes, we see that we misspelled the <literal>xcoord</literal>
          field (we wrote <literal>xcoor</literal> instead). This is unusual
          behavior for Python, as normally when you assign a value to a
          non-existent instance variable, Python creates a new variable with
          that name. Such a feature can be dangerous when dealing with an
          object that contains a fixed list of field names. PyTables checks
          that the field exists and raises a <literal>KeyError</literal> if
          the check fails.</para>
        </section>

        <section>
          <title>Data type checking</title>

          <para>Finally, the last issue which we will find here is a
          <literal>TypeError</literal> exception:</para>

          <para><screen>$ python tutorial2.py
Traceback (most recent call last):
  File "tutorial2.py", line 75, in ?
    event['ADCcount'] = "sss"          # Wrong type
  File "tableExtension.pyx", line 1111, in tableExtension.Row.__setitem__
TypeError: invalid type (&lt;type 'str'&gt;) for column ``ADCcount``</screen>And,
          if we change the affected line to read:</para>

          <screen>event.ADCcount = i * 2        # Correct type</screen>

          <para>we will see that the script ends well.</para>

          <para>You can see the structure created with this (corrected) script
          in <xref linkend="tutorial2-tableview" xrefstyle="select: label" />.
          In particular, note the multidimensional column cells in table
          <literal>/Particles/TParticle2</literal>.</para>

          <figure id="tutorial2-tableview">
            <title>Table hierarchy for tutorial 2.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" fileref="tutorial2-tableview.png"
                           format="PNG" scale="75" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" fileref="tutorial2-tableview.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>
      </section>

      <section id="LinksTutorial">
        <title>Using links for more convenient access to nodes</title>

        <para>Links are special nodes that can be used to create additional
        paths to your existing nodes.  PyTables supports three kinds of links:
        hard links, soft links (aka symbolic links) and, if using HDF5 1.8.x,
        external links.</para>

        <para>Hard links let the user create additional paths to access
        another node in the same file, and once created, they are
        indistinguishable from the referred node object, except that they have
        different paths in the object tree.  For example, if the referred node
        is, say, a <literal>Table</literal> object, then the new hard link
        will become a <literal>Table</literal> object itself.  From this point
        on, you will be able to access the same <literal>Table</literal>
        object from two different paths: the original one and the new hard
        link path.  If you delete one path to the table, you will be able to
        reach it via the other path.</para>

        <para>Soft links are similar to hard links, but they keep their own
        personality.  When you create a soft link to another node, you will
        get a new <literal>SoftLink</literal> object
        that <emphasis>refers</emphasis> to that node.  However, in order to
        access the referred node, you need to <emphasis>dereference</emphasis>
        it.</para>

        <para>Finally, external links are like soft links, with the difference
        that these are meant to point to nodes
        in <emphasis>external</emphasis> files instead of nodes in the same
        file.  They are represented by the <literal>ExternalLink</literal>
        class and, like soft links, you need to dereference them in order to
        get access to the pointed node.</para>

        <section>
          <title>Interactive example</title>

          <para>Now we are going to learn how to deal with links. You can find
            the code used in this section
            in <literal>examples/links.py</literal>.
          </para>

          <para>First, let's create a file with some group structure:</para>

          <screen>>>> import tables as tb
>>> f1 = tb.openFile('links1.h5', 'w')
>>> g1 = f1.createGroup('/', 'g1')
>>> g2 = f1.createGroup(g1, 'g2')</screen>

          <para>Now, we will put some datasets on the <literal>/g1</literal>
            and <literal>/g1/g2</literal> groups:</para>

          <screen>>>> a1 = f1.createCArray(g1, 'a1', tb.Int64Atom(), shape=(10000,))
>>> t1 = f1.createTable(g2, 't1', {'f1': tb.IntCol(), 'f2': tb.FloatCol()})</screen>

          <para>We can start the party now.  We are going to create a new
            group, say <literal>/gl</literal>, where we will put our links and
            will start creating one hard link too:</para>

          <screen>>>> gl = f1.createGroup('/', 'gl')
>>> ht = f1.createHardLink(gl, 'ht', '/g1/g2/t1')  # ht points to t1
>>> print "``%s`` is a hard link to: ``%s``" % (ht, t1)
``/gl/ht (Table(0,)) ''`` is a hard link to: ``/g1/g2/t1 (Table(0,)) ''``</screen>

          <para>You can see how we've created a hard link
          in <literal>/gl/ht</literal> which is pointing to the existing table
          in <literal>/g1/g2/t1</literal>.  Have look at how the hard link is
          represented; it looks like a table, and actually, it is
          an <emphasis>real</emphasis> table.  We have two different paths to
          access that table, the original <literal>/g1/g2/t1</literal> and the
          new one <literal>/gl/ht</literal>.  If we remove the original path
          we still can reach the table by using the new path: </para>

          <screen>>>> t1.remove()
>>> print "table continues to be accessible in: ``%s``" % f1.getNode('/gl/ht')
table continues to be accessible in: ``/gl/ht (Table(0,)) ''``</screen>

          <para>So far so good. Now, let's create a couple of soft
          links:</para>

          <screen>>>> la1 = f1.createSoftLink(gl, 'la1', '/g1/a1')  # la1 points to a1
>>> print "``%s`` is a soft link to: ``%s``" % (la1, la1.target)
``/gl/la1 (SoftLink) -> /g1/a1`` is a soft link to: ``/g1/a1``
>>> lt = f1.createSoftLink(gl, 'lt', '/g1/g2/t1')  # lt points to t1
>>> print "``%s`` is a soft link to: ``%s``" % (lt, lt.target)
``/gl/lt (SoftLink) -> /g1/g2/t1 (dangling)`` is a soft link to: ``/g1/g2/t1``</screen>

          <para>Okay, we see how the first link <literal>/gl/la1</literal>
          points to the array <literal>/g1/a1</literal>.  Notice how the link
          prints as a <literal>SoftLink</literal>, and how the referred node
          is stored in the <literal>target</literal> instance attribute.  The
          second link (<literal>/gt/lt</literal>) pointing
          to <literal>/g1/g2/t1</literal> also has been created successfully,
          but by better inspecting the string representation of it, we see
          that is labeled as <literal>'(dangling)'</literal>.  Why is this?
          Well, you should remember that we recently removed
          the <literal>/g1/g2/t1</literal> path to access
          table <literal>t1</literal>.  When printing it, the object knows
          that it points to <emphasis>nowhere</emphasis> and reports this.
          This is a nice way to quickly know whether a soft link points to an
          exiting node or not.</para>

          <para>So, let's re-create the removed path to <literal>t1</literal>
          table:</para>

          <screen>>>> t1 = f1.createHardLink('/g1/g2', 't1', '/gl/ht')
>>> print "``%s`` is not dangling anymore" % (lt,)
``/gl/lt (SoftLink) -> /g1/g2/t1`` is not dangling anymore</screen>

          <para>and the soft link is pointing to an existing node now.</para>

          <para>Of course, for soft links to serve any actual purpose we need
          a way to get the pointed node.  It happens that soft links are
          callable, and that's the way to get the referred nodes back:</para>

          <screen>>>> plt = lt()
>>> print "dereferred lt node: ``%s``" % plt
dereferred lt node: ``/g1/g2/t1 (Table(0,)) ''``
>>> pla1 = la1()
>>> print "dereferred la1 node: ``%s``" % pla1
dereferred la1 node: ``/g1/a1 (CArray(10000,)) ''``</screen>

          <para>Now, <literal>plt</literal> is a Python reference to
          the <literal>t1</literal> table while <literal>pla1</literal> refers
          to the <literal>a1</literal> array.  Easy, uh?</para>

          <para>Let's suppose now that <literal>a1</literal> is an array whose
          access speed is critical for our application.  One possible solution
          is to move the entire file into a faster disk, say, a solid state
          disk so that access latencies can be reduced quite a lot.  However,
          it happens that our file is too big to fit into our shiny new
          (although small in capacity) SSD disk.  A solution is to copy just
          the <literal>a1</literal> array into a separate file that would fit
          into our SSD disk.  However, our application would be able to handle
          two files instead of only one, adding significantly more complexity,
          which is not a good thing.</para>

          <para>External links to the rescue!  As we've already said, external
          links are like soft links, but they are designed to link objects in
          external files.  Back to our problem, let's copy
          the <literal>a1</literal> array into a different file:</para>

          <screen>>>> f2 = tb.openFile('links2.h5', 'w')
>>> new_a1 = a1.copy(f2.root, 'a1')
>>> f2.close()  # close the other file</screen>

          <para>And now, we can remove the existing soft link and create the
          external link in its place:</para>

          <screen>>>> la1.remove()
>>> la1 = f1.createExternalLink(gl, 'la1', 'links2.h5:/a1')
/home/faltet/PyTables/pytables/branches/links/tables/file.py:971:
Incompat16Warning: external links are only supported when PyTables is compiled
against HDF5 1.8.x series and they, and their parent groups, are unreadable
with HDF5 1.6.x series.  You can set `warn16incompat` argument to false to
disable this warning.
  Incompat16Warning)</screen>

          <para>First, you should notice the warning when creating the
          external link: due to the need to change the format of the group
          containing an external link (introduced in HDF5 1.8), the parent
          groups (and hence, <emphasis>all</emphasis> its children) of
          external links are not readable with applications linked with HDF5
          1.6.x.  You should have this in mind if interoperability is
          important to you.  At any rate, you can disable the warning by
          setting the <literal>warn16incompat</literal> argument to true.
          See <literal>File.createExternalLink()</literal>
          <xref linkend="createExternalLinkDescr" xrefstyle="select: label" />
          for more info.</para>

          <para>But, when using HDF5 1.8.x (I'm supposing that you are using
          it for exercising this part of the tutorial), the external link is
          completely functional:</para>

          <screen>>>> print "``%s`` is an external link to: ``%s``" % (la1, la1.target)
``/gl/la1 (ExternalLink) -> links2.h5:/a1`` is an external link to: ``links2.h5:/a1``</screen>

          <para>Let's try dereferring it:</para>

          <screen>>>> new_a1 = la1()  # dereferrencing la1 returns a1 in links2.h5
>>> print "dereferred la1 node:  ``%s``" % new_a1
dereferred la1 node:  ``/a1 (CArray(10000,)) ''``</screen>

          <para>Well, it seems like we can access the external node.  But just
            to make sure that the node is in the other file:</para>

          <screen>>>> print "new_a1 file:", new_a1._v_file.filename
new_a1 file: links2.h5</screen>

          <para>Okay, the node is definitely in the external file.  So, you
          won't have to worry about your application: it will work exactly the
          same no matter the link is internal (soft) or external.</para>

          <para>Finally, here it is a dump of the objects in the final file,
            just to get a better idea of what we ended with:</para>

          <screen>>>> f1.close()
>>> exit()
$ ptdump links1.h5
/ (RootGroup) ''
/g1 (Group) ''
/g1/a1 (CArray(10000,)) ''
/gl (Group) ''
/gl/ht (Table(0,)) ''
/gl/la1 (ExternalLink) -> links2.h5:/a1
/gl/lt (SoftLink) -> /g1/g2/t1
/g1/g2 (Group) ''
/g1/g2/t1 (Table(0,)) ''</screen>

          <para>This ends this tutorial.  I hope it helped you to appreciate
          how useful links can be.  I'm sure you will find other ways in which
          you can use links that better fit your own needs.</para>

        </section>

      </section>

      <section id="UndoTutorial">
        <title>Exercising the Undo/Redo feature</title>

        <para>PyTables has integrated support for undoing and/or redoing
        actions. This functionality lets you put marks in specific places of
        your hierarchy manipulation operations, so that you can make your HDF5
        file pop back (<emphasis>undo</emphasis>) to a specific mark (for
        example for inspecting how your hierarchy looked at that point). You
        can also go forward to a more recent marker
        (<emphasis>redo</emphasis>). You can even do jumps to the marker you
        want using just one instruction as we will see shortly.</para>

        <para>You can undo/redo all the operations that are related to object
        tree management, like creating, deleting, moving or renaming nodes (or
        complete sub-hierarchies) inside a given object tree. You can also
        undo/redo operations (i.e. creation, deletion or modification) of
        persistent node attributes. However, when actions include
        <emphasis>internal</emphasis> modifications of datasets (that includes
        <literal>Table.append</literal>, <literal>Table.modifyRows</literal>
        or <literal>Table.removeRows</literal> among others), they cannot be
        undone/redone currently.</para>

        <para>This capability can be useful in many situations, like for
        example when doing simulations with multiple branches. When you have
        to choose a path to follow in such a situation, you can put a mark
        there and, if the simulation is not going well, you can go back to
        that mark and start another path. Other possible application is
        defining coarse-grained operations which operate in a
        transactional-like way, i.e. which return the database to its previous
        state if the operation finds some kind of problem while running. You
        can probably devise many other scenarios where the Undo/Redo feature
        can be useful to you <footnote>
            <para>You can even <emphasis>hide</emphasis> nodes temporarily.
            Will you be able to find out how?</para>
          </footnote>.</para>

        <section>
          <title>A basic example</title>

          <para>In this section, we are going to show the basic behavior of
          the Undo/Redo feature. You can find the code used in this example in
          <literal>examples/tutorial3-1.py</literal>. A somewhat more complex
          example will be explained in the next section.</para>

          <para>First, let's create a file:</para>

          <screen>>>> import tables
>>> fileh = tables.openFile("tutorial3-1.h5", "w", title="Undo/Redo demo 1")</screen>

          <para>And now, activate the Undo/Redo feature with the method
          <literal>enableUndo</literal> (see <xref
          linkend="File.enableUndo" />) of <literal>File</literal>:</para>

          <screen>>>> fileh.enableUndo()</screen>

          <para>From now on, all our actions will be logged internally by
          PyTables. Now, we are going to create a node (in this case an
          <literal>Array</literal> object):</para>

          <screen>>>> one = fileh.createArray('/', 'anarray', [3,4], "An array")</screen>

          <para>Now, mark this point:</para>

          <screen>>>> fileh.mark()
1</screen>

          <para>We have marked the current point in the sequence of actions.
          In addition, the <literal>mark()</literal> method has returned the
          identifier assigned to this new mark, that is 1 (mark #0 is reserved
          for the implicit mark at the beginning of the action log). In the
          next section we will see that you can also assign a
          <emphasis>name</emphasis> to a mark (see <xref
          linkend="File.mark" /> for more info on <literal>mark()</literal>).
          Now, we are going to create another array:</para>

          <screen>>>> another = fileh.createArray('/', 'anotherarray', [4,5], "Another array")</screen>

          <para>Right. Now, we can start doing funny things. Let's say that we
          want to pop back to the previous mark (that whose value was 1, do
          you remember?). Let's introduce the <literal>undo()</literal> method
          (see <xref linkend="File.undo" />):</para>

          <screen>>>> fileh.undo()</screen>

          <para>Fine, what do you think it happened? Well, let's have a look
          at the object tree:</para>

          <screen>>>> print fileh
tutorial3-1.h5 (File) 'Undo/Redo demo 1'
Last modif.: 'Tue Mar 13 11:43:55 2007'
Object Tree:
/ (RootGroup) 'Undo/Redo demo 1'
/anarray (Array(2,)) 'An array'</screen>

          <para>What happened with the <literal>/anotherarray</literal> node
          we've just created? You guess it, it has disappeared because it was
          created <emphasis>after</emphasis> the mark 1. If you are curious
          enough you may well ask where it has gone. Well, it has not been
          deleted completely; it has been just moved into a special, hidden,
          group of PyTables that renders it invisible and waiting for a chance
          to be reborn.</para>

          <para>Now, unwind once more, and look at the object tree:</para>

          <screen>>>> fileh.undo()
>>> print fileh
tutorial3-1.h5 (File) 'Undo/Redo demo 1'
Last modif.: 'Tue Mar 13 11:43:55 2007'
Object Tree:
/ (RootGroup) 'Undo/Redo demo 1'</screen>

          <para>Oops, <literal>/anarray</literal> has disappeared as well!.
          Don't worry, it will revisit us very shortly. So, you might be
          somewhat lost right now; in which mark are we?. Let's ask the
          <literal>getCurrentMark()</literal> method (see <xref
          linkend="File.getCurrentMark" />) in the file handler:</para>

          <screen>>>> print fileh.getCurrentMark()
0</screen>

          <para>So we are at mark #0, remember? Mark #0 is an implicit mark
          that is created when you start the log of actions when calling
          <literal>File.enableUndo()</literal>. Fine, but you are missing your
          too-young-to-die arrays. What can we do about that?
          <literal>File.redo()</literal> (see <xref linkend="File.redo" />) to
          the rescue:</para>

          <screen>>>> fileh.redo()
>>> print fileh
tutorial3-1.h5 (File) 'Undo/Redo demo 1'
Last modif.: 'Tue Mar 13 11:43:55 2007'
Object Tree:
/ (RootGroup) 'Undo/Redo demo 1'
/anarray (Array(2,)) 'An array'</screen>

          <para>Great! The <literal>/anarray</literal> array has come into
          life again. Just check that it is alive and well:</para>

          <screen>>>> fileh.root.anarray.read()
[3, 4]
>>> fileh.root.anarray.title
'An array'</screen>

          <para>Well, it looks pretty similar than in its previous life;
          what's more, it is exactly the same object!:</para>

          <screen>>>> fileh.root.anarray is one
True</screen>

          <para>It just was moved to the the hidden group and back again, but
          that's all! That's kind of fun, so we are going to do the same with
          <literal>/anotherarray</literal>:</para>

          <screen>>>> fileh.redo()
>>> print fileh
tutorial3-1.h5 (File) 'Undo/Redo demo 1'
Last modif.: 'Tue Mar 13 11:43:55 2007'
Object Tree:
/ (RootGroup) 'Undo/Redo demo 1'
/anarray (Array(2,)) 'An array'
/anotherarray (Array(2,)) 'Another array'</screen>

          <para>Welcome back, <literal>/anotherarray</literal>! Just a couple
          of sanity checks:</para>

          <screen>>>> assert fileh.root.anotherarray.read() == [4,5]
>>> assert fileh.root.anotherarray.title == "Another array"
>>> fileh.root.anotherarray is another
True</screen>

          <para>Nice, you managed to turn your data back into life.
          Congratulations! But wait, do not forget to close your action log
          when you don't need this feature anymore:</para>

          <screen>>>> fileh.disableUndo()</screen>

          <para>That will allow you to continue working with your data without
          actually requiring PyTables to keep track of all your actions, and
          more importantly, allowing your objects to die completely if they
          have to, not requiring to keep them anywhere, and hence saving
          process time and space in your database file.</para>
        </section>

        <section>
          <title>A more complete example</title>

          <para>Now, time for a somewhat more sophisticated demonstration of
          the Undo/Redo feature. In it, several marks will be set in different
          parts of the code flow and we will see how to jump between these
          marks with just one method call. You can find the code used in this
          example in <literal>examples/tutorial3-2.py</literal></para>

          <para>Let's introduce the first part of the code:</para>

          <screen>import tables

# Create an HDF5 file
fileh = tables.openFile('tutorial3-2.h5', 'w', title='Undo/Redo demo 2')

         #'-**-**-**-**-**-**- enable undo/redo log  -**-**-**-**-**-**-**-'
fileh.enableUndo()

# Start undoable operations
fileh.createArray('/', 'otherarray1', [3,4], 'Another array 1')
fileh.createGroup('/', 'agroup', 'Group 1')
# Create a 'first' mark
fileh.mark('first')
fileh.createArray('/agroup', 'otherarray2', [4,5], 'Another array 2')
fileh.createGroup('/agroup', 'agroup2', 'Group 2')
# Create a 'second' mark
fileh.mark('second')
fileh.createArray('/agroup/agroup2', 'otherarray3', [5,6], 'Another array 3')
# Create a 'third' mark
fileh.mark('third')
fileh.createArray('/', 'otherarray4', [6,7], 'Another array 4')
fileh.createArray('/agroup', 'otherarray5', [7,8], 'Another array 5')</screen>

          <para>You can see how we have set several marks interspersed in the
          code flow, representing different states of the database. Also, note
          that we have assigned <emphasis>names</emphasis> to these marks,
          namely <literal>'first'</literal>, <literal>'second'</literal> and
          <literal>'third'</literal>.</para>

          <para>Now, start doing some jumps back and forth in the states of
          the database:</para>

          <screen># Now go to mark 'first'
fileh.goto('first')
assert '/otherarray1' in fileh
assert '/agroup' in fileh
assert '/agroup/agroup2' not in fileh
assert '/agroup/otherarray2' not in fileh
assert '/agroup/agroup2/otherarray3' not in fileh
assert '/otherarray4' not in fileh
assert '/agroup/otherarray5' not in fileh
# Go to mark 'third'
fileh.goto('third')
assert '/otherarray1' in fileh
assert '/agroup' in fileh
assert '/agroup/agroup2' in fileh
assert '/agroup/otherarray2' in fileh
assert '/agroup/agroup2/otherarray3' in fileh
assert '/otherarray4' not in fileh
assert '/agroup/otherarray5' not in fileh
# Now go to mark 'second'
fileh.goto('second')
assert '/otherarray1' in fileh
assert '/agroup' in fileh
assert '/agroup/agroup2' in fileh
assert '/agroup/otherarray2' in fileh
assert '/agroup/agroup2/otherarray3' not in fileh
assert '/otherarray4' not in fileh
assert '/agroup/otherarray5' not in fileh</screen>

          <para>Well, the code above shows how easy is to jump to a certain
          mark in the database by using the <literal>goto()</literal> method
          (see <xref linkend="File.goto" />).</para>

          <para>There are also a couple of implicit marks for going to the
          beginning or the end of the saved states: 0 and -1. Going to mark #0
          means go to the beginning of the saved actions, that is, when method
          <literal>fileh.enableUndo()</literal> was called. Going to mark #-1
          means go to the last recorded action, that is the last action in the
          code flow.</para>

          <para>Let's see what happens when going to the end of the action
          log:</para>

          <screen># Go to the end
fileh.goto(-1)
assert '/otherarray1' in fileh
assert '/agroup' in fileh
assert '/agroup/agroup2' in fileh
assert '/agroup/otherarray2' in fileh
assert '/agroup/agroup2/otherarray3' in fileh
assert '/otherarray4' in fileh
assert '/agroup/otherarray5' in fileh
# Check that objects have come back to life in a sane state
assert fileh.root.otherarray1.read() == [3,4]
assert fileh.root.agroup.otherarray2.read() == [4,5]
assert fileh.root.agroup.agroup2.otherarray3.read() == [5,6]
assert fileh.root.otherarray4.read() == [6,7]
assert fileh.root.agroup.otherarray5.read() == [7,8]</screen>

          <para>Try yourself going to the beginning of the action log
          (remember, the mark #0) and check the contents of the object
          tree.</para>

          <para>We have nearly finished this demonstration. As always, do not
          forget to close the action log as well as the database:</para>

          <screen>#'-**-**-**-**-**-**- disable undo/redo log  -**-**-**-**-**-**-**-'
fileh.disableUndo()

# Close the file
fileh.close()</screen>

          <para>You might want to check other examples on Undo/Redo feature
          that appear in <literal>examples/undo-redo.py</literal>.</para>
        </section>
      </section>

      <section>
        <title>Using enumerated types</title>

        <para>PyTables includes support for handling enumerated types. Those
        types are defined by providing an exhaustive <emphasis>set</emphasis>
        or <emphasis>list</emphasis> of possible, named values for a variable
        of that type. Enumerated variables of the same type are usually
        compared between them for equality and sometimes for order, but are
        not usually operated upon.</para>

        <para>Enumerated values have an associated <emphasis>name</emphasis>
        and <emphasis>concrete value</emphasis>. Every name is unique and so
        are concrete values. An enumerated variable always takes the concrete
        value, not its name. Usually, the concrete value is not used directly,
        and frequently it is entirely irrelevant. For the same reason, an
        enumerated variable is not usually compared with concrete values out
        of its enumerated type. For that kind of use, standard variables and
        constants are more adequate.</para>

        <para>PyTables provides the <literal>Enum</literal> (see <xref
        linkend="EnumClassDescr" xrefstyle="select: label" />) class to
        provide support for enumerated types. Each instance of
        <literal>Enum</literal> is an enumerated type (or
        <emphasis>enumeration</emphasis>). For example, let us create an
        enumeration of colors<footnote>
            <para>All these examples can be found in
            <literal>examples/enum.py</literal>.</para>
          </footnote>:</para>

        <screen>>>> import tables
>>> colorList = ['red', 'green', 'blue', 'white', 'black']
>>> colors = tables.Enum(colorList)</screen>

        <para>Here we used a simple list giving the names of enumerated
        values, but we left the choice of concrete values up to the
        <literal>Enum</literal> class. Let us see the enumerated pairs to
        check those values:</para>

        <screen>>>> print "Colors:", [v for v in colors]
Colors: [('blue', 2), ('black', 4), ('white', 3), ('green', 1), ('red', 0)]</screen>

        <para>Names have been given automatic integer concrete values. We can
        iterate over the values in an enumeration, but we will usually be more
        interested in accessing single values. We can get the concrete value
        associated with a name by accessing it as an attribute or as an item
        (the later can be useful for names not resembling Python
        identifiers):</para>

        <screen>>>> print "Value of 'red' and 'white':", (colors.red, colors.white)
Value of 'red' and 'white': (0, 3)
>>> print "Value of 'yellow':", colors.yellow
Value of 'yellow':
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in ?
  File ".../tables/misc/enum.py", line 230, in __getattr__
    raise AttributeError(*ke.args)
AttributeError: no enumerated value with that name: 'yellow'
>>>
>>> print "Value of 'red' and 'white':", (colors['red'], colors['white'])
Value of 'red' and 'white': (0, 3)
>>> print "Value of 'yellow':", colors['yellow']
Value of 'yellow':
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in ?
  File ".../tables/misc/enum.py", line 189, in __getitem__
    raise KeyError("no enumerated value with that name: %r" % (name,))
KeyError: "no enumerated value with that name: 'yellow'"</screen>

        <para>See how accessing a value that is not in the enumeration raises
        the appropriate exception. We can also do the opposite action and get
        the name that matches a concrete value by using the
        <literal>__call__()</literal> method of
        <literal>Enum</literal>:</para>

        <screen>>>> print "Name of value %s:" % colors.red, colors(colors.red)
Name of value 0: red
>>> print "Name of value 1234:", colors(1234)
Name of value 1234:
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in ?
  File ".../tables/misc/enum.py", line 320, in __call__
    raise ValueError(
ValueError: no enumerated value with that concrete value: 1234</screen>

        <para>You can see what we made as using the enumerated type to
        <emphasis>convert</emphasis> a concrete value into a name in the
        enumeration. Of course, values out of the enumeration can not be
        converted.</para>

        <section>
          <title>Enumerated columns</title>

          <para>Columns of an enumerated type can be declared by using the
          <literal>EnumCol</literal> (see <xref linkend="ColClassDescr"
          xrefstyle="select: label" />) class. To see how this works, let us
          open a new PyTables file and create a table to collect the simulated
          results of a probabilistic experiment. In it, we have a bag full of
          colored balls; we take a ball out and annotate the time of
          extraction and the color of the ball.</para>

          <screen>>>> h5f = tables.openFile('enum.h5', 'w')
>>> class BallExt(tables.IsDescription):
      ballTime = tables.Time32Col()
      ballColor = tables.EnumCol(colors, 'black', base='uint8')

>>> tbl = h5f.createTable(
      '/', 'extractions', BallExt, title="Random ball extractions")
>>> </screen>

          <para>We declared the <literal>ballColor</literal> column to be of
          the enumerated type <literal>colors</literal>, with a default value
          of <literal>black</literal>. We also stated that we are going to
          store concrete values as unsigned 8-bit integer values<footnote>
              <para>In fact, only integer values are supported right now, but
              this may change in the future.</para>
            </footnote>.</para>

          <para>Let us use some random values to fill the table:</para>

          <screen>>>> import time
>>> import random
>>> now = time.time()
>>> row = tbl.row
>>> for i in range(10):
      row['ballTime'] = now + i
      row['ballColor'] = colors[random.choice(colorList)]  # notice this
      row.append()
>>> </screen>

          <para>Notice how we used the <literal>__getitem__()</literal> call
          of <literal>colors</literal> to get the concrete value to store in
          <literal>ballColor</literal>. You should know that this way of
          appending values to a table does automatically check for the
          validity on enumerated values. For instance:</para>

          <screen>>>> row['ballTime'] = now + 42
>>> row['ballColor'] = 1234
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "tableExtension.pyx", line 1086, in tableExtension.Row.__setitem__
  File ".../tables/misc/enum.py", line 320, in __call__
    "no enumerated value with that concrete value: %r" % (value,))
ValueError: no enumerated value with that concrete value: 1234</screen>

          <para>But take care that this check is <emphasis>only</emphasis>
          performed here and not in other methods such as
          <literal>tbl.append()</literal> or
          <literal>tbl.modifyRows()</literal>. Now, after flushing the table
          we can see the results of the insertions:</para>

          <screen>>>> tbl.flush()
>>> for r in tbl:
      ballTime = r['ballTime']
      ballColor = colors(r['ballColor'])  # notice this
      print "Ball extracted on %d is of color %s." % (ballTime, ballColor)

Ball extracted on 1173785568 is of color green.
Ball extracted on 1173785569 is of color black.
Ball extracted on 1173785570 is of color white.
Ball extracted on 1173785571 is of color black.
Ball extracted on 1173785572 is of color black.
Ball extracted on 1173785573 is of color red.
Ball extracted on 1173785574 is of color green.
Ball extracted on 1173785575 is of color red.
Ball extracted on 1173785576 is of color white.
Ball extracted on 1173785577 is of color white.</screen>

          <para>As a last note, you may be wondering how to have access to the
          enumeration associated with <literal>ballColor</literal> once the
          file is closed and reopened. You can call
          <literal>tbl.getEnum('ballColor')</literal> (see <xref
          linkend="Table.getEnum"/>) to get the enumeration back.</para>
        </section>

        <section>
          <title>Enumerated arrays</title>

          <para><literal>EArray</literal> and <literal>VLArray</literal>
          leaves can also be declared to store enumerated values by means of
          the <literal>EnumAtom</literal> (see <xref linkend="AtomClassDescr"
          xrefstyle="select: label" />) class, which works very much like
          <literal>EnumCol</literal> for tables. Also,
          <literal>Array</literal> leaves can be used to open native HDF
          enumerated arrays.</para>

          <para>Let us create a sample <literal>EArray</literal> containing
          ranges of working days as bidimensional values:</para>

          <screen>>>> workingDays = {'Mon': 1, 'Tue': 2, 'Wed': 3, 'Thu': 4, 'Fri': 5}
>>> dayRange = tables.EnumAtom(workingDays, 'Mon', base='uint16')
>>> earr = h5f.createEArray('/', 'days', dayRange, (0, 2), title="Working day ranges")
>>> earr.flavor = 'python'</screen>

          <para>Nothing surprising, except for a pair of details. In the first
          place, we use a <emphasis>dictionary</emphasis> instead of a list to
          explicitly set concrete values in the enumeration. In the second
          place, there is no explicit <literal>Enum</literal> instance
          created! Instead, the dictionary is passed as the first argument to
          the constructor of <literal>EnumAtom</literal>. If the constructor
          gets a list or a dictionary instead of an enumeration, it
          automatically builds the enumeration from it.</para>

          <para>Now let us feed some data to the array:</para>

          <screen>>>> wdays = earr.getEnum()
>>> earr.append([(wdays.Mon, wdays.Fri), (wdays.Wed, wdays.Fri)])
>>> earr.append([(wdays.Mon, 1234)])</screen>

          <para>Please note that, since we had no explicit
          <literal>Enum</literal> instance, we were forced to use
          <literal>getEnum()</literal> (see <xref linkend="EArrayMethodsDescr"
          xrefstyle="select: label" />) to get it from the array (we could
          also have used <literal>dayRange.enum</literal>).  Also note that we
          were able to append an invalid value (1234). Array methods do not
          check the validity of enumerated values.</para>

          <para>Finally, we will print the contents of the array:</para>

          <screen>>>> for (d1, d2) in earr:
      print "From %s to %s (%d days)." % (wdays(d1), wdays(d2), d2-d1+1)

From Mon to Fri (5 days).
From Wed to Fri (3 days).
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 2, in &lt;module&gt;
  File ".../tables/misc/enum.py", line 320, in __call__
    "no enumerated value with that concrete value: %r" % (value,))
ValueError: no enumerated value with that concrete value: 1234</screen>

          <para>That was an example of operating on concrete values. It also
          showed how the value-to-name conversion failed because of the value
          not belonging to the enumeration.</para>

          <para>Now we will close the file, and this little tutorial on
          enumerated types is done:</para>

          <screen>>>> h5f.close()</screen>
        </section>
      </section>

      <section>
        <title>Dealing with nested structures in tables</title>

        <para>PyTables supports the handling of nested structures (or nested
        datatypes, as you prefer) in table objects, allowing you to define
        arbitrarily nested columns.</para>

        <para>An example will clarify what this means. Let's suppose that you
        want to group your data in pieces of information that are more related
        than others pieces in your table, So you may want to tie them up
        together in order to have your table better structured but also be
        able to retrieve and deal with these groups more easily.</para>

        <para>You can create such a nested substructures by just nesting
        subclasses of <literal>IsDescription</literal>. Let's see one example
        (okay, it's a bit silly, but will serve for demonstration
        purposes):</para>

        <screen>from tables import *

class Info(IsDescription):
    """A sub-structure of Test"""
    _v_pos = 2   # The position in the whole structure
    name = StringCol(10)
    value = Float64Col(pos=0)

colors = Enum(['red', 'green', 'blue'])

class NestedDescr(IsDescription):
    """A description that has several nested columns"""
    color = EnumCol(colors, 'red', base='uint32')
    info1 = Info()
    class info2(IsDescription):
        _v_pos = 1
        name = StringCol(10)
        value = Float64Col(pos=0)
        class info3(IsDescription):
            x = Float64Col(dflt=1)
            y = UInt8Col(dflt=1)</screen>

        <para>The root class is <literal>NestedDescr</literal> and both
        <literal>info1</literal> and <literal>info2</literal> are
        <emphasis>substructures</emphasis> of it. Note how
        <literal>info1</literal> is actually an instance of the class
        <literal>Info</literal> that was defined prior to
        <literal>NestedDescr</literal>. Also, there is a third substructure,
        namely <literal>info3</literal> that hangs from the substructure
        <literal>info2</literal>. You can also define positions of
        substructures in the containing object by declaring the special class
        attribute <literal>_v_pos</literal>.</para>

        <section>
          <title>Nested table creation</title>

          <para>Now that we have defined our nested structure, let's create a
          <emphasis>nested</emphasis> table, that is a table with columns that
          contain other subcolumns.</para>

          <screen>>>> fileh = openFile("nested-tut.h5", "w")
>>> table = fileh.createTable(fileh.root, 'table', NestedDescr)</screen>

          <para>Done! Now, we have to feed the table with some values. The
          problem is how we are going to reference to the nested fields.
          That's easy, just use a <literal>'/'</literal> character to separate
          names in different nested levels. Look at this:</para>

          <screen>>>> row = table.row
>>> for i in range(10):
       row['color'] = colors[['red', 'green', 'blue'][i%3]]
       row['info1/name'] = "name1-%s" % i
       row['info2/name'] = "name2-%s" % i
       row['info2/info3/y'] =  i
       # All the rest will be filled with defaults
       row.append()

>>> table.flush()
>>> table.nrows
10</screen>

          <para>You see? In order to fill the fields located in the
          substructures, we just need to specify its full path in the table
          hierarchy.</para>
        </section>

        <section>
          <title>Reading nested tables</title>

          <para>Now, what happens if we want to read the table? What kind of
          data container will we get? Well, it's worth trying it:</para>

          <screen>>>> nra = table[::4]
>>> nra
array([(((1.0, 0), 'name2-0', 0.0), ('name1-0', 0.0), 0L),
       (((1.0, 4), 'name2-4', 0.0), ('name1-4', 0.0), 1L),
       (((1.0, 8), 'name2-8', 0.0), ('name1-8', 0.0), 2L)],
      dtype=[('info2', [('info3', [('x', '&gt;f8'), ('y', '|u1')]),
             ('name', '|S10'), ('value', '&gt;f8')]),
             ('info1', [('name', '|S10'), ('value', '&gt;f8')]),
             ('color', '&gt;u4')])</screen>

          <para>What we got is a NumPy array with a <emphasis>compound, nested
          datatype</emphasis> (its <literal>dtype</literal> is a list of
          name-datatype tuples). We read one row for each four in the table,
          giving a result of three rows.</para>

          <note>
            <para>When using the <literal>numarray</literal> flavor
            (deprecated), you will get an instance of the
            <literal>NestedRecArray</literal> class that lives in the
            <literal>tables.nra</literal> package.
            <literal>NestedRecArray</literal> is actually a subclass of the
            <literal>RecArray</literal> object of the
            <literal>numarray.records</literal> module. You can get more info
            about <literal>NestedRecArray</literal> object in <xref
            linkend="NestedRecArrayClassDescr"
            xrefstyle="select: label" />.</para>
          </note>

          <para>You can make use of the above object in many different ways.
          For example, you can use it to append new data to the existing table
          object:</para>

          <screen>>>> table.append(nra)
>>> table.nrows
13</screen>

          <para>Or, to create new tables:</para>

          <screen>>>> table2 = fileh.createTable(fileh.root, 'table2', nra)
>>> table2[:]
array([(((1.0, 0), 'name2-0', 0.0), ('name1-0', 0.0), 0L),
       (((1.0, 4), 'name2-4', 0.0), ('name1-4', 0.0), 1L),
       (((1.0, 8), 'name2-8', 0.0), ('name1-8', 0.0), 2L)],
      dtype=[('info2', [('info3', [('x', '&lt;f8'), ('y', '|u1')]),
             ('name', '|S10'), ('value', '&lt;f8')]),
	     ('info1', [('name', '|S10'), ('value', '&lt;f8')]),
	     ('color', '&lt;u4')])</screen>

          <para>Finally, we can select nested values that fulfill some
          condition:</para>

          <screen>>>> names = [ x['info2/name'] for x in table if x['color'] == colors.red ]
>>> names
['name2-0', 'name2-3', 'name2-6', 'name2-9', 'name2-0']</screen>

          <para>Note that the row accessor does not provide the natural naming
          feature, so you have to completely specify the path of your desired
          columns in order to reach them.</para>
        </section>

        <section>
          <title>Using Cols accessor</title>

          <para>We can use the <literal>cols</literal> attribute object (see
          <xref linkend="ColsClassDescr" xrefstyle="select: label" />) of the
          table so as to quickly access the info located in the interesting
          substructures:</para>

          <screen>>>> table.cols.info2[1:5]
array([((1.0, 1), 'name2-1', 0.0), ((1.0, 2), 'name2-2', 0.0),
       ((1.0, 3), 'name2-3', 0.0), ((1.0, 4), 'name2-4', 0.0)],
      dtype=[('info3', [('x', '&lt;f8'), ('y', '|u1')]), ('name', '|S10'),
             ('value', '&lt;f8')])</screen>

          <para>Here, we have made use of the cols accessor to access to the
          <emphasis>info2</emphasis> substructure and an slice operation to
          get access to the subset of data we were interested in; you probably
          have recognized the natural naming approach here. We can continue
          and ask for data in <emphasis>info3</emphasis> substructure:</para>

          <screen>>>> table.cols.info2.info3[1:5]
array([(1.0, 1), (1.0, 2), (1.0, 3), (1.0, 4)],
      dtype=[('x', '&lt;f8'), ('y', '|u1')])</screen>

          <para>You can also use the <literal>_f_col</literal> method to get a
          handler for a column:</para>

          <screen>>>> table.cols._f_col('info2')
/table.cols.info2 (Cols), 3 columns
  info3 (Cols(), Description)
  name (Column(), |S10)
  value (Column(), float64)</screen>

          <para>Here, you've got another <literal>Cols</literal> object
          handler because <emphasis>info2</emphasis> was a nested column. If
          you select a non-nested column, you will get a regular
          <literal>Column</literal> instance:</para>

          <screen>>>> table.cols._f_col('info2/info3/y')
/table.cols.info2.info3.y (Column(), uint8, idx=None)</screen>

          <para>To sum up, the <literal>cols</literal> accessor is a very
          handy and powerful way to access data in your nested tables. Don't
          be afraid of using it, specially when doing interactive work.</para>
        </section>

        <section>
          <title>Accessing meta-information of nested tables</title>

          <para>Tables have an attribute called <literal>description</literal>
          which points to an instance of the <literal>Description</literal>
          class (see <xref linkend="DescriptionClassDescr" xrefstyle="select:
          label" />) and is useful to discover different meta-information
          about table data.</para>

          <para>Let's see how it looks like:</para>

          <screen>>>> table.description
{
  "info2": {
    "info3": {
      "x": Float64Col(shape=(), dflt=1.0, pos=0),
      "y": UInt8Col(shape=(), dflt=1, pos=1)},
    "name": StringCol(itemsize=10, shape=(), dflt='', pos=1),
    "value": Float64Col(shape=(), dflt=0.0, pos=2)},
  "info1": {
    "name": StringCol(itemsize=10, shape=(), dflt='', pos=0),
    "value": Float64Col(shape=(), dflt=0.0, pos=1)},
  "color": EnumCol(enum=Enum({'blue': 2, 'green': 1, 'red': 0}), dflt='red',
                   base=UInt32Atom(shape=(), dflt=0), shape=(), pos=2)}</screen>

          <para>As you can see, it provides very useful information on both
          the formats and the structure of the columns in your table.</para>

          <para>This object also provides a natural naming approach to access
          to subcolumns metadata:</para>

          <screen>>>> table.description.info1
{
    "name": StringCol(itemsize=10, shape=(), dflt='', pos=0),
    "value": Float64Col(shape=(), dflt=0.0, pos=1)}
>>> table.description.info2.info3
{
      "x": Float64Col(shape=(), dflt=1.0, pos=0),
      "y": UInt8Col(shape=(), dflt=1, pos=1)}</screen>

          <para>There are other variables that can be interesting for
          you:</para>

          <screen>>>> table.description._v_nestedNames
[('info2', [('info3', ['x', 'y']), 'name', 'value']),
 ('info1', ['name', 'value']), 'color']
>>> table.description.info1._v_nestedNames
['name', 'value']</screen>

          <para><literal>_v_nestedNames</literal> provides the names of the
          columns as well as its structure. You can see that there are the
          same attributes for the different levels of the
          <literal>Description</literal> object, because the levels are
          <emphasis>also</emphasis> <literal>Description</literal> objects
          themselves.</para>

          <para>There is a special attribute, called
          <literal>_v_nestedDescr</literal>, that can be useful to create
          nested record arrays that imitate the structure of the table (or a
          subtable thereof):</para>

          <screen>>>> import numpy
>>> table.description._v_nestedDescr
[('info2', [('info3', [('x', '()f8'), ('y', '()u1')]), ('name', '()S10'),
 ('value', '()f8')]), ('info1', [('name', '()S10'), ('value', '()f8')]),
 ('color', '()u4')]
>>> numpy.rec.array(None, shape=0,
                    dtype=table.description._v_nestedDescr)
recarray([],
      dtype=[('info2', [('info3', [('x', '&gt;f8'), ('y', '|u1')]),
             ('name', '|S10'), ('value', '&gt;f8')]),
	     ('info1', [('name', '|S10'), ('value', '&gt;f8')]),
	     ('color', '&gt;u4')])
>>> numpy.rec.array(None, shape=0,
                    dtype=table.description.info2._v_nestedDescr)
recarray([],
      dtype=[('info3', [('x', '&gt;f8'), ('y', '|u1')]), ('name', '|S10'),
             ('value', '&gt;f8')])
>>> from tables import nra
>>> nra.array(None, descr=table.description._v_nestedDescr)
array(
[],
descr=[('info2', [('info3', [('x', '()f8'), ('y', '()u1')]),
       ('name', '()S10'), ('value', '()f8')]), ('info1', [('name', '()S10'),
       ('value', '()f8')]), ('color', '()u4')],
shape=0)</screen>

          <para>You can see we have created two equivalent arrays: one with
          NumPy (the first) and one with the <literal>nra</literal> package
          (the last). The later implements nested record arrays for
          <literal>numarray</literal> (see <xref
          linkend="NestedRecArrayClassDescr"
          xrefstyle="select: label" />).</para>

          <para>Finally, there is a special iterator of the
          <literal>Description</literal> class, called
          <literal>_f_walk</literal> that is able to return you the different
          columns of the table:</para>

          <screen>>>> for coldescr in table.description._f_walk():
      print "column--&gt;",coldescr

column--&gt; Description([('info2', [('info3', [('x', '()f8'), ('y', '()u1')]),
                       ('name', '()S10'), ('value', '()f8')]),
                       ('info1', [('name', '()S10'), ('value', '()f8')]),
                       ('color', '()u4')])
column--&gt; EnumCol(enum=Enum({'blue': 2, 'green': 1, 'red': 0}), dflt='red',
                            base=UInt32Atom(shape=(), dflt=0), shape=(), pos=2)
column--&gt; Description([('info3', [('x', '()f8'), ('y', '()u1')]), ('name', '()S10'),
                       ('value', '()f8')])
column--&gt; StringCol(itemsize=10, shape=(), dflt='', pos=1)
column--&gt; Float64Col(shape=(), dflt=0.0, pos=2)
column--&gt; Description([('name', '()S10'), ('value', '()f8')])
column--&gt; StringCol(itemsize=10, shape=(), dflt='', pos=0)
column--&gt; Float64Col(shape=(), dflt=0.0, pos=1)
column--&gt; Description([('x', '()f8'), ('y', '()u1')])
column--&gt; Float64Col(shape=(), dflt=1.0, pos=0)
column--&gt; UInt8Col(shape=(), dflt=1, pos=1)</screen>

          <para>See the <xref linkend="DescriptionClassDescr"
          xrefstyle="select: label" /> for the complete listing of attributes
          and methods of <literal>Description</literal>.</para>

          <para>Well, this is the end of this tutorial. As always, do not
          forget to close your files:</para>

          <screen>>>> fileh.close()</screen>

          <para>Finally, you may want to have a look at your resulting data
          file:</para>

          <screen>$ ptdump -d nested-tut.h5
/ (RootGroup) ''
/table (Table(13,)) ''
  Data dump:
[0] (((1.0, 0), 'name2-0', 0.0), ('name1-0', 0.0), 0L)
[1] (((1.0, 1), 'name2-1', 0.0), ('name1-1', 0.0), 1L)
[2] (((1.0, 2), 'name2-2', 0.0), ('name1-2', 0.0), 2L)
[3] (((1.0, 3), 'name2-3', 0.0), ('name1-3', 0.0), 0L)
[4] (((1.0, 4), 'name2-4', 0.0), ('name1-4', 0.0), 1L)
[5] (((1.0, 5), 'name2-5', 0.0), ('name1-5', 0.0), 2L)
[6] (((1.0, 6), 'name2-6', 0.0), ('name1-6', 0.0), 0L)
[7] (((1.0, 7), 'name2-7', 0.0), ('name1-7', 0.0), 1L)
[8] (((1.0, 8), 'name2-8', 0.0), ('name1-8', 0.0), 2L)
[9] (((1.0, 9), 'name2-9', 0.0), ('name1-9', 0.0), 0L)
[10] (((1.0, 0), 'name2-0', 0.0), ('name1-0', 0.0), 0L)
[11] (((1.0, 4), 'name2-4', 0.0), ('name1-4', 0.0), 1L)
[12] (((1.0, 8), 'name2-8', 0.0), ('name1-8', 0.0), 2L)
/table2 (Table(3,)) ''
  Data dump:
[0] (((1.0, 0), 'name2-0', 0.0), ('name1-0', 0.0), 0L)
[1] (((1.0, 4), 'name2-4', 0.0), ('name1-4', 0.0), 1L)
[2] (((1.0, 8), 'name2-8', 0.0), ('name1-8', 0.0), 2L)</screen>

          <para>Most of the code in this section is also available in
          <literal>examples/nested-tut.py</literal>.</para>

          <para>All in all, PyTables provides a quite comprehensive toolset to
          cope with nested structures and address your classification needs.
          However, caveat emptor, be sure to not nest your data too deeply or
          you will get inevitably messed interpreting too intertwined lists,
          tuples and description objects.</para>
        </section>
      </section>

      <section>
        <title>Other examples in PyTables distribution</title>

        <para>Feel free to examine the rest of examples in directory
        <literal>examples/</literal>, and try to understand them. We have
        written several practical sample scripts to give you an idea of the
        PyTables capabilities, its way of dealing with HDF5 objects, and how
        it can be used in the real world.</para>
      </section>
    </chapter>

    <chapter id="libraryReference">
      <title>Library Reference</title>

      <para></para>

      <para>PyTables implements several classes to represent the different
      nodes in the object tree. They are named <literal>File</literal>,
      <literal>Group</literal>, <literal>Leaf</literal>,
      <literal>Table</literal>, <literal>Array</literal>,
      <literal>CArray</literal>, <literal>EArray</literal>,
      <literal>VLArray</literal> and <literal>UnImplemented</literal>. Another
      one allows the user to complement the information on these different
      objects; its name is <literal>AttributeSet</literal>. Finally, another
      important class called <literal>IsDescription</literal> allows to build
      a <literal>Table</literal> record description by declaring a subclass of
      it. Many other classes are defined in PyTables, but they can be regarded
      as helpers whose goal is mainly to declare the <emphasis>data type
      properties</emphasis> of the different first class objects and will be
      described at the end of this chapter as well.</para>

      <para>An important function, called <literal>openFile</literal> is
      responsible to create, open or append to files. In addition, a few
      utility functions are defined to guess if the user supplied file is a
      <emphasis>PyTables</emphasis> or <emphasis>HDF5</emphasis> file. These
      are called <literal>isPyTablesFile()</literal> and
      <literal>isHDF5File()</literal>, respectively. There exists also a
      function called <literal>whichLibVersion()</literal> that informs about
      the versions of the underlying C libraries (for example, HDF5 or
      <literal>Zlib</literal>) and another called
      <literal>print_versions()</literal> that prints all the versions of the
      software that PyTables relies on. Finally, <literal>test()</literal>
      lets you run the complete test suite from a Python console
      interactively.</para>

      <para>Let's start discussing the first-level variables and functions
      available to the user, then the different classes defined in
      PyTables.</para>

      <section>
        <title><literal>tables</literal> variables and functions</title>

        <section>
          <title>Global variables</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis
              role="bold">__version__</emphasis></glossterm>

              <glossdef>
                <para>The PyTables version number.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">hdf5Version</emphasis></glossterm>

              <glossdef>
                <para>The underlying HDF5 library version number.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">is_pro</emphasis></glossterm>

              <glossdef>
                <para>True for PyTables Professional edition, false
                otherwise.</para>

                <note>
                    <para>PyTables Professional edition has been released
                    under an open source license. Starting with version 2.3,
                    PyTables includes all features of PyTables Pro.</para>

                    <para>In order to reflect the presence of advanced
                    features <emphasis role="bold">is_pro</emphasis> is always
                    set to True.</para>

                    <para><emphasis role="bold">is_pro</emphasis> should be
                    considered <emphasis role="bold">deprecated</emphasis>.
                    It will be removed in the next major release.</para>
                </note>

              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section id="GlobalFunctDescr">
          <title>Global functions</title>

          <section id="copyFileDescr" xreflabel="description">
            <title>copyFile(srcfilename, dstfilename, overwrite=False,
            **kwargs)</title>

            <para>An easy way of copying one PyTables file to another.</para>

            <para>This function allows you to copy an existing PyTables file
            named <literal>srcfilename</literal> to another file called
            <literal>dstfilename</literal>. The source file must exist and be
            readable. The destination file can be overwritten in place if
            existing by asserting the <literal>overwrite</literal>
            argument.</para>

            <para>This function is a shorthand for the
            <literal>File.copyFile()</literal> method, which acts on an
            already opened file. <literal>kwargs</literal> takes keyword
            arguments used to customize the copying process. See the
            documentation of <literal>File.copyFile()</literal> (see <xref
            linkend="File.copyFile" />) for a description of those
            arguments.</para>
          </section>

          <section id="isHDF5FileDescr" xreflabel="description">
            <title>isHDF5File(filename)</title>

            <para>Determine whether a file is in the HDF5 format.</para>

            <para>When successful, it returns a true value if the file is an
            HDF5 file, false otherwise. If there were problems identifying the
            file, an <literal>HDF5ExtError</literal> is raised.</para>
          </section>

          <section id="isPyTablesFileDescr" xreflabel="description">
            <title>isPyTablesFile(filename)</title>

            <para>Determine whether a file is in the PyTables format.</para>

            <para>When successful, it returns the format version string if the
            file is a PyTables file, <literal>None</literal> otherwise.  If
            there were problems identifying the file,
            an <literal>HDF5ExtError</literal> is raised.</para>

          </section>

          <section id="lrange" xreflabel="description">
            <title>lrange([start, ]stop[, step])</title>

            <para>Iterate over long ranges.</para>

            <para>This is similar to <literal>xrange()</literal>, but it
            allows 64-bit arguments on all platforms.  The results of the
            iteration are sequentially yielded in the form of
            <literal>numpy.int64</literal> values, but getting random
            individual items is not supported.</para>

            <para>Because of the Python 32-bit limitation on object lengths,
            the <literal>length</literal> attribute (which is also a
            <literal>numpy.int64</literal> value) should be used instead of
            the <literal>len()</literal> syntax.</para>

            <para>Default <literal>start</literal> and <literal>step</literal>
            arguments are supported in the same way as in
            <literal>xrange()</literal>.  When the standard
            <literal>[x]range()</literal> Python objects support 64-bit
            arguments, this iterator will be deprecated.</para>
          </section>

          <section id="openFileDescr" xreflabel="description">
            <title>openFile(filename, mode='r', title='', rootUEP="/",
            filters=None, **kwargs)</title>

            <para>Open a PyTables (or generic HDF5) file and return a
            <literal>File</literal> object.</para>

            <para>Arguments:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis
                role="bold">filename</emphasis></glossterm>

                <glossdef>
                  <para>The name of the file (supports environment variable
                  expansion). It is suggested that file names have any of the
                  <literal>.h5</literal>, <literal>.hdf</literal> or
                  <literal>.hdf5</literal> extensions, although this is not
                  mandatory.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">mode</emphasis></glossterm>

                <glossdef>
                  <para>The mode to open the file. It can be one of the
                  following:</para>

                  <glosslist>
                    <glossentry>
                      <glossterm><emphasis
                      role="bold">'r'</emphasis></glossterm>

                      <glossdef>
                        <para>Read-only; no data can be modified.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">'w'</emphasis></glossterm>

                      <glossdef>
                        <para>Write; a new file is created (an existing file
                        with the same name would be deleted).</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">'a'</emphasis></glossterm>

                      <glossdef>
                        <para>Append; an existing file is opened for reading
                        and writing, and if the file does not exist it is
                        created.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">'r+'</emphasis></glossterm>

                      <glossdef>
                        <para>It is similar to <literal>'a'</literal>, but the
                        file must already exist.</para>
                      </glossdef>
                    </glossentry>
                  </glosslist>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">title</emphasis></glossterm>

                <glossdef>
                  <para>If the file is to be created, a
                  <literal>TITLE</literal> string attribute will be set on the
                  root group with the given value. Otherwise, the title will
                  be read from disk, and this will not have any effect.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">rootUEP</emphasis></glossterm>

                <glossdef>
                  <para>The root User Entry Point. This is a group in the HDF5
                  hierarchy which will be taken as the starting point to
                  create the object tree. It can be whatever existing group in
                  the file, named by its HDF5 path. If it does not exist, an
                  <literal>HDF5ExtError</literal> is issued. Use this if you
                  do not want to build the <emphasis>entire</emphasis> object
                  tree, but rather only a <emphasis>subtree</emphasis> of
                  it.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">filters</emphasis></glossterm>

                <glossdef>
                  <para>An instance of the <literal>Filters</literal> (see
                  <xref linkend="FiltersClassDescr"
                  xrefstyle="select: label" />) class that provides
                  information about the desired I/O filters applicable to the
                  leaves that hang directly from the <emphasis>root
                  group</emphasis>, unless other filter properties are
                  specified for these leaves. Besides, if you do not specify
                  filter properties for child groups, they will inherit these
                  ones, which will in turn propagate to child nodes.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>In addition, it recognizes the names of parameters present
            in <literal>tables/parameters.py</literal> as additional keyword
            arguments.  See
            <xref linkend="parametersFiles" xrefstyle="select: label" /> for a
            detailed info on the supported parameters.</para>

            <note>
              <para>If you need to deal with a large number of nodes in an
                efficient way, please see <xref linkend="LRUOptim"
                xrefstyle="select: label" /> for more info and advices about
                the integrated node cache engine.</para>
            </note>

          </section>

          <section id="setBloscMaxThreads" xreflabel="description">
            <title>setBloscMaxThreads(nthreads)</title>

            <para>Set the maximum number of threads that Blosc can use.</para>

            <para>This actually overrides the <literal>MAX_THREADS</literal>
              setting in <literal>tables/parameters.py</literal>, so the new
              value will be effective until this function is called again or a
              new file with a different <literal>MAX_THREADS</literal> value
              is specified.
            </para>

            <para>Returns the previous setting for maximum threads.</para>

          </section>

          <section id="print_versions" xreflabel="description">
            <title>print_versions()</title>

            <para>Print all the versions of software that PyTables relies
            on.</para>
          </section>

          <section id="restrict_flavors" xreflabel="description">
            <title>restrict_flavors(keep=['python'])</title>

            <para>Disable all flavors except those in
            <literal>keep</literal>.</para>

            <para>Providing an empty <literal>keep</literal> sequence implies
            disabling all flavors (but the internal one).  If the sequence is
            not specified, only optional flavors are disabled.</para>

            <important>
              <para>Once you disable a flavor, it can not be enabled
              again.</para>
            </important>
          </section>

          <section>
            <title>split_type(type)</title>

            <para>Split a PyTables <literal>type</literal> into a PyTables
            kind and an item size.</para>

            <para>Returns a tuple of <literal>(kind, itemsize)</literal>. If
            no item size is present in the <literal>type</literal> (in the
            form of a precision), the returned item size is
            <literal>None</literal>.</para>

            <screen>>>> split_type('int32')
('int', 4)
>>> split_type('string')
('string', None)
>>> split_type('int20')
Traceback (most recent call last):
  ...
ValueError: precision must be a multiple of 8: 20
>>> split_type('foo bar')
Traceback (most recent call last):
  ...
ValueError: malformed type: 'foo bar'</screen>
          </section>

          <section id="testDescr" xreflabel="description">
            <title>test(verbose=False, heavy=False)</title>

            <para>Run all the tests in the test suite.</para>

            <para>If <literal>verbose</literal> is set, the test suite will
            emit messages with full verbosity (not recommended unless you are
            looking into a certain problem).</para>

            <para>If <literal>heavy</literal> is set, the test suite will be
            run in <emphasis>heavy</emphasis> mode (you should be careful with
            this because it can take a lot of time and resources from your
            computer).</para>
          </section>

          <section id="whichLibVersionDescr" xreflabel="description">
            <title>whichLibVersion(name)</title>

            <para>Get version information about a C library.</para>

            <para>If the library indicated by <literal>name</literal> is
            available, this function returns a 3-tuple containing the major
            library version as an integer, its full version as a string, and
            the version date as a string. If the library is not available,
            <literal>None</literal> is returned.</para>

            <para>The currently supported library names are
            <literal>hdf5</literal>, <literal>zlib</literal>,
            <literal>lzo</literal> and <literal>bzip2</literal>. If another
            name is given, a <literal>ValueError</literal> is raised.</para>
          </section>
        </section>
      </section>

      <section id="FileClassDescr">
        <title>The <literal>File</literal> class</title>

        <para>In-memory representation of a PyTables file.</para>

        <para>An instance of this class is returned when a PyTables file is
        opened with the <literal>openFile()</literal> (see <xref
        linkend="openFileDescr"/>) function. It offers methods to manipulate
        (create, rename, delete...) nodes and handle their attributes, as well
        as methods to traverse the object tree. The <emphasis>user entry
        point</emphasis> to the object tree attached to the HDF5 file is
        represented in the <literal>rootUEP</literal> attribute. Other
        attributes are available.</para>

        <para><literal>File</literal> objects support an <emphasis>Undo/Redo
        mechanism</emphasis> which can be enabled with the
        <literal>enableUndo()</literal> (see <xref
        linkend="File.enableUndo"/>) method. Once the Undo/Redo mechanism is
        enabled, explicit <emphasis>marks</emphasis> (with an optional unique
        name) can be set on the state of the database using the
        <literal>mark()</literal> (see <xref linkend="File.mark"/>)
        method. There are two implicit marks which are always available: the
        initial mark (0) and the final mark (-1).  Both the identifier of a
        mark and its name can be used in <emphasis>undo</emphasis> and
        <emphasis>redo</emphasis> operations.</para>

        <para>Hierarchy manipulation operations (node creation, movement and
        removal) and attribute handling operations (setting and deleting) made
        after a mark can be undone by using the <literal>undo()</literal> (see
        <xref linkend="File.undo"/>) method, which returns the database to the
        state of a past mark. If <literal>undo()</literal> is not followed by
        operations that modify the hierarchy or attributes, the
        <literal>redo()</literal> (see <xref linkend="File.redo"/>) method can
        be used to return the database to the state of a future mark. Else,
        future states of the database are forgotten.</para>

        <para>Note that data handling operations can not be undone nor redone
        by now. Also, hierarchy manipulation operations on nodes that do not
        support the Undo/Redo mechanism issue an
        <literal>UndoRedoWarning</literal> <emphasis>before</emphasis>
        changing the database.</para>

        <para>The Undo/Redo mechanism is persistent between sessions and can
        only be disabled by calling the <literal>disableUndo()</literal> (see
        <xref linkend="File.disableUndo"/>) method.</para>

        <para>File objects can also act as context managers when using the
        <literal>with</literal> statement introduced in Python 2.5.  When
        exiting a context, the file is automatically closed.</para>

        <section>
          <title><literal>File</literal> instance variables</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">filename</emphasis></glossterm>

              <glossdef>
                <para>The name of the opened file.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">filters</emphasis></glossterm>

              <glossdef>
                <para>Default filter properties for the root group (see <xref
                linkend="FiltersClassDescr" xrefstyle="select: label"
                />).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">format_version</emphasis></glossterm>

              <glossdef>
                <para>The PyTables version number of this file.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">isopen</emphasis></glossterm>

              <glossdef>
                <para>True if the underlying file is open, false
                otherwise.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">mode
              </emphasis></glossterm>

              <glossdef>
                <para>The mode in which the file was opened.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">open_count</emphasis></glossterm>

              <glossdef>
                <para>The number of times this file has been opened
                currently.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">root</emphasis></glossterm>

              <glossdef>
                <para>The <emphasis>root</emphasis> of the object tree
                hierarchy (a <literal>Group</literal> instance).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">rootUEP</emphasis></glossterm>

              <glossdef>
                <para>The UEP (user entry point) group name in the file (see
                the <literal>openFile()</literal> function in <xref
                linkend="openFileDescr" xrefstyle="select: label" />).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">title
              </emphasis></glossterm>

              <glossdef>
                <para>The title of the root group in the file.</para>
              </glossdef>
            </glossentry>

          </glosslist>
        </section>

        <section id="FileMethods_file_handling">
          <title><literal>File</literal> methods — file handling</title>

          <section>
            <title>close()</title>

            <para>Flush all the alive leaves in object tree and close the
            file.</para>
          </section>

          <section id="File.copyFile" xreflabel="description">
            <title>copyFile(dstfilename, overwrite=False, **kwargs)</title>

            <para>Copy the contents of this file to
            <literal>dstfilename</literal>.</para>

            <para><literal>dstfilename</literal> must be a path string
            indicating the name of the destination file. If it already exists,
            the copy will fail with an <literal>IOError</literal>, unless the
            <literal>overwrite</literal> argument is true, in which case the
            destination file will be overwritten in place. In this last case,
            the destination file should be closed or ugly errors will
            happen.</para>

            <para>Additional keyword arguments may be passed to customize the
            copying process. For instance, title and filters may be changed,
            user attributes may be or may not be copied, data may be
            sub-sampled, stats may be collected, etc. Arguments unknown to
            nodes are simply ignored. Check the documentation for copying
            operations of nodes to see which options they support.</para>

            <para>In addition, it recognizes the names of parameters present
            in <literal>tables/parameters.py</literal> as additional keyword
            arguments.  See
            <xref linkend="parametersFiles" xrefstyle="select: label" /> for a
            detailed info on the supported parameters.</para>

            <para>Copying a file usually has the beneficial side effect of
            creating a more compact and cleaner version of the original
            file.</para>
          </section>

          <section>
            <title>flush()</title>

            <para>Flush all the alive leaves in the object tree.</para>
          </section>

          <section>
            <title>fileno()</title>

            <para>Return the underlying OS integer file descriptor.</para>

            <para>This is needed for lower-level file interfaces, such as the
            <literal>fcntl</literal> module.</para>
          </section>

          <section id="File.__enter__" xreflabel="description">
            <title>__enter__()</title>

            <para>Enter a context and return the same file.</para>
          </section>

          <section id="File.__exit__" xreflabel="description">
            <title>__exit__([*exc_info])</title>

            <para>Exit a context and close the file.</para>
          </section>

          <section id="File.__str__" xreflabel="description">
            <title>__str__()</title>

            <para>Return a short string representation of the object
            tree.</para>

            <para>Example of use:</para>

            <screen>>>> f = tables.openFile('data/test.h5')
>>> print f
data/test.h5 (File) 'Table Benchmark'
Last modif.: 'Mon Sep 20 12:40:47 2004'
Object Tree:
/ (Group) 'Table Benchmark'
/tuple0 (Table(100,)) 'This is the table title'
/group0 (Group) ''
/group0/tuple1 (Table(100,)) 'This is the table title'
/group0/group1 (Group) ''
/group0/group1/tuple2 (Table(100,)) 'This is the table title'
/group0/group1/group2 (Group) ''</screen>
          </section>

          <section id="File.__repr__" xreflabel="description">
            <title>__repr__()</title>

            <para>Return a detailed string representation of the object
            tree.</para>
          </section>
        </section>

        <section id="FileMethods_hierarchy_manipulation">
          <title><literal>File</literal> methods — hierarchy manipulation</title>

          <section id="File.copyChildren" xreflabel="description">
            <title>copyChildren(srcgroup, dstgroup, overwrite=False,
            recursive=False, createparents=False, **kwargs)</title>

            <para>Copy the children of a group into another group.</para>

            <para>This method copies the nodes hanging from the source group
            <literal>srcgroup</literal> into the destination group
            <literal>dstgroup</literal>. Existing destination nodes can be
            replaced by asserting the <literal>overwrite</literal> argument.
            If the <literal>recursive</literal> argument is true, all
            descendant nodes of <literal>srcnode</literal> are recursively
            copied. If <literal>createparents</literal> is true, the needed
            groups for the given destination parent group path to exist will
            be created.</para>

            <para><literal>kwargs</literal> takes keyword arguments used to
            customize the copying process. See the documentation of
            <literal>Group._f_copyChildren()</literal> (see <xref
            linkend="Group._f_copyChildren" />) for a description of those
            arguments.</para>
          </section>

          <section id="File.copyNode" xreflabel="description">
            <title>copyNode(where, newparent=None, newname=None, name=None,
            overwrite=False, recursive=False, createparents=False,
            **kwargs)</title>

            <para>Copy the node specified by <literal>where</literal> and
            <literal>name</literal> to
            <literal>newparent/newname</literal>.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where,
                name</emphasis></glossterm>

                <glossdef>
                  <para>These arguments work as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">newparent</emphasis></glossterm>

                <glossdef>
                  <para>The destination group that the node will be copied
                  into (a path name or a <literal>Group</literal>
                  instance). If not specified or <literal>None</literal>, the
                  current parent group is chosen as the new parent.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">newname</emphasis></glossterm>

                <glossdef>
                  <para>The name to be assigned to the new copy in its
                  destination (a string).  If it is not specified or
                  <literal>None</literal>, the current name is chosen as the
                  new name.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>Additional keyword arguments may be passed to customize the
            copying process. The supported arguments depend on the kind of
            node being copied. See <literal>Group._f_copy()</literal> (<xref
            linkend="Group._f_copy"/>) and <literal>Leaf.copy()</literal>
            (<xref linkend="Leaf.copy"/>) for more information on their
            allowed keyword arguments.</para>

            <para>This method returns the newly created copy of the source
            node (i.e. the destination node).  See
            <literal>Node._f_copy()</literal> (<xref linkend="Node._f_copy"/>)
            for further details on the semantics of copying nodes.</para>
          </section>

          <section id="createArrayDescr" xreflabel="description">
            <title>createArray(where, name, object, title='', byteorder=None,
            createparents=False)</title>

            <para>Create a new array with the given <literal>name</literal> in
            <literal>where</literal> location.  See the
            <literal>Array</literal> class (in <xref linkend="ArrayClassDescr"
            xrefstyle="select: label" />) for more information on
            arrays.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">object</emphasis></glossterm>

                <glossdef>
                  <para>The array or scalar to be saved.  Accepted types are
                  NumPy arrays and scalars, <literal>numarray</literal> arrays
                  and string arrays (deprecated), Numeric arrays and scalars
                  (deprecated), as well as native Python sequences and scalars,
                  provided that values are regular (i.e. they are not like
                  <literal>[[1,2],2]</literal>) and homogeneous (i.e. all the
                  elements are of the same type).</para>

                  <para>Also, objects that have some of their dimensions equal
                  to 0 are not supported (use an <literal>EArray</literal>
                  node (see <xref linkend="EArrayClassDescr"
                  xrefstyle="select: label" />) if you want to store an array
                  with one of its dimensions equal to 0).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">byteorder</emphasis></glossterm>

                <glossdef>
                  <para>The byteorder of the data <emphasis>on
                  disk</emphasis>, specified as <literal>'little'</literal> or
                  <literal>'big'</literal>.  If this is not specified, the
                  byteorder is that of the given
                  <literal>object</literal>.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>See <literal>File.createTable()</literal> (<xref
            linkend="createTableDescr" xrefstyle="select: label" />) for more
            information on the rest of parameters.</para>
          </section>

          <section id="createCArrayDescr" xreflabel="description">
            <title>createCArray(where, name, atom, shape, title='',
            filters=None, chunkshape=None, byteorder=None,
            createparents=False)</title>

            <para>Create a new chunked array with the given
            <literal>name</literal> in <literal>where</literal> location.  See
            the <literal>CArray</literal> class (in <xref
            linkend="CArrayClassDescr" xrefstyle="select: label" />) for more
            information on chunked arrays.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">atom</emphasis></glossterm>

                <glossdef>
                  <para>An <literal>Atom</literal> (see <xref
                  linkend="AtomClassDescr" xrefstyle="select: label" />)
                  instance representing the <emphasis>type</emphasis> and
                  <emphasis>shape</emphasis> of the atomic objects to be
                  saved.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">shape</emphasis></glossterm>

                <glossdef>
                  <para>The shape of the new array.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">chunkshape</emphasis></glossterm>

                <glossdef>
                  <para>The shape of the data chunk to be read or written in a
                  single HDF5 I/O operation.  Filters are applied to those
                  chunks of data.  The dimensionality of
                  <literal>chunkshape</literal> must be the same as that of
                  <literal>shape</literal>.  If <literal>None</literal>, a
                  sensible value is calculated (which is recommended).</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>See <literal>File.createTable()</literal> (<xref
            linkend="createTableDescr" xrefstyle="select: label" />) for more
            information on the rest of parameters.</para>
          </section>

          <section id="createEArrayDescr" xreflabel="description">
            <title>createEArray(where, name, atom, shape, title='',
            filters=None, expectedrows=EXPECTED_ROWS_EARRAY, chunkshape=None,
            byteorder=None, createparents=False)</title>

            <para>Create a new enlargeable array with the given
            <literal>name</literal> in <literal>where</literal> location.  See
            the <literal>EArray</literal> (in <xref linkend="EArrayClassDescr"
            xrefstyle="select: label" />) class for more information on
            enlargeable arrays.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">atom</emphasis></glossterm>

                <glossdef>
                  <para>An <literal>Atom</literal> (see <xref
                  linkend="AtomClassDescr" xrefstyle="select: label" />)
                  instance representing the <emphasis>type</emphasis> and
                  <emphasis>shape</emphasis> of the atomic objects to be
                  saved.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">shape</emphasis></glossterm>

                <glossdef>
                  <para>The shape of the new array.  One (and only one) of the
                  shape dimensions <emphasis>must</emphasis> be 0.  The
                  dimension being 0 means that the resulting
                  <literal>EArray</literal> object can be extended along it.
                  Multiple enlargeable dimensions are not supported right
                  now.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">expectedrows</emphasis></glossterm>

                <glossdef>
                  <para>A user estimate about the number of row elements that
                  will be added to the growable dimension in the
                  <literal>EArray</literal> node.  If not provided, the
                  default value is <literal>EXPECTED_ROWS_EARRAY</literal>
                  (see <literal>tables/parameters.py</literal>).  If you plan
                  to create either a much smaller or a much bigger array try
                  providing a guess; this will optimize the HDF5 B-Tree
                  creation and management process time and the amount of
                  memory used.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">chunkshape</emphasis></glossterm>

                <glossdef>
                  <para>The shape of the data chunk to be read or written in a
                  single HDF5 I/O operation.  Filters are applied to those
                  chunks of data.  The dimensionality of
                  <literal>chunkshape</literal> must be the same as that of
                  <literal>shape</literal> (beware: no dimension should be 0
                  this time!).  If <literal>None</literal>, a sensible value
                  is calculated based on the <literal>expectedrows</literal>
                  parameter (which is recommended).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">byteorder</emphasis></glossterm>

                <glossdef>
                  <para>The byteorder of the data <emphasis>on
                  disk</emphasis>, specified as <literal>'little'</literal> or
                  <literal>'big'</literal>. If this is not specified, the
                  byteorder is that of the platform.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>See <literal>File.createTable()</literal> (<xref
            linkend="createTableDescr" xrefstyle="select: label" />) for more
            information on the rest of parameters.</para>
          </section>

          <section id="createExternalLinkDescr" xreflabel="description">
            <title>createExternalLink(where, name, target,
              createparents=False, warn16incompat=False)</title>

            <para>Create an external link to a <literal>target</literal> node
              with the given <literal>name</literal>
              in <literal>where</literal> location.  <literal>target</literal>
              can be a node object in another file or a path string in the
              form <literal>file:/path/to/node</literal>.  If
              <literal>createparents</literal> is true, the intermediate
              groups required for reaching <literal>where</literal> are
              created (the default is not doing so).
            </para>

            <para>The purpose of the <literal>warn16incompat</literal>
              argument is to avoid an <literal>Incompat16Warning</literal>
              (see below).  The default is to issue the warning.
            </para>

            <para>The returned node is an <literal>ExternalLink</literal>
              instance.  See the
              <literal>ExternalLink</literal> class (in
              <xref linkend="ExternalLinkClassDescr" xrefstyle="select: label"
              />) for more information on external links.</para>

            <warning>
              <para>External links are only supported when PyTables is
                compiled against HDF5 1.8.x series.  When using PyTables with
                HDF5 1.6.x, the <emphasis>parent</emphasis> group containing
                external link objects will be mapped to
                an <literal>Unknown</literal> instance (see
                <xref linkend="UnknownClassDescr" xrefstyle="select: label"
                />) and you won't be able to access <emphasis>any</emphasis>
                node hanging of this parent group.  It follows that if the
                parent group containing the external link is the root group,
                you won't be able to read <emphasis>any</emphasis> information
                contained in the file when using HDF5 1.6.x.</para>
            </warning>

          </section>

          <section id="createGroupDescr" xreflabel="description">
            <title>createGroup(where, name, title='', filters=None,
            createparents=False)</title>

            <para>Create a new group with the given <literal>name</literal> in
            <literal>where</literal> location.  See the
            <literal>Group</literal> class (in <xref linkend="GroupClassDescr"
            xrefstyle="select: label" />) for more information on
            groups.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis
                role="bold">filters</emphasis></glossterm>

                <glossdef>
                  <para>An instance of the <literal>Filters</literal> class
                  (see <xref linkend="FiltersClassDescr"
                  xrefstyle="select: label" />) that provides information
                  about the desired I/O filters applicable to the leaves that
                  hang directly from this new group (unless other filter
                  properties are specified for these leaves). Besides, if you
                  do not specify filter properties for its child groups, they
                  will inherit these ones.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>See <literal>File.createTable()</literal> (<xref
            linkend="createTableDescr" xrefstyle="select: label" />) for more
            information on the rest of parameters.</para>
          </section>

          <section id="createHardLinkDescr" xreflabel="description">
            <title>createHardLink(where, name, target,
              createparents=False)</title>

            <para>Create a hard link to a <literal>target</literal> node with
              the given <literal>name</literal> in <literal>where</literal>
              location.  <literal>target</literal> can be a node object or a
              path string.  If <literal>createparents</literal> is true, the
              intermediate groups required for
              reaching <literal>where</literal> are created (the default is
              not doing so).</para>

              <para>The returned node is a regular <literal>Group</literal>
              or <literal>Leaf</literal> instance.</para>

          </section>

          <section id="createSoftLinkDescr" xreflabel="description">
            <title>createSoftLink(where, name, target,
              createparents=False)</title>

            <para>Create a soft link (aka symbolic link) to
              a <literal>target</literal> node with the
              given <literal>name</literal> in <literal>where</literal>
              location.  <literal>target</literal> can be a node object or a
              path string.  If <literal>createparents</literal> is true, the
              intermediate groups required for
              reaching <literal>where</literal> are created (the default is
              not doing so).</para>

            <para>The returned node is a <literal>SoftLink</literal> instance.
              See the
              <literal>SoftLink</literal> class (in
              <xref linkend="SoftLinkClassDescr" xrefstyle="select: label" />)
              for more information on soft links.</para>

          </section>

          <section id="createTableDescr" xreflabel="description">
            <title>createTable(where, name, description, title='',
            filters=None, expectedrows=EXPECTED_ROWS_TABLE, chunkshape=None,
            byteorder=None, createparents=False)</title>

            <para>Create a new table with the given <literal>name</literal> in
            <literal>where</literal> location.  See the
            <literal>Table</literal> (in <xref linkend="TableClassDescr"
            xrefstyle="select: label" />) class for more information on
            tables.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where</emphasis></glossterm>

                <glossdef>
                  <para>The parent group where the new table will hang from.
                  It can be a path string (for example
                  <literal>'/level1/leaf5'</literal>), or a
                  <literal>Group</literal> instance (see <xref
                  linkend="GroupClassDescr" xrefstyle="select: label"
                  />).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">name</emphasis></glossterm>

                <glossdef>
                  <para>The name of the new table.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">description</emphasis></glossterm>

                <glossdef>
                  <para></para>

                  <para>This is an object that describes the table, i.e. how
                  many columns it has, their names, types, shapes, etc.  It
                  can be any of the following:</para>

                  <glosslist>
                    <glossentry>
                      <glossterm><emphasis role="bold">A user-defined
                      class</emphasis></glossterm>

                      <glossdef>
                        <para>This should inherit from the
                        <literal>IsDescription</literal> class (see <xref
                        linkend="IsDescriptionClassDescr" xrefstyle="select:
                        label" />) where table fields are specified.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis role="bold">A
                      dictionary</emphasis></glossterm>

                      <glossdef>
                        <para>For example, when you do not know beforehand
                        which structure your table will have).</para>

                        <!-- manual-only -->
                        <para>See <xref linkend="secondExample"
                        xrefstyle="select: label" /> for an example of using a
                        dictionary to describe a table.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis role="bold">A
                      <literal>Description</literal>
                      instance</emphasis></glossterm>

                      <glossdef>
                        <para>You can use the <literal>description</literal>
                        attribute of another table to create a new one with
                        the same structure.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis role="bold">A
                      NumPy dtype</emphasis></glossterm>

                      <glossdef>
                        <para>A completely general structured NumPy
                        dtype.</para>

                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis role="bold">A
                      <literal>NumPy (record)
                      array</literal> instance</emphasis></glossterm>

                      <glossdef>
                        <para>The dtype of this record array will be used as
                          the description.  Also, in case the array has actual
                          data, it will be injected into the newly created
                          table.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis role="bold">A
                      <literal>RecArray</literal>
                      instance</emphasis> (deprecated)</glossterm>

                      <glossdef>
                        <para>Object from the <literal>numarray</literal>
                          package.  This does not give you the possibility to
                          create a nested table.  Array data is injected into
                          the new table.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis role="bold">A
                      <literal>NestedRecArray</literal>
                      instance</emphasis> (deprecated)</glossterm>

                      <glossdef>
                        <para>If you want to have nested columns in your table
                        and you are using
                        <literal>numarray</literal>, you can use this
                        object. Array data is injected into the new
                        table.</para>

                        <!-- manual-only -->
                        <para>See <xref linkend="NestedRecArrayClassDescr"
                        xrefstyle="select: label" /> for a description of the
                        <literal>NestedRecArray</literal> class.
                        </para>
                      </glossdef>
                    </glossentry>
                  </glosslist>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">title</emphasis></glossterm>

                <glossdef>
                  <para>A description for this node (it sets the
                  <literal>TITLE</literal> HDF5 attribute on disk).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">filters</emphasis></glossterm>

                <glossdef>
                  <para>An instance of the <literal>Filters</literal> class
                  (see <xref linkend="FiltersClassDescr"
                  xrefstyle="select: label" />) that provides information
                  about the desired I/O filters to be applied during the life
                  of this object.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">expectedrows</emphasis></glossterm>

                <glossdef>
                  <para>A user estimate of the number of records that will be
                  in the table. If not provided, the default value is
                  <literal>EXPECTED_ROWS_TABLE</literal> (see
                  <literal>tables/parameters.py</literal>). If you plan to
                  create a bigger table try providing a guess; this will
                  optimize the HDF5 B-Tree creation and management process
                  time and memory used.</para>

                  <!-- manual-only -->
                  <para>See <xref linkend="expectedRowsOptim"
                  xrefstyle="select: label" /> for a discussion on the issue
                  of providing a number of expected rows.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">chunkshape</emphasis></glossterm>

                <glossdef>
                  <para>The shape of the data chunk to be read or written in a
                  single HDF5 I/O operation. Filters are applied to those
                  chunks of data. The rank of the
                  <literal>chunkshape</literal> for tables must be 1. If
                  <literal>None</literal>, a sensible value is calculated
                  based on the <literal>expectedrows</literal> parameter
                  (which is recommended).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">byteorder</emphasis></glossterm>

                <glossdef>
                  <para>The byteorder of data <emphasis>on disk</emphasis>,
                  specified as <literal>'little'</literal> or
                  <literal>'big'</literal>. If this is not specified, the
                  byteorder is that of the platform, unless you passed an
                  array as the <literal>description</literal>, in which case
                  its byteorder will be used.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">createparents</emphasis></glossterm>

                <glossdef>
                  <para>Whether to create the needed groups for the parent
                  path to exist (not done by default).</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="createVLArrayDescr" xreflabel="description">
            <title>createVLArray(where, name, atom, title='', filters=None,
            expectedsizeinMB=1.0, chunkshape=None, byteorder=None,
            createparents=False)</title>

            <para>Create a new variable-length array with the given
            <literal>name</literal> in <literal>where</literal> location.  See
            the <literal>VLArray</literal> (in <xref
            linkend="VLArrayClassDescr" xrefstyle="select: label" />) class
            for more information on variable-length arrays.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">atom</emphasis></glossterm>

                <glossdef>
                  <para>An <literal>Atom</literal> (see <xref
                  linkend="AtomClassDescr" xrefstyle="select: label" />)
                  instance representing the <emphasis>type</emphasis> and
                  <emphasis>shape</emphasis> of the atomic objects to be
                  saved.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">expectedsizeinMB</emphasis></glossterm>

                <glossdef>
                  <para>An user estimate about the size (in MB) in the final
                  <literal>VLArray</literal> node. If not provided, the
                  default value is 1 MB. If you plan to create either a much
                  smaller or a much bigger array try providing a guess; this
                  will optimize the HDF5 B-Tree creation and management
                  process time and the amount of memory used. If you want to
                  specify your own chunk size for I/O purposes, see also the
                  <literal>chunkshape</literal> parameter below.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">chunkshape</emphasis></glossterm>

                <glossdef>
                  <para>The shape of the data chunk to be read or written in a
                  single HDF5 I/O operation. Filters are applied to those
                  chunks of data. The dimensionality of
                  <literal>chunkshape</literal> must be 1. If
                  <literal>None</literal>, a sensible value is calculated
                  (which is recommended).</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>See <literal>File.createTable()</literal> (<xref
            linkend="createTableDescr" xrefstyle="select: label" />) for more
            information on the rest of parameters.</para>
          </section>

          <section id="File.moveNode" xreflabel="description">
            <title>moveNode(where, newparent=None, newname=None, name=None,
            overwrite=False, createparents=False)</title>

            <para>Move the node specified by <literal>where</literal> and
            <literal>name</literal> to
            <literal>newparent/newname</literal>.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where,
                name</emphasis></glossterm>

                <glossdef>
                  <para>These arguments work as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">newparent</emphasis></glossterm>

                <glossdef>
                  <para>The destination group the node will be moved into (a
                  path name or a <literal>Group</literal> instance). If it is
                  not specified or <literal>None</literal>, the current parent
                  group is chosen as the new parent.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">newname</emphasis></glossterm>

                <glossdef>
                  <para>The new name to be assigned to the node in its
                  destination (a string). If it is not specified or
                  <literal>None</literal>, the current name is chosen as the
                  new name.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>The other arguments work as in
            <literal>Node._f_move()</literal> (see <xref
            linkend="Node._f_move" />).</para>
          </section>

          <section id="File.removeNode" xreflabel="description">
            <title>removeNode(where, name=None, recursive=False)</title>

            <para>Remove the object node <emphasis>name</emphasis> under
            <emphasis>where</emphasis> location.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where,
                name</emphasis></glossterm>

                <glossdef>
                  <para>These arguments work as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">recursive</emphasis></glossterm>

                <glossdef>
                  <para>If not supplied or false, the node will be removed
                  only if it has no children; if it does, a
                  <literal>NodeError</literal> will be raised. If supplied
                  with a true value, the node and all its descendants will be
                  completely removed.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="File.renameNode" xreflabel="description">
            <title>renameNode(where, newname, name=None,
            overwrite=False)</title>

            <para>Change the name of the node specified by
            <literal>where</literal> and <literal>name</literal> to
            <literal>newname</literal>.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where,
                name</emphasis></glossterm>

                <glossdef>
                  <para>These arguments work as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">newname</emphasis></glossterm>

                <glossdef>
                  <para>The new name to be assigned to the node (a
                  string).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">overwrite</emphasis></glossterm>

                <glossdef>
                  <para>Whether to recursively remove a node with the same
                  <literal>newname</literal> if it already exists (not done by
                  default).</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>
        </section>

        <section id="FileMethods_tree_traversal">
          <title><literal>File</literal> methods — tree traversal</title>

          <section id="File.getNode" xreflabel="description">
            <title>getNode(where, name=None, classname=None)</title>

            <para>Get the node under <literal>where</literal> with the given
            <literal>name</literal>.</para>

            <para><literal>where</literal> can be a <literal>Node</literal>
            instance (see <xref linkend="NodeClassDescr" xrefstyle="select:
            label" />) or a path string leading to a node. If no
            <literal>name</literal> is specified, that node is
            returned.</para>

            <para>If a <literal>name</literal> is specified, this must be a
            string with the name of a node under <literal>where</literal>.  In
            this case the <literal>where</literal> argument can only lead to a
            <literal>Group</literal> (see <xref linkend="GroupClassDescr"
            xrefstyle="select: label" />) instance (else a
            <literal>TypeError</literal> is raised). The node called
            <literal>name</literal> under the group <literal>where</literal>
            is returned.</para>

            <para>In both cases, if the node to be returned does not exist, a
            <literal>NoSuchNodeError</literal> is raised. Please note that
            hidden nodes are also considered.</para>

            <para>If the <literal>classname</literal> argument is specified,
            it must be the name of a class derived from
            <literal>Node</literal>. If the node is found but it is not an
            instance of that class, a <literal>NoSuchNodeError</literal> is
            also raised.</para>
          </section>

          <section id="File.isVisibleNode" xreflabel="description">
            <title>isVisibleNode(path)</title>

            <para>Is the node under <literal>path</literal> visible?</para>

            <para>If the node does not exist, a
            <literal>NoSuchNodeError</literal> is raised.</para>
          </section>



          <section id="File.iterNodes" xreflabel="description">
            <title>iterNodes(where, classname=None)</title>

            <para>Iterate over children nodes hanging from
            <literal>where</literal>.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where</emphasis></glossterm>

                <glossdef>
                  <para>This argument works as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">classname</emphasis></glossterm>

                <glossdef>
                  <para>If the name of a class derived from
                  <literal>Node</literal> (see <xref linkend="NodeClassDescr"
                  xrefstyle="select: label" />) is supplied, only instances of
                  that class (or subclasses of it) will be returned.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>The returned nodes are alphanumerically sorted by their
            name.  This is an iterator version of
            <literal>File.listNodes()</literal> (see <xref
            linkend="File.listNodes"/>).</para>
          </section>

          <section id="File.listNodes" xreflabel="description">
            <title>listNodes(where, classname=None)</title>

            <para>Return a <emphasis>list</emphasis> with children nodes
            hanging from <literal>where</literal>.</para>

            <para>This is a list-returning version of
            <literal>File.iterNodes()</literal> (see <xref
            linkend="File.iterNodes"/>).</para>
          </section>

          <section id="walkGroupsDescr" xreflabel="description">
            <title>walkGroups(where='/')</title>

            <para>Recursively iterate over groups (not leaves) hanging from
            <literal>where</literal>.</para>

            <para>The <literal>where</literal> group itself is listed first
            (preorder), then each of its child groups (following an
            alphanumerical order) is also traversed, following the same
            procedure.  If <literal>where</literal> is not supplied, the root
            group is used.</para>

            <para>The <literal>where</literal> argument can be a path string
            or a <literal>Group</literal> instance (see <xref
            linkend="GroupClassDescr" xrefstyle="select: label" />).</para>
          </section>

          <section id="File.walkNodes" xreflabel="description">
            <title>walkNodes(where="/", classname="")</title>

            <para>Recursively iterate over nodes hanging from
            <literal>where</literal>.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where</emphasis></glossterm>

                <glossdef>
                  <para>If supplied, the iteration starts from (and includes)
                  this group. It can be a path string or a
                  <literal>Group</literal> instance (see <xref
                  linkend="GroupClassDescr" xrefstyle="select: label"
                  />).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">classname</emphasis></glossterm>

                <glossdef>
                  <para>If the name of a class derived from
                  <literal>Node</literal> (see <xref linkend="GroupClassDescr"
                  xrefstyle="select: label" />) is supplied, only instances of
                  that class (or subclasses of it) will be returned.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>Example of use:</para>

            <screen># Recursively print all the nodes hanging from '/detector'.
print "Nodes hanging from group '/detector':"
for node in h5file.walkNodes('/detector', classname='EArray'):
    print node</screen>
          </section>

          <section id="File.__contains__" xreflabel="description">
            <title>__contains__(path)</title>

            <para>Is there a node with that <literal>path</literal>?</para>

            <para>Returns <literal>True</literal> if the file has a node with
            the given <literal>path</literal> (a string),
            <literal>False</literal> otherwise.</para>
          </section>

          <section id="File.__iter__" xreflabel="description">
            <title>__iter__()</title>

            <para>Recursively iterate over the nodes in the tree.</para>

            <para>This is equivalent to calling
            <literal>File.walkNodes()</literal> (see <xref
            linkend="File.walkNodes" />) with no arguments.</para>

            <para>Example of use:</para>

            <screen># Recursively list all the nodes in the object tree.
h5file = tables.openFile('vlarray1.h5')
print "All nodes in the object tree:"
for node in h5file:
    print node</screen>
          </section>
        </section>

        <section id="FileMethods_undo_redo_support">
          <title><literal>File</literal> methods — Undo/Redo support</title>

          <section id="File.disableUndo" xreflabel="description">
            <title>disableUndo()</title>

            <para>Disable the Undo/Redo mechanism.</para>

            <para>Disabling the Undo/Redo mechanism leaves the database in the
            current state and forgets past and future database states. This
            makes <literal>File.mark()</literal> (see <xref
            linkend="File.mark" />), <literal>File.undo()</literal> (see <xref
            linkend="File.undo" />), <literal>File.redo()</literal> (see <xref
            linkend="File.redo" />) and other methods fail with an
            <literal>UndoRedoError</literal>.</para>

            <para>Calling this method when the Undo/Redo mechanism is already
            disabled raises an <literal>UndoRedoError</literal>.</para>
          </section>

          <section id="File.enableUndo" xreflabel="description">
            <title>enableUndo(filters=Filters( complevel=1))</title>

            <para>Enable the Undo/Redo mechanism.</para>

            <para>This operation prepares the database for undoing and redoing
            modifications in the node hierarchy. This allows
            <literal>File.mark()</literal> (see <xref linkend="File.mark" />),
            <literal>File.undo()</literal> (see <xref linkend="File.undo" />),
            <literal>File.redo()</literal> (see <xref linkend="File.redo" />)
            and other methods to be called.</para>

            <para>The <literal>filters</literal> argument, when specified,
            must be an instance of class <literal>Filters</literal> (see <xref
            linkend="FiltersClassDescr" xrefstyle="select: label" />) and is
            meant for setting the compression values for the action log. The
            default is having compression enabled, as the gains in terms of
            space can be considerable. You may want to disable compression if
            you want maximum speed for Undo/Redo operations.</para>

            <para>Calling this method when the Undo/Redo mechanism is already
            enabled raises an <literal>UndoRedoError</literal>.</para>
          </section>

          <section id="File.getCurrentMark" xreflabel="description">
            <title>getCurrentMark()</title>

            <para>Get the identifier of the current mark.</para>

            <para>Returns the identifier of the current mark. This can be used
            to know the state of a database after an application crash, or to
            get the identifier of the initial implicit mark after a call to
            <literal>File.enableUndo()</literal> (see <xref
            linkend="File.enableUndo" />).</para>

            <para>This method can only be called when the Undo/Redo mechanism
            has been enabled. Otherwise, an <literal>UndoRedoError</literal>
            is raised.</para>
          </section>

          <section id="File.goto" xreflabel="description">
            <title>goto(mark)</title>

            <para>Go to a specific mark of the database.</para>

            <para>Returns the database to the state associated with the
            specified <literal>mark</literal>. Both the identifier of a mark
            and its name can be used.</para>

            <para>This method can only be called when the Undo/Redo mechanism
            has been enabled. Otherwise, an <literal>UndoRedoError</literal>
            is raised.</para>
          </section>

          <section id="File.isUndoEnabled" xreflabel="description">
            <title>isUndoEnabled()</title>

            <para>Is the Undo/Redo mechanism enabled?</para>

            <para>Returns <literal>True</literal> if the Undo/Redo mechanism
            has been enabled for this file, <literal>False</literal>
            otherwise. Please note that this mechanism is persistent, so a
            newly opened PyTables file may already have Undo/Redo
            support enabled.</para>
          </section>

          <section id="File.mark" xreflabel="description">
            <title>mark(name=None)</title>

            <para>Mark the state of the database.</para>

            <para>Creates a mark for the current state of the database. A
            unique (and immutable) identifier for the mark is returned. An
            optional <literal>name</literal> (a string) can be assigned to the
            mark. Both the identifier of a mark and its name can be used in
            <literal>File.undo()</literal> (see <xref linkend="File.undo" />)
            and <literal>File.redo()</literal> (see <xref linkend="File.redo"
            />) operations. When the <literal>name</literal> has already been
            used for another mark, an <literal>UndoRedoError</literal> is
            raised.</para>

            <para>This method can only be called when the Undo/Redo mechanism
            has been enabled. Otherwise, an <literal>UndoRedoError</literal>
            is raised.</para>
          </section>

          <section id="File.redo" xreflabel="description">
            <title>redo(mark=None)</title>

            <para>Go to a future state of the database.</para>

            <para>Returns the database to the state associated with the
            specified <literal>mark</literal>. Both the identifier of a mark
            and its name can be used. If the <literal>mark</literal> is
            omitted, the next created mark is used. If there are no future
            marks, or the specified <literal>mark</literal> is not newer than
            the current one, an <literal>UndoRedoError</literal> is
            raised.</para>

            <para>This method can only be called when the Undo/Redo mechanism
            has been enabled. Otherwise, an <literal>UndoRedoError</literal>
            is raised.</para>
          </section>

          <section id="File.undo" xreflabel="description">
            <title>undo(mark=None)</title>

            <para>Go to a past state of the database.</para>

            <para>Returns the database to the state associated with the
            specified <literal>mark</literal>. Both the identifier of a mark
            and its name can be used. If the <literal>mark</literal> is
            omitted, the last created mark is used. If there are no past
            marks, or the specified <literal>mark</literal> is not older than
            the current one, an <literal>UndoRedoError</literal> is
            raised.</para>

            <para>This method can only be called when the Undo/Redo mechanism
            has been enabled. Otherwise, an <literal>UndoRedoError</literal>
            is raised.</para>
          </section>
        </section>

        <section id="FileMethods_attribute_handling">
          <title><literal>File</literal> methods — attribute handling</title>

          <section id="File.copyNodeAttrs" xreflabel="description">
            <title>copyNodeAttrs(where, dstnode, name=None)</title>

            <para>Copy PyTables attributes from one node to another.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where,
                name</emphasis></glossterm>

                <glossdef>
                  <para>These arguments work as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">dstnode</emphasis></glossterm>

                <glossdef>
                  <para>The destination node where the attributes will be
                  copied to. It can be a path string or a
                  <literal>Node</literal> instance (see <xref
                  linkend="NodeClassDescr" xrefstyle="select: label"
                  />).</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="File.delNodeAttr" xreflabel="description">
            <title>delNodeAttr(where, attrname, name=None)</title>

            <para>Delete a PyTables attribute from the given node.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where,
                name</emphasis></glossterm>

                <glossdef>
                  <para>These arguments work as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">attrname</emphasis></glossterm>

                <glossdef>
                  <para>The name of the attribute to delete.  If the named
                  attribute does not exist, an
                  <literal>AttributeError</literal> is raised.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="File.getNodeAttr" xreflabel="description">
            <title>getNodeAttr(where, attrname, name=None)</title>

            <para>Get a PyTables attribute from the given node.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where,
                name</emphasis></glossterm>

                <glossdef>
                  <para>These arguments work as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">attrname</emphasis></glossterm>

                <glossdef>
                  <para>The name of the attribute to retrieve.  If the named
                  attribute does not exist, an
                  <literal>AttributeError</literal> is raised.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="File.setNodeAttr" xreflabel="description">
            <title>setNodeAttr(where, attrname, attrvalue, name=None)</title>

            <para>Set a PyTables attribute for the given node.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">where,
                name</emphasis></glossterm>

                <glossdef>
                  <para>These arguments work as in
                  <literal>File.getNode()</literal> (see <xref
                  linkend="File.getNode" />), referencing the node to be acted
                  upon.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">attrname</emphasis></glossterm>

                <glossdef>
                  <para>The name of the attribute to set.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">attrvalue</emphasis></glossterm>

                <glossdef>
                  <para>The value of the attribute to set. Any kind of Python
                  object (like strings, ints, floats, lists, tuples, dicts,
                  small NumPy/Numeric/numarray objects...) can be stored as an
                  attribute. However, if necessary, <literal>cPickle</literal>
                  is automatically used so as to serialize objects that you
                  might want to save. See the <literal>AttributeSet</literal>
                  class (in <xref linkend="AttributeSetClassDescr"
                  xrefstyle="select: label" />) for details.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>If the node already has a large number of attributes, a
            <literal>PerformanceWarning</literal> is issued.</para>
          </section>
        </section>
      </section>

      <section id="NodeClassDescr">
        <title>The <literal>Node</literal> class</title>

        <para>Abstract base class for all PyTables nodes.</para>

        <para>This is the base class for <emphasis>all</emphasis> nodes in a
        PyTables hierarchy. It is an abstract class, i.e. it may not be
        directly instantiated; however, every node in the hierarchy is an
        instance of this class.</para>

        <para>A PyTables node is always hosted in a PyTables
        <emphasis>file</emphasis>, under a <emphasis>parent group</emphasis>,
        at a certain <emphasis>depth</emphasis> in the node hierarchy. A node
        knows its own <emphasis>name</emphasis> in the parent group and its
        own <emphasis>path name</emphasis> in the file.</para>

        <para>All the previous information is location-dependent, i.e. it may
        change when moving or renaming a node in the hierarchy. A node also
        has location-independent information, such as its <emphasis>HDF5
        object identifier</emphasis> and its <emphasis>attribute
        set</emphasis>.</para>

        <para>This class gathers the operations and attributes (both
        location-dependent and independent) which are common to all PyTables
        nodes, whatever their type is. Nonetheless, due to natural naming
        restrictions, the names of all of these members start with a reserved
        prefix (see the <literal>Group</literal> class in <xref
        linkend="GroupClassDescr" xrefstyle="select: label" />).</para>

        <para>Sub-classes with no children (e.g. <emphasis>leaf
        nodes</emphasis>) may define new methods, attributes and properties to
        avoid natural naming restrictions. For instance,
        <literal>_v_attrs</literal> may be shortened to
        <literal>attrs</literal> and <literal>_f_rename</literal> to
        <literal>rename</literal>. However, the original methods and
        attributes should still be available.</para>

        <section>
          <title><literal>Node</literal> instance variables — location
          dependent</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_depth</emphasis></glossterm>

              <glossdef>
                <para>The depth of this node in the tree (an non-negative
                integer value).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">_v_file</emphasis></glossterm>

              <glossdef>
                <para>The hosting <literal>File</literal> instance (see <xref
                linkend="FileClassDescr" xrefstyle="select: label" />).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_name</emphasis></glossterm>

              <glossdef>
                <para>The name of this node in its parent group (a
                string).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_parent</emphasis></glossterm>

              <glossdef>
                <para>The parent <literal>Group</literal> instance (see <xref
                linkend="GroupClassDescr" xrefstyle="select: label"
                />).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_pathname</emphasis></glossterm>

              <glossdef>
                <para>The path of this node in the tree (a string).</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section id="NodeClassInstanceVariables">
          <title><literal>Node</literal> instance variables — location
          independent</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">_v_attrs</emphasis></glossterm>

              <glossdef>
                <para>The associated <literal>AttributeSet</literal> instance
                (see <xref linkend="AttributeSetClassDescr" xrefstyle="select:
                label" />).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_isopen</emphasis></glossterm>

              <glossdef>
                <para>Whether this node is open or not.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">_v_objectID</emphasis></glossterm>

              <glossdef>
                <para>A node identifier (may change from run to run).</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title><literal>Node</literal> instance variables — attribute
          shorthands</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">_v_title</emphasis></glossterm>

              <glossdef>
                <para>A description of this node. A shorthand for
                <literal>TITLE</literal> attribute.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title><literal>Node</literal> methods — hierarchy manipulation</title>

          <section id="Node._f_close" xreflabel="description">
            <title>_f_close()</title>

            <para>Close this node in the tree.</para>

            <para>This releases all resources held by the node, so it should
            not be used again. On nodes with data, it may be flushed to
            disk.</para>

            <para>You should not need to close nodes manually because they are
            automatically opened/closed when they are loaded/evicted from the
            integrated LRU cache.</para>
          </section>

          <section id="Node._f_copy" xreflabel="description">
            <title>_f_copy(newparent=None, newname=None, overwrite=False,
            recursive=False, createparents=False, **kwargs)</title>

            <para>Copy this node and return the new node.</para>

            <para>Creates and returns a copy of the node, maybe in a different
            place in the hierarchy. <literal>newparent</literal> can be a
            <literal>Group</literal> object (see <xref
            linkend="GroupClassDescr" xrefstyle="select: label" />) or a
            pathname in string form. If it is not specified or
            <literal>None</literal>, the current parent group is chosen as the
            new parent.  <literal>newname</literal> must be a string with a
            new name. If it is not specified or <literal>None</literal>, the
            current name is chosen as the new name. If
            <literal>recursive</literal> copy is stated, all descendants are
            copied as well. If <literal>createparents</literal> is true, the
            needed groups for the given new parent group path to exist will be
            created.</para>

            <para>Copying a node across databases is supported but can not be
            undone. Copying a node over itself is not allowed, nor it is
            recursively copying a node into itself. These result in a
            <literal>NodeError</literal>. Copying over another existing node
            is similarly not allowed, unless the optional
            <literal>overwrite</literal> argument is true, in which case that
            node is recursively removed before copying.</para>

            <para>Additional keyword arguments may be passed to customize the
            copying process. For instance, title and filters may be changed,
            user attributes may be or may not be copied, data may be
            sub-sampled, stats may be collected, etc. See the documentation
            for the particular node type.</para>

            <para>Using only the first argument is equivalent to copying the
            node to a new location without changing its name. Using only the
            second argument is equivalent to making a copy of the node in the
            same group.</para>
          </section>

          <section id="Node._f_isVisible" xreflabel="description">
            <title>_f_isVisible()</title>

            <para>Is this node visible?</para>
          </section>

          <section id="Node._f_move" xreflabel="description">
            <title>_f_move(newparent=None, newname=None, overwrite=False,
            createparents=False)</title>

            <para>Move or rename this node.</para>

            <para>Moves a node into a new parent group, or changes the name of
            the node. <literal>newparent</literal> can be a
            <literal>Group</literal> object (see <xref
            linkend="GroupClassDescr" xrefstyle="select: label" />) or a
            pathname in string form. If it is not specified or
            <literal>None</literal>, the current parent group is chosen as the
            new parent.  <literal>newname</literal> must be a string with a
            new name. If it is not specified or <literal>None</literal>, the
            current name is chosen as the new name. If
            <literal>createparents</literal> is true, the needed groups for
            the given new parent group path to exist will be created.</para>

            <para>Moving a node across databases is not allowed, nor it is
            moving a node <emphasis>into</emphasis> itself. These result in a
            <literal>NodeError</literal>. However, moving a node
            <emphasis>over</emphasis> itself is allowed and simply does
            nothing. Moving over another existing node is similarly not
            allowed, unless the optional <literal>overwrite</literal> argument
            is true, in which case that node is recursively removed before
            moving.</para>

            <para>Usually, only the first argument will be used, effectively
            moving the node to a new location without changing its name.
            Using only the second argument is equivalent to renaming the node
            in place.</para>
          </section>

          <section id="Node._f_remove" xreflabel="description">
            <title>_f_remove(recursive=False, force=False)</title>

            <para>Remove this node from the hierarchy.</para>

            <para>If the node has children, recursive removal must be stated
            by giving <literal>recursive</literal> a true value; otherwise, a
            <literal>NodeError</literal> will be raised.</para>

            <para>If the node is a link to a <literal>Group</literal> object,
            and you are sure that you want to delete it, you can do this by
            setting the <literal>force</literal> flag to true.</para>

          </section>

          <section id="Node._f_rename" xreflabel="description">
            <title>_f_rename(newname, overwrite=False)</title>

            <para>Rename this node in place.</para>

            <para>Changes the name of a node to <emphasis>newname</emphasis>
            (a string).  If a node with the same <literal>newname</literal>
            already exists and <literal>overwrite</literal> is true,
            recursively remove it before renaming.</para>
          </section>
        </section>

        <section>
          <title><literal>Node</literal> methods — attribute handling</title>

          <section id="Node._f_delAttr" xreflabel="description">
            <title>_f_delAttr(name)</title>

            <para>Delete a PyTables attribute from this node.</para>

            <para>If the named attribute does not exist, an
            <literal>AttributeError</literal> is raised.</para>
          </section>

          <section id="Node._f_getAttr" xreflabel="description">
            <title>_f_getAttr(name)</title>

            <para>Get a PyTables attribute from this node.</para>

            <para>If the named attribute does not exist, an
            <literal>AttributeError</literal> is raised.</para>
          </section>

          <section id="Node._f_setAttr" xreflabel="description">
            <title>_f_setAttr(name, value)</title>

            <para>Set a PyTables attribute for this node.</para>

            <para>If the node already has a large number of attributes, a
            <literal>PerformanceWarning</literal> is issued.</para>
          </section>
        </section>
      </section>

      <section id="GroupClassDescr">
        <title>The <literal>Group</literal> class</title>

        <para>Basic PyTables grouping structure.</para>

        <para>Instances of this class are grouping structures containing
        <emphasis>child</emphasis> instances of zero or more groups or leaves,
        together with supporting metadata. Each group has exactly one
        <emphasis>parent</emphasis> group.</para>

        <para>Working with groups and leaves is similar in many ways to
        working with directories and files, respectively, in a Unix
        filesystem. As with Unix directories and files, objects in the object
        tree are often described by giving their full (or absolute) path
        names. This full path can be specified either as a string (like in
        <literal>'/group1/group2'</literal>) or as a complete object path
        written in <emphasis>natural naming</emphasis> schema (like in
        <literal>file.root.group1.group2</literal>).
        <!-- manual-only -->
        See <xref linkend="ObjectTreeSection" xrefstyle="select: label"
        /> for more information on natural naming.</para>

        <para>A collateral effect of the <emphasis>natural naming</emphasis>
        schema is that the names of members in the <literal>Group</literal>
        class and its instances must be carefully chosen to avoid colliding
        with existing children node names.  For this reason and to avoid
        polluting the children namespace all members in a
        <literal>Group</literal> start with some reserved prefix, like
        <literal>_f_</literal> (for public methods), <literal>_g_</literal>
        (for private ones), <literal>_v_</literal> (for instance variables) or
        <literal>_c_</literal> (for class variables). Any attempt to create a
        new child node whose name starts with one of these prefixes will raise
        a <literal>ValueError</literal> exception.</para>

        <para>Another effect of natural naming is that children named after
        Python keywords or having names not valid as Python identifiers (e.g.
        <literal>class</literal>, <literal>$a</literal> or
        <literal>44</literal>) can not be accessed using the
        <literal>node.child</literal> syntax. You will be forced to use
        <literal>node._f_getChild(child)</literal> to access them (which is
        recommended for programmatic accesses).</para>

        <para>You will also need to use <literal>_f_getChild()</literal> to
        access an existing child node if you set a Python attribute in the
        <literal>Group</literal> with the same name as that node (you will get
        a <literal>NaturalNameWarning</literal> when doing this).</para>

        <section>
          <title><literal>Group</literal> instance variables</title>

          <para>The following instance variables are provided in addition to
          those in <literal>Node</literal> (see <xref linkend="NodeClassDescr"
          xrefstyle="select: label" />):</para>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_children</emphasis></glossterm>

              <glossdef>
                <para>Dictionary with all nodes hanging from this
                group.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_filters</emphasis></glossterm>

              <glossdef>
                <para>Default filter properties for child nodes.</para>

                <para>You can (and are encouraged to) use this property to
                get, set and delete the <literal>FILTERS</literal> HDF5
                attribute of the group, which stores a
                <literal>Filters</literal> instance (see <xref
                linkend="FiltersClassDescr" xrefstyle="select:label" />). When
                the group has no such attribute, a default
                <literal>Filters</literal> instance is used.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_groups</emphasis></glossterm>

              <glossdef>
                <para>Dictionary with all groups hanging from this
                group.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_hidden</emphasis></glossterm>

              <glossdef>
                <para>Dictionary with all hidden nodes hanging from this
                group.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_leaves</emphasis></glossterm>

              <glossdef>
                <para>Dictionary with all leaves hanging from this
                group.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_links</emphasis></glossterm>

              <glossdef>
                <para>Dictionary with all links hanging from this
                group.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">_v_nchildren</emphasis></glossterm>

              <glossdef>
                <para>The number of children hanging from this group.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">
              _v_unknown</emphasis></glossterm>

              <glossdef>
                <para>Dictionary with all unknown nodes hanging from this
                group.</para>
              </glossdef>
            </glossentry>

          </glosslist>
        </section>

        <section>
          <title><literal>Group</literal> methods</title>

          <para><emphasis>Caveat: </emphasis>The following methods are
          documented for completeness, and they can be used without any
          problem. However, you should use the high-level counterpart methods
          in the <literal>File</literal> class (see <xref
          linkend="FileClassDescr" xrefstyle="select: label" />, because they
          are most used in documentation and examples, and are a bit more
          powerful than those exposed here.</para>

          <para>The following methods are provided in addition to those in
          <literal>Node</literal> (see <xref linkend="NodeClassDescr"
          xrefstyle="select: label" />):</para>

          <section id="Group._f_close" xreflabel="description">
            <title>_f_close()</title>

            <para>Close this group and all its descendents.</para>

            <para>This method has the behavior described in
            <literal>Node._f_close()</literal> (see <xref
            linkend="Node._f_close" />).  It should be noted that this
            operation closes all the nodes descending from this group.</para>

            <para>You should not need to close nodes manually because they are
            automatically opened/closed when they are loaded/evicted from the
            integrated LRU cache.</para>

          </section>

          <section id="Group._f_copy" xreflabel="description">
            <title>_f_copy(newparent, newname, overwrite=False,
            recursive=False, createparents=False, **kwargs)</title>

            <para>Copy this node and return the new one.</para>

            <para>This method has the behavior described in
            <literal>Node._f_copy()</literal> (see <xref
            linkend="Node._f_copy" />). In addition, it recognizes the
            following keyword arguments:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">title</emphasis></glossterm>

                <glossdef>
                  <para>The new title for the destination. If omitted or
                  <literal>None</literal>, the original title is used. This
                  only applies to the topmost node in recursive copies.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">filters</emphasis></glossterm>

                <glossdef>
                  <para>Specifying this parameter overrides the original
                  filter properties in the source node. If specified, it must
                  be an instance of the <literal>Filters</literal> class (see
                  <xref linkend="FiltersClassDescr"
                  xrefstyle="select: label" />). The default is to copy the
                  filter properties from the source node.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">copyuserattrs</emphasis></glossterm>

                <glossdef>
                  <para>You can prevent the user attributes from being copied
                  by setting this parameter to <literal>False</literal>. The
                  default is to copy them.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">stats</emphasis></glossterm>

                <glossdef>
                  <para>This argument may be used to collect statistics on the
                  copy process. When used, it should be a dictionary with keys
                  <literal>'groups'</literal>, <literal>'leaves'</literal>,
                  <literal>'links'</literal> and
                  <literal>'bytes'</literal> having a numeric value. Their
                  values will be incremented to reflect the number of groups,
                  leaves and bytes, respectively, that have been copied during
                  the operation.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="Group._f_copyChildren" xreflabel="description">
            <title>_f_copyChildren(dstgroup, overwrite=False, recursive=False,
            createparents=False, **kwargs)</title>

            <para>Copy the children of this group into another group.</para>

            <para>Children hanging directly from this group are copied into
            <literal>dstgroup</literal>, which can be a
            <literal>Group</literal> (see <xref linkend="GroupClassDescr"
            xrefstyle="select: label" />) object or its pathname in string
            form. If <literal>createparents</literal> is true, the needed
            groups for the given destination group path to exist will be
            created.</para>

            <para>The operation will fail with a <literal>NodeError</literal>
            if there is a child node in the destination group with the same
            name as one of the copied children from this one, unless
            <literal>overwrite</literal> is true; in this case, the former
            child node is recursively removed before copying the later.</para>

            <para>By default, nodes descending from children groups of this
            node are not copied. If the <literal>recursive</literal> argument
            is true, all descendant nodes of this node are recursively
            copied.</para>

            <para>Additional keyword arguments may be passed to customize the
            copying process. For instance, title and filters may be changed,
            user attributes may be or may not be copied, data may be
            sub-sampled, stats may be collected, etc. Arguments unknown to
            nodes are simply ignored. Check the documentation for copying
            operations of nodes to see which options they support.</para>
          </section>

          <section id="Group._f_getChild" xreflabel="description">
            <title>_f_getChild(childname)</title>

            <para>Get the child called <literal>childname</literal> of this
            group.</para>

            <para>If the child exists (be it visible or not), it is returned.
            Else, a <literal>NoSuchNodeError</literal> is raised.</para>

            <para>Using this method is recommended over
            <literal>getattr()</literal> when doing programmatic accesses to
            children if <literal>childname</literal> is unknown beforehand or
            when its name is not a valid Python identifier.</para>
          </section>

          <section id="Group._f_iterNodes" xreflabel="description">
            <title>_f_iterNodes(classname=None)</title>

            <para>Iterate over children nodes.</para>

            <para>Child nodes are yielded alphanumerically sorted by node
            name.  If the name of a class derived from <literal>Node</literal>
            (see <xref linkend="NodeClassDescr" xrefstyle="select: label" />)
            is supplied in the <literal>classname</literal> parameter, only
            instances of that class (or subclasses of it) will be
            returned.</para>

            <para>This is an iterator version of
            <literal>Group._f_listNodes()</literal> (see <xref
            linkend="Group._f_listNodes" />).</para>
          </section>

          <section id="Group._f_listNodes" xreflabel="description">
            <title>_f_listNodes(classname=None)</title>

            <para>Return a <emphasis>list</emphasis> with children
            nodes.</para>

            <para>This is a list-returning version of
            <literal>Group._f_iterNodes()</literal> (see <xref
            linkend="Group._f_iterNodes" />).</para>
          </section>

          <section id="Group._f_walkGroups" xreflabel="description">
            <title>_f_walkGroups()</title>

            <para>Recursively iterate over descendant groups (not
            leaves).</para>

            <para>This method starts by yielding <emphasis>self</emphasis>,
            and then it goes on to recursively iterate over all child groups
            in alphanumerical order, top to bottom (preorder), following the
            same procedure.</para>
          </section>

          <section id="Group._f_walkNodes" xreflabel="description">
            <title>_f_walkNodes(classname=None)</title>

            <para>Iterate over descendant nodes.</para>

            <para>This method recursively walks <emphasis>self</emphasis> top
            to bottom (preorder), iterating over child groups in
            alphanumerical order, and yielding nodes.  If
            <literal>classname</literal> is supplied, only instances of the
            named class are yielded.</para>

            <para>If <emphasis>classname</emphasis> is
            <literal>Group</literal>, it behaves like
            <emphasis>Group._f_walkGroups()</emphasis> (see <xref
            linkend="Group._f_walkGroups" />), yielding only groups.  If you
            don't want a recursive behavior, use
            <emphasis>Group._f_iterNodes()</emphasis> (see <xref
            linkend="Group._f_iterNodes" />) instead.</para>

            <para>Example of use:</para>

            <screen># Recursively print all the arrays hanging from '/'
print "Arrays in the object tree '/':"
for array in h5file.root._f_walkNodes('Array', recursive=True):
    print array</screen>
          </section>
        </section>

        <section>
          <title><literal>Group</literal> special methods</title>

          <para>Following are described the methods that automatically trigger
          actions when a <literal>Group</literal> instance is accessed in a
          special way.</para>

          <para>This class defines the <literal>__setattr__</literal>,
          <literal>__getattr__</literal> and <literal>__delattr__</literal>
          methods, and they set, get and delete <emphasis>ordinary Python
          attributes</emphasis> as normally intended. In addition to that,
          <literal>__getattr__</literal> allows getting <emphasis>child
          nodes</emphasis> by their name for the sake of easy interaction on
          the command line, as long as there is no Python attribute with the
          same name. Groups also allow the interactive completion (when using
          <literal>readline</literal>) of the names of child nodes. For
          instance:</para>

          <screen>nchild = group._v_nchildren  # get a Python attribute

# Add a Table child called 'table' under 'group'.
h5file.createTable(group, 'table', myDescription)

table = group.table          # get the table child instance
group.table = 'foo'          # set a Python attribute
# (PyTables warns you here about using the name of a child node.)
foo = group.table            # get a Python attribute
del group.table              # delete a Python attribute
table = group.table          # get the table child instance again</screen>

          <section id="Group.__contains__" xreflabel="description">
            <title>__contains__(name)</title>

            <para>Is there a child with that <literal>name</literal>?</para>

            <para>Returns a true value if the group has a child node (visible
            or hidden) with the given <emphasis>name</emphasis> (a string),
            false otherwise.</para>
          </section>

          <section id="Group.__delattr__" xreflabel="description">
            <title>__delattr__(name)</title>

            <para>Delete a Python attribute called
            <literal>name</literal>.</para>

            <para>This method deletes an <emphasis>ordinary Python
            attribute</emphasis> from the object. It does
            <emphasis>not</emphasis> remove children nodes from this group;
            for that, use <literal>File.removeNode()</literal> (see <xref
            linkend="File.removeNode" />) or
            <literal>Node._f_remove()</literal> (see <xref
            linkend="Node._f_remove" />). It does <emphasis>neither</emphasis>
            delete a PyTables node attribute; for that, use
            <literal>File.delNodeAttr()</literal> (see <xref
            linkend="File.delNodeAttr" />),
            <literal>Node._f_delAttr()</literal> (see <xref
            linkend="Node._f_delAttr" />) or <literal>Node._v_attrs</literal>
            (see <xref linkend="NodeClassInstanceVariables" xrefstyle="select:
            label" />).</para>

            <para>If there is an attribute and a child node with the same
            <literal>name</literal>, the child node will be made accessible
            again via natural naming.</para>
          </section>

          <section id="Group.__getattr__" xreflabel="description">
            <title>__getattr__(name)</title>

            <para>Get a Python attribute or child node called
            <literal>name</literal>.</para>

            <para>If the object has a Python attribute called
            <literal>name</literal>, its value is returned. Else, if the node
            has a child node called <literal>name</literal>, it is returned.
            Else, an <literal>AttributeError</literal> is raised.</para>
          </section>

          <section id="Group.__iter__" xreflabel="description">
            <title>__iter__()</title>

            <para>Iterate over the child nodes hanging directly from the
            group.</para>

            <para>This iterator is <emphasis>not</emphasis> recursive.
            Example of use:</para>

            <screen># Non-recursively list all the nodes hanging from '/detector'
print "Nodes in '/detector' group:"
for node in h5file.root.detector:
    print node</screen>
          </section>

          <section id="Group.__repr__" xreflabel="description">
            <title>__repr__()</title>

            <para>Return a detailed string representation of the group.</para>

            <para>Example of use:</para>

            <screen>>>> f = tables.openFile('data/test.h5')
>>> f.root.group0
/group0 (Group) 'First Group'
  children := ['tuple1' (Table), 'group1' (Group)]</screen>
          </section>

          <section id="Group.__setattr__" xreflabel="description">
            <title>__setattr__(name, value)</title>

            <para>Set a Python attribute called <literal>name</literal> with
            the given <literal>value</literal>.</para>

            <para>This method stores an <emphasis>ordinary Python
            attribute</emphasis> in the object. It does
            <emphasis>not</emphasis> store new children nodes under this
            group; for that, use the <literal>File.create*()</literal> methods
            (see the <literal>File</literal> class in <xref
            linkend="FileClassDescr" xrefstyle="select: label" />). It does
            <emphasis>neither</emphasis> store a PyTables node attribute; for
            that, use <literal>File.setNodeAttr()</literal> (see <xref
            linkend="File.setNodeAttr" />),
            <literal>Node._f_setAttr()</literal> (see <xref
            linkend="Node._f_setAttr" />) or <literal>Node._v_attrs</literal>
            (see <xref linkend="NodeClassInstanceVariables" xrefstyle="select:
            label" />).</para>

            <para>If there is already a child node with the same
            <literal>name</literal>, a <literal>NaturalNameWarning</literal>
            will be issued and the child node will not be accessible via
            natural naming nor <literal>getattr()</literal>. It will still be
            available via <literal>File.getNode()</literal> (see <xref
            linkend="File.getNode" />), <literal>Group._f_getChild()</literal>
            (see <xref linkend="Group._f_getChild" />) and children
            dictionaries in the group (if visible).</para>
          </section>

          <section id="Group.__str__" xreflabel="description">
            <title>__str__()</title>

            <para>Return a short string representation of the group.</para>

            <para>Example of use:</para>

            <screen>>>> f=tables.openFile('data/test.h5')
>>> print f.root.group0
/group0 (Group) 'First Group'</screen>
          </section>
        </section>
      </section>

      <section id="LeafClassDescr">
        <title>The <literal>Leaf</literal> class</title>

        <para>Abstract base class for all PyTables leaves.</para>

        <para>A leaf is a node (see the <literal>Node</literal> class in <xref
        linkend="NodeClassDescr" xrefstyle="select: label" />) which hangs
        from a group (see the <literal>Group</literal> class in <xref
        linkend="GroupClassDescr" xrefstyle="select: label" />) but, unlike a
        group, it can not have any further children below it (i.e. it is an
        end node).</para>

        <para>This definition includes all nodes which contain actual data
        (datasets handled by the <literal>Table</literal> —see <xref
        linkend="TableClassDescr" xrefstyle="select: label" />—,
        <literal>Array</literal> —see <xref linkend="ArrayClassDescr"
        xrefstyle="select: label" />—, <literal>CArray</literal> —see <xref
        linkend="CArrayClassDescr" xrefstyle="select: label" />—,
        <literal>EArray</literal> —see <xref linkend="EArrayClassDescr"
        xrefstyle="select: label" />— and <literal>VLArray</literal> —see
        <xref linkend="VLArrayClassDescr" xrefstyle="select: label" />—
        classes) and unsupported nodes (the <literal>UnImplemented</literal>
        class —<xref linkend="UnImplementedClassDescr" xrefstyle="select:
        label" />) —these classes do in fact inherit from
        <literal>Leaf</literal>.</para>

        <section id="LeafInstanceVariables">
          <title><literal>Leaf</literal> instance variables</title>

          <para>These instance variables are provided in addition to those in
          <literal>Node</literal> (see <xref linkend="NodeClassDescr"
          xrefstyle="select: label" />):</para>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis
              role="bold">byteorder</emphasis></glossterm>

              <glossdef>
                <para>The byte ordering of the leaf data <emphasis>on
                disk</emphasis>.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">chunkshape</emphasis></glossterm>

              <glossdef>
                <para>The HDF5 chunk size for chunked leaves (a tuple).</para>

                <para>This is read-only because you cannot change the chunk
                size of a leaf once it has been created.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">dtype</emphasis></glossterm>

              <glossdef>
                <para>The NumPy <literal>dtype</literal> that most closely
                  matches this leaf type.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">extdim</emphasis></glossterm>

              <glossdef>
                <para>The index of the enlargeable dimension (-1 if
                none).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">filters</emphasis></glossterm>

              <glossdef>
                <para>Filter properties for this leaf —see
                <literal>Filters</literal> in <xref
                linkend="FiltersClassDescr" xrefstyle="select: label"
                />.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">flavor</emphasis></glossterm>

              <glossdef>
                <para>The type of data object read from this leaf.</para>

                <para>It can be any of <literal>'numpy'</literal>,
                <literal>'numarray'</literal>, <literal>'numeric'</literal> or
                <literal>'python'</literal> (the set of supported flavors
                depends on which packages you have installed on your
                system).</para>

                <para>You can (and are encouraged to) use this property to
                get, set and delete the <literal>FLAVOR</literal> HDF5
                attribute of the leaf. When the leaf has no such attribute,
                the default flavor is used.</para>

                <warning>
                  <para>The <literal>'numarray'</literal> and
                  <literal>'numeric'</literal> flavors are deprecated since
                  version 2.3. Support for these flavors will be removed in
                  future versions.</para>
                </warning>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">maindim</emphasis></glossterm>

              <glossdef>
                <para>The dimension along which iterators work.</para>
                <para>Its value is 0 (i.e. the first dimension) when the
                dataset is not extendable, and <literal>self.extdim</literal>
                (where available) for extendable ones.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">nrows</emphasis></glossterm>

              <glossdef>
                <para>The length of the main dimension of the leaf
                data.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">nrowsinbuf</emphasis></glossterm>

              <glossdef>
                <para>The number of rows that fit in internal input
                buffers.</para>

                <para>You can change this to fine-tune the speed or memory
                requirements of your application.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">shape</emphasis></glossterm>

              <glossdef>
                <para>The shape of data in the leaf.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title><literal>Leaf</literal> instance variables — aliases</title>

          <para>The following are just easier-to-write aliases to their
          <literal>Node</literal> (see <xref linkend="NodeClassDescr"
          xrefstyle="select: label" />) counterparts (indicated between
          parentheses):</para>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">attrs</emphasis></glossterm>

              <glossdef>
                <para>The associated <literal>AttributeSet</literal> instance
                —see <xref linkend="AttributeSetClassDescr" xrefstyle="select:
                label" />— (<literal>Node._v_attrs</literal>).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">name</emphasis></glossterm>

              <glossdef>
                <para>The name of this node in its parent group
                (<literal>Node._v_name</literal>).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">objectID</emphasis></glossterm>

              <glossdef>
                <para>A node identifier (may change from run to run).
                (<literal>Node._v_objectID</literal>).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">title</emphasis></glossterm>

              <glossdef>
                <para>A description for this node
                (<literal>Node._v_title</literal>).</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title><literal>Leaf</literal> methods</title>

          <section id="Leaf.close" xreflabel="description">
            <title>close(flush=True)</title>

            <para>Close this node in the tree.</para>

            <para>This method is completely equivalent to
            <literal>Leaf._f_close()</literal> (see <xref
            linkend="Leaf._f_close" />).</para>
          </section>

          <section id="Leaf.copy" xreflabel="description">
            <title>copy(newparent, newname, overwrite=False,
            createparents=False, **kwargs)</title>

            <para>Copy this node and return the new one.</para>

            <para>This method has the behavior described in
            <literal>Node._f_copy()</literal> (see <xref
            linkend="Node._f_copy" />). Please note that there is no
            <literal>recursive</literal> flag since leaves do not have child
            nodes. In addition, this method recognizes the following keyword
            arguments:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">title</emphasis></glossterm>

                <glossdef>
                  <para>The new title for the destination. If omitted or
                  <literal>None</literal>, the original title is used.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">filters</emphasis></glossterm>

                <glossdef>
                  <para>Specifying this parameter overrides the original
                  filter properties in the source node. If specified, it must
                  be an instance of the <literal>Filters</literal> class (see
                  <xref linkend="FiltersClassDescr"
                  xrefstyle="select: label" />). The default is to copy the
                  filter properties from the source node.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">copyuserattrs</emphasis></glossterm>

                <glossdef>
                  <para>You can prevent the user attributes from being copied
                  by setting this parameter to <literal>False</literal>. The
                  default is to copy them.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">start, stop,
                step</emphasis></glossterm>

                <glossdef>
                  <para>Specify the range of rows to be copied; the default is
                  to copy all the rows.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">stats</emphasis></glossterm>

                <glossdef>
                  <para>This argument may be used to collect statistics on the
                  copy process. When used, it should be a dictionary with keys
                  <literal>'groups'</literal>, <literal>'leaves'</literal> and
                  <literal>'bytes'</literal> having a numeric value. Their
                  values will be incremented to reflect the number of groups,
                  leaves and bytes, respectively, that have been copied during
                  the operation.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">chunkshape</emphasis></glossterm>

                <glossdef>
                  <para>The chunkshape of the new leaf.  It supports a couple
                  of special values.  A value of <literal>keep</literal> means
                  that the chunkshape will be the same than original leaf
                  (this is the default).  A value of <literal>auto</literal>
                  means that a new shape will be computed automatically in
                  order to ensure best performance when accessing the dataset
                  through the main dimension.  Any other value should be an
                  integer or a tuple matching the dimensions of the
                  leaf.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <warning>
              <para>Note that unknown parameters passed to this method will be
              ignored, so may want to double check the spell of these (i.e. if
              you write them incorrectly, they will most probably be
              ignored).</para>
            </warning>

          </section>

          <section id="Leaf.delAttr" xreflabel="description">
            <title>delAttr(name)</title>

            <para>Delete a PyTables attribute from this node.</para>

            <para>This method has the behavior described in
            <literal>Node._f_delAttr()</literal> (see <xref
            linkend="Node._f_delAttr" />).</para>
          </section>

          <section id="Leaf.flush" xreflabel="description">
            <title>flush()</title>

            <para>Flush pending data to disk.</para>

            <para>Saves whatever remaining buffered data to disk. It also
            releases I/O buffers, so if you are filling many datasets in the
            same PyTables session, please call <literal>flush()</literal>
            extensively so as to help PyTables to keep memory requirements
            low.</para>
          </section>

          <section id="Leaf.getAttr" xreflabel="description">
            <title>getAttr(name)</title>

            <para>Get a PyTables attribute from this node.</para>

            <para>This method has the behavior described in
            <literal>Node._f_getAttr()</literal> (see <xref
            linkend="Node._f_getAttr" />).</para>
          </section>

          <section id="Leaf.isVisible" xreflabel="description">
            <title>isVisible()</title>

            <para>Is this node visible?</para>

            <para>This method has the behavior described in
            <literal>Node._f_isVisible()</literal> (see <xref
            linkend="Node._f_isVisible" />).</para>
          </section>

          <section id="Leaf.move" xreflabel="description">
            <title>move(newparent=None, newname=None, overwrite=False,
            createparents=False)</title>

            <para>Move or rename this node.</para>

            <para>This method has the behavior described in
            <literal>Node._f_move()</literal> (see <xref
            linkend="Node._f_move" />).</para>
          </section>

          <section id="Leaf.rename" xreflabel="description">
            <title>rename(newname)</title>

            <para>Rename this node in place.</para>

            <para>This method has the behavior described in
            <literal>Node._f_rename()</literal> (see <xref
            linkend="Node._f_rename" />).</para>
          </section>

          <section id="Leaf.remove" xreflabel="description">
            <title>remove()</title>

            <para>Remove this node from the hierarchy.</para>

            <para>This method has the behavior described in
            <literal>Node._f_remove()</literal> (see <xref
            linkend="Node._f_remove" />). Please note that there is no
            <literal>recursive</literal> flag since leaves do not have child
            nodes.</para>
          </section>

          <section id="Leaf.setAttr" xreflabel="description">
            <title>setAttr(name, value)</title>

            <para>Set a PyTables attribute for this node.</para>

            <para>This method has the behavior described in
            <literal>Node._f_setAttr()</literal> (see <xref
            linkend="Node._f_setAttr" />).</para>
          </section>

          <section id="Leaf.truncate" xreflabel="description">
            <title>truncate(size)</title>

            <para>Truncate the main dimension to be <literal>size</literal>
            rows.</para>

            <para>If the main dimension previously was larger than this
            <literal>size</literal>, the extra data is lost.  If the main
            dimension previously was shorter, it is extended, and the extended
            part is filled with the default values.</para>

            <para>The truncation operation can only be applied to
            <emphasis>enlargeable</emphasis> datasets, else a
            <literal>TypeError</literal> will be raised.</para>

            <warning>
              <para>If you are using the HDF5 1.6.x series, and due to
              limitations of them, <literal>size</literal> must be greater
              than zero (i.e. the dataset can not be completely emptied).  A
              <literal>ValueError</literal> will be issued if you are using
              HDF5 1.6.x and try to pass a zero size to this method.  Also,
              HDF5 1.6.x has the problem that it cannot work against
              <literal>CArray</literal> objects (again, a
              <literal>ValueError</literal> will be issued).  HDF5 1.8.x
              doesn't undergo these problems.</para>
            </warning>
          </section>

          <section id="Leaf.__len__" xreflabel="description">
            <title>__len__()</title>
            <para>Return the length of the main dimension of the leaf
            data.</para>
            <para>Please note that this may raise an
            <literal>OverflowError</literal> on 32-bit platforms for datasets
            having more than 2**31-1 rows.  This is a limitation of Python
            that you can work around by using the <literal>nrows</literal> or
            <literal>shape</literal> attributes.</para>
          </section>

          <section id="Leaf._f_close" xreflabel="description">
            <title>_f_close(flush=True)</title>

            <para>Close this node in the tree.</para>

            <para>This method has the behavior described in
            <literal>Node._f_close()</literal> (see <xref
            linkend="Node._f_close" />).  Besides that, the optional argument
            <literal>flush</literal> tells whether to flush pending data to
            disk or not before closing.</para>
          </section>
        </section>
      </section>

      <section id="TableClassDescr">
        <title>The <literal>Table</literal> class</title>

        <para>This class represents heterogeneous datasets in an HDF5
        file.</para>

        <para>Tables are leaves (see the <literal>Leaf</literal> class in
        <xref linkend="LeafClassDescr" xrefstyle="select: label" />) whose
        data consists of a unidimensional sequence of
        <emphasis>rows</emphasis>, where each row contains one or more
        <emphasis>fields</emphasis>.  Fields have an associated unique
        <emphasis>name</emphasis> and <emphasis>position</emphasis>, with the
        first field having position 0.  All rows have the same fields, which
        are arranged in <emphasis>columns</emphasis>.</para>

        <para>Fields can have any type supported by the <literal>Col</literal>
        class (see <xref linkend="ColClassDescr" xrefstyle="select: label" />)
        and its descendants, which support multidimensional data.  Moreover, a
        field can be <emphasis>nested</emphasis> (to an arbitrary depth),
        meaning that it includes further fields inside.  A field named
        <literal>x</literal> inside a nested field <literal>a</literal> in a
        table can be accessed as the field <literal>a/x</literal> (its
        <emphasis>path name</emphasis>) from the table.</para>

        <para>The structure of a table is declared by its description, which
        is made available in the <literal>Table.description</literal>
        attribute (see <xref linkend="TableInstanceVariablesDescr"
        xrefstyle="select: label" />).</para>

        <para>This class provides new methods to read, write and search table
        data efficiently.  It also provides special Python methods to allow
        accessing the table as a normal sequence or array (with extended
        slicing supported).</para>

        <para>PyTables supports <emphasis>in-kernel</emphasis> searches
        working simultaneously on several columns using complex conditions.
        These are faster than selections using Python expressions.  See the
        <literal>Tables.where()</literal> method —<xref linkend="Table.where"
        />— for more information on in-kernel searches.
        <!-- manual-only -->
        See also <xref linkend="inkernelSearch" xrefstyle="select: label" />
        for a detailed review of the advantages and shortcomings of in-kernel
        searches.
        </para>

        <para>Non-nested columns can be <emphasis>indexed</emphasis>.
        Searching an indexed column can be several times faster than searching
        a non-nested one.  Search methods automatically take advantage of
        indexing where available.</para>

        <para>When iterating a table, an object from the
        <literal>Row</literal> (see <xref linkend="RowClassDescr"
        xrefstyle="select: label" />) class is used.  This object allows to
        read and write data one row at a time, as well as to perform queries
        which are not supported by in-kernel syntax (at a much lower speed, of
        course).
        <!-- manual-only -->
        See the tutorial sections in <xref linkend="usage" xrefstyle="select:
        label" /> on how to use the <literal>Row</literal> interface.</para>

        <para>Objects of this class support access to individual columns via
        <emphasis>natural naming</emphasis> through the
        <literal>Table.cols</literal> accessor (see <xref
        linkend="TableInstanceVariablesDescr" xrefstyle="select: label" />).
        Nested columns are mapped to <literal>Cols</literal> instances, and
        non-nested ones to <literal>Column</literal> instances.  See the
        <literal>Column</literal> class in <xref linkend="ColumnClassDescr"
        xrefstyle="select: label" /> for examples of this feature.</para>

        <section id="TableInstanceVariablesDescr">
          <title><literal>Table</literal> instance variables</title>

          <para>The following instance variables are provided in addition to
          those in <literal>Leaf</literal> (see <xref linkend="LeafClassDescr"
          xrefstyle="select: label" />).  Please note that there are several
          <literal>col*</literal> dictionaries to ease retrieving information
          about a column directly by its path name, avoiding the need to walk
          through <literal>Table.description</literal> or
          <literal>Table.cols</literal>.</para>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis
              role="bold">autoIndex</emphasis></glossterm>

              <glossdef>
                <para>Automatically keep column indexes up to date?</para>

                <para>Setting this value states whether existing indexes
                should be automatically updated after an append operation or
                recomputed after an index-invalidating operation (i.e. removal
                and modification of rows). The default is true.</para>

                <para>This value gets into effect whenever a column is
                altered. If you don't have automatic indexing activated and
                you want to do an immediate update use
                <literal>Table.flushRowsToIndex()</literal> (see <xref
                linkend="Table.flushRowsToIndex" />); for immediate reindexing
                of invalidated indexes, use
                <literal>Table.reIndexDirty()</literal> (see <xref
                linkend="Table.reIndexDirty" />).</para>

                <para>This value is persistent.
                </para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">coldescrs</emphasis></glossterm>

              <glossdef>
                <para>Maps the name of a column to its <literal>Col</literal>
                description (see <xref linkend="ColClassDescr"
                xrefstyle="select: label" />).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">coldflts</emphasis></glossterm>

              <glossdef>
                <para>Maps the name of a column to its default value.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">coldtypes</emphasis></glossterm>

              <glossdef>
                <para>Maps the name of a column to its NumPy data type.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">colindexed</emphasis></glossterm>

              <glossdef>
                <para>Is the column which name is used as a key indexed?
                </para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">colindexes</emphasis></glossterm>

              <glossdef>
                <para>A dictionary with the indexes of the indexed columns.
                </para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">colinstances</emphasis></glossterm>

              <glossdef>
                <para>Maps the name of a column to its
                <literal>Column</literal> (see <xref
                linkend="ColumnClassDescr" xrefstyle="select: label" />) or
                <literal>Cols</literal> (see <xref linkend="ColsClassDescr"
                xrefstyle="select: label" />) instance.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">colnames</emphasis></glossterm>

              <glossdef>
                <para>A list containing the names of
                <emphasis>top-level</emphasis> columns in the table.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">colpathnames</emphasis></glossterm>

              <glossdef>
                <para>A list containing the pathnames of
                <emphasis>bottom-level</emphasis> columns in the table.</para>

                <para>These are the leaf columns obtained when walking the
                table description left-to-right, bottom-first. Columns inside
                a nested column have slashes (<literal>/</literal>) separating
                name components in their pathname.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">cols</emphasis></glossterm>

              <glossdef>
                <para>A <literal>Cols</literal> instance that provides
                <emphasis>natural naming</emphasis> access to non-nested
                (<literal>Column</literal>, see <xref
                linkend="ColumnClassDescr" xrefstyle="select: label" />) and
                nested (<literal>Cols</literal>, see <xref
                linkend="ColsClassDescr" xrefstyle="select: label" />)
                columns.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">coltypes</emphasis></glossterm>

              <glossdef>
                <para>Maps the name of a column to its PyTables
                data type.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">description</emphasis></glossterm>

              <glossdef>
                <para>A <literal>Description</literal> instance (see <xref
                linkend="DescriptionClassDescr" xrefstyle="select: label" />)
                reflecting the structure of the table.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">extdim</emphasis></glossterm>

              <glossdef>
                <para>The index of the enlargeable dimension (always 0 for
                tables).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">indexed</emphasis></glossterm>

              <glossdef>
                <para>Does this table have any indexed columns?
                </para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">indexedcolpathnames</emphasis></glossterm>

              <glossdef>
                <para>List of the pathnames of indexed columns in the
                table.
                </para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">nrows</emphasis></glossterm>

              <glossdef>
                <para>The current number of rows in the table.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">row</emphasis></glossterm>

              <glossdef>
                <para>The associated <literal>Row</literal> instance (see
                <xref linkend="RowClassDescr" xrefstyle="select: label"
                />).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">rowsize</emphasis></glossterm>

              <glossdef>
                <para>The size in bytes of each row in the table.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section id="TableMethods_reading">
          <title><literal>Table</literal> methods — reading</title>

          <section id="Table.col" xreflabel="description">
            <title>col(name)</title>

            <para>Get a column from the table.</para>

            <para>If a column called <literal>name</literal> exists in the
            table, it is read and returned as a NumPy object, or as a
            <literal>numarray</literal> object (depending on the flavor of the
            table). If it does not exist, a <literal>KeyError</literal> is
            raised.</para>

            <para>Example of use:</para>

            <screen>narray = table.col('var2')</screen>

            <para>That statement is equivalent to:</para>

            <screen>narray = table.read(field='var2')</screen>

            <para>Here you can see how this method can be used as a shorthand
            for the <literal>Table.read()</literal> method (see <xref
            linkend="Table.read" />).</para>
          </section>

          <section id="Table.iterrows" xreflabel="description">
            <title>iterrows(start=None, stop=None, step=None)</title>

            <para>Iterate over the table using a <literal>Row</literal>
            instance (see <xref linkend="RowClassDescr" xrefstyle="select:
            label" />).</para>

            <para>If a range is not supplied, <emphasis>all the
            rows</emphasis> in the table are iterated upon —you can also use
            the <literal>Table.__iter__()</literal> special method (see <xref
            linkend="Table.__iter__"/>) for that purpose. If you want to
            iterate over a given <emphasis>range of rows</emphasis> in the
            table, you may use the <literal>start</literal>,
            <literal>stop</literal> and <literal>step</literal> parameters,
            which have the same meaning as in <literal>Table.read()</literal>
            (see <xref linkend="Table.read" />).</para>

            <para>Example of use:</para>

            <screen>result = [ row['var2'] for row in table.iterrows(step=5)
           if row['var1'] &lt;= 20 ]</screen>

            <note>
              <para>This iterator can be nested (see
              <literal>Table.where()</literal> —<xref linkend="Table.where"
              />— for an example).</para>
            </note>

            <warning>
              <para>When in the middle of a table row iterator, you should not
              use methods that can change the number of rows in the table
              (like <literal>Table.append()</literal> or
              <literal>Table.removeRows()</literal>) or unexpected errors will
              happen.</para>
            </warning>

          </section>

          <section id="Table.itersequence" xreflabel="description">
            <title>itersequence(sequence)</title>

            <para>Iterate over a <literal>sequence</literal> of row
            coordinates.</para>

            <note>
              <para>This iterator can be nested (see
              <literal>Table.where()</literal> —<xref linkend="Table.where"
              />— for an example).</para>
            </note>
          </section>

          <section id="Table.itersorted" xreflabel="description">
            <title>itersorted(sortby, checkCSI=False, start=None, stop=None,
            step=None)</title>

            <para>Iterate table data following the order of the index
            of <literal>sortby</literal> column.</para>

            <para><literal>sortby</literal> column must have associated a
            <literal>full</literal> index.  If you want to ensure a fully
            sorted order, the index must be a CSI one.  You may want to use
            the <literal>checkCSI</literal> argument in order to explicitly
            check for the existence of a CSI index.</para>

            <para>The meaning of the <literal>start</literal>,
            <literal>stop</literal> and <literal>step</literal> arguments is
            the same as in <literal>Table.read()</literal> (see <xref
            linkend="Table.read" />).  However, in this case a negative value
            of <literal>step</literal> is supported, meaning that the results
            will be returned in reverse sorted order.</para>

          </section>

          <section id="Table.read" xreflabel="description">
            <title>read(start=None, stop=None, step=None, field=None)</title>

            <para>Get data in the table as a (record) array.</para>

            <para>The <literal>start</literal>, <literal>stop</literal> and
            <literal>step</literal> parameters can be used to select only a
            <emphasis>range of rows</emphasis> in the table. Their meanings
            are the same as in the built-in <literal>range()</literal> Python
            function, except that negative values of <literal>step</literal>
            are not allowed yet. Moreover, if only <literal>start</literal> is
            specified, then <literal>stop</literal> will be set to
            <literal>start+1</literal>. If you do not specify neither
            <literal>start</literal> nor <literal>stop</literal>, then
            <emphasis>all the rows</emphasis> in the table are
            selected.</para>

            <para>If <literal>field</literal> is supplied only the named
            column will be selected.  If the column is not nested, an
            <emphasis>array</emphasis> of the current flavor will be returned;
            if it is, a <emphasis>record array</emphasis> will be used
            instead.  I no <literal>field</literal> is specified, all the
            columns will be returned in a record array of the current flavor.
            <!-- manual-only -->
            More specifically, when the flavor is
            <literal>'numarray'</literal> (deprecated) and a record array is
            needed, a <literal>NestedRecArray</literal> (see <xref
            linkend="NestedRecArrayClassDescr" xrefstyle="select: label" />)
            will be returned.
            </para>

            <para>Columns under a nested column can be specified in the
            <literal>field</literal> parameter by using a slash character
            (<literal>/</literal>) as a separator
            (e.g. <literal>'position/x'</literal>).</para>
          </section>

          <section id="Table.readCoordinates"  xreflabel="description">
            <title>readCoordinates(coords, field=None)</title>

            <para>Get a set of rows given their indexes as a (record)
            array.</para>

            <para>This method works much like the <literal>read()</literal>
            method (see <xref linkend="Table.read" />), but it uses a sequence
            (<literal>coords</literal>) of row indexes to select the wanted
            columns, instead of a column range.</para>

            <para>The selected rows are returned in an array or record array
            of the current flavor.</para>
          </section>

          <section id="Table.readSorted"  xreflabel="description">
            <title>readSorted(sortby, checkCSI=False, field=None, start=None,
            stop=None, step=None)</title>

            <para>Read table data following the order of the index
            of <literal>sortby</literal> column.</para>

            <para><literal>sortby</literal> column must have associated a
            <literal>full</literal> index.  If you want to ensure a fully
            sorted order, the index must be a CSI one.  You may want to use
            the <literal>checkCSI</literal> argument in order to explicitly
            check for the existence of a CSI index.</para>

            <para>If <literal>field</literal> is supplied only the named
            column will be selected.  If the column is not nested, an
            <emphasis>array</emphasis> of the current flavor will be returned;
            if it is, a <emphasis>record array</emphasis> will be used
            instead.  If no <literal>field</literal> is specified, all the
            columns will be returned in a record array of the current
            flavor.</para>

            <para>The meaning of the <literal>start</literal>,
            <literal>stop</literal> and <literal>step</literal> arguments is
            the same as in <literal>Table.read()</literal> (see <xref
            linkend="Table.read" />).  However, in this case a negative value
            of <literal>step</literal> is supported, meaning that the results
            will be returned in reverse sorted order.</para>

          </section>

          <section id="Table.__getitem__" xreflabel="description">
            <title>__getitem__(key)</title>

            <para>Get a row or a range of rows from the table.</para>

            <para>If <literal>key</literal> argument is an integer, the
            corresponding table row is returned as a record of the current
            flavor. If <literal>key</literal> is a slice, the range of rows
            determined by it is returned as a record array of the current
            flavor.</para>

            <para>In addition, NumPy-style point selections are supported.  In
            particular, if <literal>key</literal> is a list of row
            coordinates, the set of rows determined by it is returned.
            Furthermore, if <literal>key</literal> is an array of boolean
            values, only the coordinates where <literal>key</literal>
            is <literal>True</literal> are returned.  Note that for the latter
            to work it is necessary that <literal>key</literal> list would
            contain exactly as many rows as the table has.</para>

            <para>Example of use:</para>

            <screen>record = table[4]
recarray = table[4:1000:2]
recarray = table[[4,1000]]   # only retrieves rows 4 and 1000
recarray = table[[True, False, ..., True]]</screen>

            <para>Those statements are equivalent to:</para>

            <screen>record = table.read(start=4)[0]
recarray = table.read(start=4, stop=1000, step=2)
recarray = table.readCoordinates([4,1000])
recarray = table.readCoordinates([True, False, ..., True])</screen>

            <para>Here, you can see how indexing can be used as a shorthand
            for the <literal>read()</literal> (see <xref linkend="Table.read"
            />) and <literal>readCoordinates()</literal> (see
            <xref linkend="Table.readCoordinates" />) methods.</para>
          </section>

          <section id="Table.__iter__" xreflabel="description">
            <title>__iter__()</title>

            <para>Iterate over the table using a <literal>Row</literal>
            instance (see <xref linkend="RowClassDescr" xrefstyle="select:
            label" />).</para>

            <para>This is equivalent to calling
            <literal>Table.iterrows()</literal> (see <xref
            linkend="Table.iterrows" />) with default arguments, i.e. it
            iterates over <emphasis>all the rows</emphasis> in the
            table.</para>

            <para>Example of use:</para>

            <screen>result = [ row['var2'] for row in table
           if row['var1'] &lt;= 20 ]</screen>

            <para>Which is equivalent to:</para>

            <screen>result = [ row['var2'] for row in table.iterrows()
           if row['var1'] &lt;= 20 ]</screen>

            <note>
              <para>This iterator can be nested (see
              <literal>Table.where()</literal> —<xref linkend="Table.where"
              />— for an example).</para>
            </note>
          </section>
        </section>

        <section id="TableMethods_writing">
          <title><literal>Table</literal> methods — writing</title>

          <section id="Table.append" xreflabel="description">
            <title>append(rows)</title>

            <para>Append a sequence of <literal>rows</literal> to the end of
            the table.</para>

            <para>The <literal>rows</literal> argument may be any object which
            can be converted to a record array compliant with the table
            structure (otherwise, a <literal>ValueError</literal> is raised).
            This includes NumPy record arrays, <literal>RecArray</literal>
            (depracated) or <literal>NestedRecArray</literal> (deprecated)
            objects if <literal>numarray</literal> is available, lists of
            tuples or array records, and a string or Python buffer.</para>

            <para>Example of use:</para>

            <screen>from tables import *
class Particle(IsDescription):
    name        = StringCol(16, pos=1) # 16-character String
    lati        = IntCol(pos=2)        # integer
    longi       = IntCol(pos=3)        # integer
    pressure    = Float32Col(pos=4)    # float  (single-precision)
    temperature = FloatCol(pos=5)      # double (double-precision)

fileh = openFile('test4.h5', mode='w')
table = fileh.createTable(fileh.root, 'table', Particle, "A table")
# Append several rows in only one call
table.append([("Particle:     10", 10, 0, 10*10, 10**2),
              ("Particle:     11", 11, -1, 11*11, 11**2),
              ("Particle:     12", 12, -2, 12*12, 12**2)])
fileh.close()</screen>

            <!-- manual-only -->
            <para>See <xref linkend="NestedRecArrayClassDescr"
            xrefstyle="select: label" /> if you are using
            <literal>numarray</literal> (deprecated) and you want to append
            data to nested columns.</para>
          </section>

          <section id="Table.modifyColumn" xreflabel="description">
            <title>modifyColumn(start=None, stop=None, step=1, column=None,
            colname=None)</title>

            <para>Modify one single column in the row slice
            <literal>[start:stop:step]</literal>.</para>

            <para>The <literal>colname</literal> argument specifies the name
            of the column in the table to be modified with the data given in
            <literal>column</literal>.  This method returns the number of rows
            modified.  Should the modification exceed the length of the table,
            an <literal>IndexError</literal> is raised before changing
            data.</para>

            <para>The <literal>column</literal> argument may be any object
            which can be converted to a (record) array compliant with the
            structure of the column to be modified (otherwise, a
            <literal>ValueError</literal> is raised).  This includes NumPy
            (record) arrays, <literal>NumArray</literal> (deprecated),
            <literal>RecArray</literal> (deprecated) or
            <literal>NestedRecArray</literal> (deprecated) objects if
            <literal>numarray</literal> is available, Numeric arrays
            if available (deprecated), lists of scalars, tuples or array
            records, and a string or Python buffer.</para>

            <!-- manual-only -->
            <para>See <xref linkend="NestedRecArrayClassDescr"
            xrefstyle="select: label" /> if you are using
            <literal>numarray</literal> (deprecated) and you want to modify
            data in a nested column.</para>
          </section>

          <section id="Table.modifyColumns" xreflabel="description">
            <title>modifyColumns(start=None, stop=None, step=1, columns=None,
            names=None)</title>

            <para>Modify a series of columns in the row slice
            <literal>[start:stop:step]</literal>.</para>

            <para>The <literal>names</literal> argument specifies the names of
            the columns in the table to be modified with the data given in
            <literal>columns</literal>.  This method returns the number of
            rows modified.  Should the modification exceed the length of the
            table, an <literal>IndexError</literal> is raised before changing
            data.</para>

            <para>The <literal>columns</literal> argument may be any object
            which can be converted to a record array compliant with the
            structure of the columns to be modified (otherwise, a
            <literal>ValueError</literal> is raised).  This includes NumPy
            record arrays, <literal>RecArray</literal> (deprecated) or
            <literal>NestedRecArray</literal> (deprecated) objects if
            <literal>numarray</literal> is available, lists of tuples or array
            records, and a string or Python buffer.</para>

            <!-- manual-only -->
            <para>See <xref linkend="NestedRecArrayClassDescr"
            xrefstyle="select: label" /> if you are using
            <literal>numarray</literal> (deprecated) and you want to modify
            data in nested columns.</para>
          </section>

          <section id="Table.modifyCoordinates" xreflabel="description">
            <title>modifyCoordinates(coords, rows)</title>

            <para>Modify a series of rows in positions specified in
            <literal>coords</literal></para>

            <para>The values in the selected rows will be modified with the
              data given in <literal>rows</literal>.  This method returns the
              number of rows modified.</para>

            <para>The possible values for the <literal>rows</literal> argument
              are the same as in <literal>Table.append()</literal> (see
              <xref linkend="Table.append" />).</para>

          </section>

          <section id="Table.modifyRows" xreflabel="description">
            <title>modifyRows(start=None, stop=None, step=1,
            rows=None)</title>

            <para>Modify a series of rows in the slice
            <literal>[start:stop:step]</literal>.</para>

            <para>The values in the selected rows will be modified with the
            data given in <literal>rows</literal>.  This method returns the
            number of rows modified.  Should the modification exceed the
            length of the table, an <literal>IndexError</literal> is raised
            before changing data.</para>

            <para>The possible values for the <literal>rows</literal> argument
            are the same as in <literal>Table.append()</literal> (see <xref
            linkend="Table.append" />).</para>

            <!-- manual-only -->
            <para>See <xref linkend="NestedRecArrayClassDescr"
            xrefstyle="select: label" /> if you are using
            <literal>numarray</literal> (deprecated) and you want to modify
            data in nested columns.</para>
          </section>

          <section id="Table.removeRows" xreflabel="description">
            <title>removeRows(start, stop=None)</title>

            <para>Remove a range of rows in the table.</para>

            <para>If only <literal>start</literal> is supplied, only this row
            is to be deleted.  If a range is supplied, i.e. both the
            <literal>start</literal> and <literal>stop</literal> parameters
            are passed, all the rows in the range are removed. A
            <literal>step</literal> parameter is not supported, and it is not
            foreseen to be implemented anytime soon.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">start</emphasis></glossterm>

                <glossdef>
                  <para>Sets the starting row to be removed. It accepts
                  negative values meaning that the count starts from the end.
                  A value of 0 means the first row.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">stop</emphasis></glossterm>

                <glossdef>
                  <para>Sets the last row to be removed to
                  <literal>stop-1</literal>, i.e. the end point is omitted (in
                  the Python <literal>range()</literal> tradition). Negative
                  values are also accepted. A special value of
                  <literal>None</literal> (the default) means removing just
                  the row supplied in <literal>start</literal>.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="Table.__setitem__" xreflabel="description">
            <title>__setitem__(key, value)</title>

            <para>Set a row or a range of rows in the table.</para>

            <para>It takes different actions depending on the type of the
            <literal>key</literal> parameter: if it is an integer, the
            corresponding table row is set to <literal>value</literal> (a
            record or sequence capable of being converted to the table
            structure). If <literal>key</literal> is a slice, the row slice
            determined by it is set to <literal>value</literal> (a record
            array or sequence capable of being converted to the table
            structure).</para>

            <para>In addition, NumPy-style point selections are supported.  In
            particular, if <literal>key</literal> is a list of row
            coordinates, the set of rows determined by it is set
            to <literal>value</literal>.  Furthermore,
            if <literal>key</literal> is an array of boolean values, only the
            coordinates where <literal>key</literal>
            is <literal>True</literal> are set to values
            from <literal>value</literal>.  Note that for the latter to work
            it is necessary that <literal>key</literal> list would contain
            exactly as many rows as the table has.</para>

            <para>Example of use:</para>

            <screen># Modify just one existing row
table[2] = [456,'db2',1.2]
# Modify two existing rows
rows = numpy.rec.array([[457,'db1',1.2],[6,'de2',1.3]],
                       formats='i4,a3,f8')
table[1:30:2] = rows             # modify a table slice
table[[1,3]] = rows              # only modifies rows 1 and 3
table[[True,False,True]] = rows  # only modifies rows 0 and 2</screen>

            <para>Which is equivalent to:</para>

            <screen>table.modifyRows(start=2, rows=[456,'db2',1.2])
rows = numpy.rec.array([[457,'db1',1.2],[6,'de2',1.3]],
                       formats='i4,a3,f8')
table.modifyRows(start=1, stop=3, step=2, rows=rows)
table.modifyCoordinates([1,3,2], rows)
table.modifyCoordinates([True, False, True], rows)</screen>

            <para>Here, you can see how indexing can be used as a shorthand
            for the <literal>modifyRows()</literal> (see
            <xref linkend="Table.modifyRows" />)
            and <literal>modifyCoordinates()</literal> (see
            <xref linkend="Table.modifyCoordinates" />) methods.</para>
          </section>
        </section>

        <section id="TableMethods_querying">
          <title><literal>Table</literal> methods — querying</title>

          <section id="Table.getWhereList" xreflabel="description">
            <title>getWhereList(condition, condvars=None, sort=False,
            start=None, stop=None, step=None)</title>

            <para>Get the row coordinates fulfilling the given
            <literal>condition</literal>.</para>

            <para>The coordinates are returned as a list of the current
            flavor.  <literal>sort</literal> means that you want to retrieve
            the coordinates ordered. The default is to not sort them.</para>

            <para>The meaning of the other arguments is the same as in the
            <literal>Table.where()</literal> method (see <xref
            linkend="Table.where" />).</para>
          </section>

          <section id="Table.readWhere" xreflabel="description">
            <title>readWhere(condition, condvars=None, field=None, start=None,
            stop=None, step=None)</title>

            <para>Read table data fulfilling the given
            <emphasis>condition</emphasis>.</para>

            <para>This method is similar to <literal>Table.read()</literal>
            (see <xref linkend="Table.read" />), having their common arguments
            and return values the same meanings. However, only the rows
            fulfilling the <emphasis>condition</emphasis> are included in the
            result.</para>

            <para>The meaning of the other arguments is the same as in the
            <literal>Table.where()</literal> method (see <xref
            linkend="Table.where" />).</para>
          </section>

          <section id="Table.where" xreflabel="description">
            <title>where(condition, condvars=None, start=None, stop=None,
            step=None)</title>

            <para>Iterate over values fulfilling a
            <literal>condition</literal>.</para>

            <para>This method returns a <literal>Row</literal> iterator (see
            <xref linkend="RowClassDescr" xrefstyle="select: label" />) which
            only selects rows in the table that satisfy the given
            <literal>condition</literal> (an expression-like string).
            <!-- manual-only -->
            For more information on condition syntax, see <xref
            linkend="conditionSyntax" xrefstyle="select: label" />.</para>

            <para>The <literal>condvars</literal> mapping may be used to
            define the variable names appearing in the
            <literal>condition</literal>. <literal>condvars</literal> should
            consist of identifier-like strings pointing to
            <literal>Column</literal> (see <xref linkend="ColumnClassDescr"
            xrefstyle="select: label" />) instances <emphasis>of this
            table</emphasis>, or to other values (which will be converted to
            arrays). A default set of condition variables is provided where
            each top-level, non-nested column with an identifier-like name
            appears. Variables in <literal>condvars</literal> override the
            default ones.</para>

            <para>When <literal>condvars</literal> is not provided or
            <literal>None</literal>, the current local and global namespace is
            sought instead of <literal>condvars</literal>. The previous
            mechanism is mostly intended for interactive usage. To disable it,
            just specify a (maybe empty) mapping as
            <literal>condvars</literal>.</para>

            <para>If a range is supplied (by setting some of the
            <literal>start</literal>, <literal>stop</literal> or
            <literal>step</literal> parameters), only the rows in that range
            <literal>and</literal> fulfilling the <literal>condition</literal>
            are used. The meaning of the <literal>start</literal>,
            <literal>stop</literal> and <literal>step</literal> parameters is
            the same as in the <literal>range()</literal> Python function,
            except that negative values of <literal>step</literal> are
            <literal>not</literal> allowed. Moreover, if only
            <literal>start</literal> is specified, then
            <literal>stop</literal> will be set to
            <literal>start+1</literal>.</para>

            <para>When possible, indexed columns participating in the
            condition will be used to speed up the search. It is recommended
            that you place the indexed columns as left and out in the
            condition as possible. Anyway, this method has always better
            performance than regular Python selections on the table.
            <!-- manual-only -->
            Please check the <xref linkend="searchOptim" xrefstyle="select:
            label" /> for more information about the performance of the
            different searching modes.</para>

            <para>You can mix this method with regular Python selections in
            order to support even more complex queries. It is strongly
            recommended that you pass the most restrictive condition as the
            parameter to this method if you want to achieve maximum
            performance.</para>

            <para>Example of use:</para>

            <screen>>>> passvalues = [ row['col3'] for row in
...                table.where('(col1 > 0) &amp; (col2 &lt;= 20)', step=5)
...                if your_function(row['col2']) ]
>>> print "Values that pass the cuts:", passvalues</screen>

            <para>Note that, from PyTables 1.1 on, you can nest several
            iterators over the same table. For example:</para>

            <screen>for p in rout.where('pressure &lt; 16'):
    for q in rout.where('pressure &lt; 9'):
        for n in rout.where('energy &lt; 10'):
            print "pressure, energy:", p['pressure'], n['energy']</screen>

            <para>In this example, iterators returned by
            <literal>Table.where()</literal> have been used, but you may as
            well use any of the other reading iterators that
            <literal>Table</literal> objects offer. See the file
            <literal>examples/nested-iter.py</literal> for the full
            code.</para>

            <warning>
              <para>When in the middle of a table row iterator, you should not
              use methods that can change the number of rows in the table
              (like <literal>Table.append()</literal> or
              <literal>Table.removeRows()</literal>) or unexpected errors will
              happen.</para>
            </warning>
          </section>

          <section id="Table.whereAppend" xreflabel="description">
            <title>whereAppend(dstTable, condition, condvars=None, start=None,
            stop=None, step=None)</title>

            <para>Append rows fulfilling the <literal>condition</literal> to
            the <literal>dstTable</literal> table.</para>

            <para><literal>dstTable</literal> must be capable of taking the
            rows resulting from the query, i.e. it must have columns with the
            expected names and compatible types. The meaning of the other
            arguments is the same as in the <literal>Table.where()</literal>
            method (see <xref linkend="Table.where" />).</para>

            <para>The number of rows appended to <literal>dstTable</literal>
            is returned as a result.</para>
          </section>

          <section id="Table.willQueryUseIndexing" xreflabel="description">
            <title>willQueryUseIndexing(condition, condvars=None)</title>

            <para>Will a query for the <literal>condition</literal> use
            indexing?</para>

            <para>The meaning of the <literal>condition</literal> and
            <emphasis>condvars</emphasis> arguments is the same as in the
            <literal>Table.where()</literal> method (see <xref
            linkend="Table.where" />). If <literal>condition</literal> can use
            indexing, this method returns a frozenset with the path names of
            the columns whose index is usable. Otherwise, it returns an empty
            list.</para>

            <para>This method is mainly intended for testing. Keep in mind
            that changing the set of indexed columns or their dirtiness may
            make this method return different values for the same arguments at
            different times.</para>

          </section>
        </section>

        <section id="TableMethods_other">
          <title><literal>Table</literal> methods — other</title>

          <section id="Table.copy" xreflabel="description">
            <title>copy(newparent=None, newname=None, overwrite=False,
            createparents=False, **kwargs)</title>

            <para>Copy this table and return the new one.</para>

            <para>This method has the behavior and keywords described in
            <literal>Leaf.copy()</literal> (see <xref linkend="Leaf.copy" />.
            Moreover, it recognises the next additional keyword
            arguments:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis
                role="bold">sortby</emphasis></glossterm>

                <glossdef>
                  <para>If specified, and <literal>sortby</literal>
                  corresponds to a column with an index, then the copy will be
                  sorted by this index.  If you want to ensure a fully sorted
                  order, the index must be a CSI one.  A reverse sorted copy
                  can be achieved by specifying a negative value for
                  the <literal>step</literal> keyword.
                  If <literal>sortby</literal> is omitted
                  or <literal>None</literal>, the original table order is
                  used.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">checkCSI</emphasis></glossterm>

                <glossdef>
                  <para>If rue and a CSI index does not exist for the
                  <literal>sortby</literal> column, an error will be raised.
                  If false (the default), it does nothing.  You can use this
                  flag in order to explicitly check for the existence of a
                  CSI index.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">propindexes</emphasis></glossterm>

                <glossdef>
                  <para>If true, the existing indexes in the source table are
                  propagated (created) to the new one.  If false (the
                  default), the indexes are not propagated.</para>
                </glossdef>
              </glossentry>
            </glosslist>

          </section>

          <section id="Table.flushRowsToIndex" xreflabel="description">
            <title>flushRowsToIndex()</title>

            <para>Add remaining rows in buffers to non-dirty indexes.</para>

            <para>This can be useful when you have chosen non-automatic
            indexing for the table (see the <literal>Table.autoIndex</literal>
            property in <xref linkend="TableInstanceVariablesDescr"
            xrefstyle="select: label" />) and you want to update the indexes
            on it.</para>

          </section>

          <section id="Table.getEnum" xreflabel="description">
            <title>getEnum(colname)</title>

            <para>Get the enumerated type associated with the named
            column.</para>

            <para>If the column named <literal>colname</literal> (a string)
            exists and is of an enumerated type, the corresponding
            <literal>Enum</literal> instance (see <xref
            linkend="EnumClassDescr" xrefstyle="select: label" />) is
            returned. If it is not of an enumerated type, a
            <literal>TypeError</literal> is raised. If the column does not
            exist, a <literal>KeyError</literal> is raised.</para>
          </section>

          <section id="Table.reIndex" xreflabel="description">
            <title>reIndex()</title>

            <para>Recompute all the existing indexes in the table.</para>

            <para>This can be useful when you suspect that, for any reason,
            the index information for columns is no longer valid and want to
            rebuild the indexes on it.</para>

          </section>

          <section id="Table.reIndexDirty" xreflabel="description">
            <title>reIndexDirty()</title>

            <para>Recompute the existing indexes in table,
            <emphasis>if</emphasis> they are dirty.</para>

            <para>This can be useful when you have set
            <literal>Table.autoIndex</literal> (see <xref
            linkend="TableInstanceVariablesDescr" xrefstyle="select: label"
            />) to false for the table and you want to update the indexes
            after a invalidating index operation
            (<literal>Table.removeRows()</literal>, for example).</para>

          </section>
        </section>

        <section id="DescriptionClassDescr">
          <title>The <literal>Description</literal> class</title>

          <para>This class represents descriptions of the structure of
          tables.</para>

          <para>An instance of this class is automatically bound to
          <literal>Table</literal> (see <xref linkend="TableClassDescr"
          xrefstyle="select: label" />) objects when they are created.  It
          provides a browseable representation of the structure of the table,
          made of non-nested (<literal>Col</literal> —see <xref
          linkend="ColClassDescr" xrefstyle="select: label" />) and nested
          (<literal>Description</literal>) columns. It also contains
          information that will allow you to build
          <literal>NestedRecArray</literal> (see <xref
          linkend="NestedRecArrayClassDescr" xrefstyle="select: label" />)
          objects suited for the different columns in a table (be they nested
          or not).</para>

          <para>Column definitions under a description can be accessed as
          attributes of it (<emphasis>natural naming</emphasis>). For
          instance, if <literal>table.description</literal> is a
          <literal>Description</literal> instance with a column named
          <literal>col1</literal> under it, the later can be accessed as
          <literal>table.description.col1</literal>. If
          <literal>col1</literal> is nested and contains a
          <literal>col2</literal> column, this can be accessed as
          <literal>table.description.col1.col2</literal>. Because of natural
          naming, the names of members start with special prefixes, like in
          the <literal>Group</literal> class (see <xref
          linkend="GroupClassDescr" xrefstyle="select: label" />).</para>

          <section>
            <title><literal>Description</literal> instance variables</title>

            <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_colObjects</emphasis></glossterm>

                <glossdef>
                  <para>A dictionary mapping the names of the columns hanging
                  directly from the associated table or nested column to their
                  respective descriptions (<literal>Col</literal> —see <xref
                  linkend="ColClassDescr" xrefstyle="select: label" />— or
                  <literal>Description</literal> —see <xref
                  linkend="DescriptionClassDescr" xrefstyle="select: label"
                  />— instances).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_dflts</emphasis></glossterm>

                <glossdef>
                  <para>A dictionary mapping the names of non-nested columns
                  hanging directly from the associated table or nested column
                  to their respective default values.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_dtype</emphasis></glossterm>

                <glossdef>
                  <para>The NumPy type which reflects the structure of this
                  table or nested column.  You can use this as the
                  <literal>dtype</literal> argument of NumPy array
                  factories.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_dtypes</emphasis></glossterm>

                <glossdef>
                  <para>A dictionary mapping the names of non-nested columns
                  hanging directly from the associated table or nested column
                  to their respective NumPy types.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_is_nested</emphasis></glossterm>

                <glossdef>
                  <para>Whether the associated table or nested column contains
                  further nested columns or not.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_itemsize</emphasis></glossterm>

                <glossdef>
                  <para>The size in bytes of an item in this table or nested
                  column.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_name</emphasis></glossterm>

                <glossdef>
                  <para>The name of this description group. The name of the
                  root group is <literal>'/'</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_names</emphasis></glossterm>

                <glossdef>
                  <para>A list of the names of the columns hanging directly
                  from the associated table or nested column. The order of the
                  names matches the order of their respective columns in the
                  containing table.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_nestedDescr</emphasis></glossterm>

                <glossdef>
                  <para>A nested list of pairs of <literal>(name,
                  format)</literal> tuples for all the columns under this
                  table or nested column. You can use this as the
                  <literal>dtype</literal> and <literal>descr</literal>
                  arguments of NumPy array and
                  <literal>NestedRecArray</literal> (see <xref
                  linkend="NestedRecArrayClassDescr" xrefstyle="select: label"
                  />) factories, respectively.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_nestedFormats</emphasis></glossterm>

                <glossdef>
                  <para>A nested list of the NumPy string formats (and shapes)
                  of all the columns under this table or nested column. You
                  can use this as the <literal>formats</literal> argument of
                  NumPy array and <literal>NestedRecArray</literal> (see <xref
                  linkend="NestedRecArrayClassDescr" xrefstyle="select: label"
                  />) factories.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_nestedlvl</emphasis></glossterm>

                <glossdef>
                  <para>The level of the associated table or nested column in
                  the nested datatype.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_nestedNames</emphasis></glossterm>

                <glossdef>
                  <para>A nested list of the names of all the columns under
                  this table or nested column. You can use this as the
                  <literal>names</literal> argument of NumPy array and
                  <literal>NestedRecArray</literal> (see <xref
                  linkend="NestedRecArrayClassDescr" xrefstyle="select: label"
                  />) factories.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_pathnames</emphasis></glossterm>

                <glossdef>
                  <para>A list of the pathnames of all the columns under this
                  table or nested column (in preorder).  If it does not
                  contain nested columns, this is exactly the same as the
                  <literal>Description._v_names</literal> attribute.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_types</emphasis></glossterm>

                <glossdef>
                  <para>A dictionary mapping the names of non-nested columns
                  hanging directly from the associated table or nested column
                  to their respective PyTables types.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title><literal>Description</literal> methods</title>

            <section>
              <title>_f_walk(type='All')</title>

              <para>Iterate over nested columns.</para>

              <para>If <literal>type</literal> is <literal>'All'</literal>
              (the default), all column description objects
              (<literal>Col</literal> and <literal>Description</literal>
              instances) are yielded in top-to-bottom order (preorder).</para>

              <para>If <literal>type</literal> is <literal>'Col'</literal> or
              <literal>'Description'</literal>, only column descriptions of
              that type are yielded.</para>
            </section>
          </section>
        </section>

        <section id="RowClassDescr">
          <title>The <literal>Row</literal> class</title>

          <para>Table row iterator and field accessor.</para>

          <para>Instances of this class are used to fetch and set the values
          of individual table fields.  It works very much like a dictionary,
          where keys are the pathnames or positions (extended slicing is
          supported) of the fields in the associated table in a specific
          row.</para>

          <para>This class provides an <emphasis>iterator interface</emphasis>
          so that you can use the same <literal>Row</literal> instance to
          access successive table rows one after the other.  There are also
          some important methods that are useful for accessing, adding and
          modifying values in tables.</para>

          <section>
            <title><literal>Row</literal> instance variables</title>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">nrow</emphasis></glossterm>

                <glossdef>
                  <para>The current row number.</para>

                  <para>This property is useful for knowing which row is being
                  dealt with in the middle of a loop or iterator.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section id="RowMethods">
            <title><literal>Row</literal> methods</title>

            <section id="Row.append" xreflabel="description">
              <title>append()</title>

              <para>Add a new row of data to the end of the dataset.</para>

              <para>Once you have filled the proper fields for the current
              row, calling this method actually appends the new data to the
              <emphasis>output buffer</emphasis> (which will eventually be
              dumped to disk).  If you have not set the value of a field, the
              default value of the column will be used.</para>

              <para>Example of use:</para>

              <screen>row = table.row
for i in xrange(nrows):
    row['col1'] = i-1
    row['col2'] = 'a'
    row['col3'] = -1.0
    row.append()
table.flush()</screen>

              <warning>
                <para>After completion of the loop in which
                <literal>Row.append()</literal> has been called, it is always
                convenient to make a call to <literal>Table.flush()</literal>
                in order to avoid losing the last rows that may still remain
                in internal buffers.</para>
              </warning>
            </section>

            <section id="Row.fetch_all_fields" xreflabel="description">
              <title>fetch_all_fields()</title>

              <para>Retrieve all the fields in the current row.</para>

              <para>Contrarily to <literal>row[:]</literal> (see <xref
              linkend="RowSpecialMethods" />), this returns row data as a
              NumPy void scalar.  For instance:</para>

              <screen>[row.fetch_all_fields() for row in table.where('col1 &lt; 3')]</screen>

              <para>will select all the rows that fulfill the given condition
              as a list of NumPy records.</para>
            </section>

            <section id="Row.update" xreflabel="description">
              <title>update()</title>

              <para>Change the data of the current row in the dataset.</para>

              <para>This method allows you to modify values in a table when
              you are in the middle of a table iterator like
              <literal>Table.iterrows()</literal> (see <xref
              linkend="Table.iterrows" />) or <literal>Table.where()</literal>
              (see <xref linkend="Table.where" />).</para>

              <para>Once you have filled the proper fields for the current
              row, calling this method actually changes data in the
              <emphasis>output buffer</emphasis> (which will eventually be
              dumped to disk).  If you have not set the value of a field, its
              original value will be used.</para>

              <para>Examples of use:</para>

              <screen>for row in table.iterrows(step=10):
    row['col1'] = row.nrow
    row['col2'] = 'b'
    row['col3'] = 0.0
    row.update()
table.flush()</screen>

              <para>which modifies every tenth row in table.  Or:</para>

              <screen>for row in table.where('col1 &gt; 3'):
    row['col1'] = row.nrow
    row['col2'] = 'b'
    row['col3'] = 0.0
    row.update()
table.flush()</screen>

              <para>which just updates the rows with values bigger than 3 in
              the first column.</para>

              <warning>
                <para>After completion of the loop in which
                <literal>Row.update()</literal> has been called, it is always
                convenient to make a call to <literal>Table.flush()</literal>
                in order to avoid losing changed rows that may still remain in
                internal buffers.</para>
              </warning>
            </section>
          </section>

          <section id="RowSpecialMethods">
            <title><literal>Row</literal> special methods</title>

            <section>
              <title>__contains__(item)</title>

              <para>Is <literal>item</literal> in this row?</para>

              <para>A true value is returned if <literal>item</literal> is
                found in current row, false otherwise.</para>
            </section>

            <section>
              <title>__getitem__(key)</title>

              <para>Get the row field specified by the
              <literal>key</literal>.</para>

              <para>The <literal>key</literal> can be a string (the name of
              the field), an integer (the position of the field) or a slice
              (the range of field positions). When <literal>key</literal> is a
              slice, the returned value is a <emphasis>tuple</emphasis>
              containing the values of the specified fields.</para>

              <para>Examples of use:</para>
              <screen>res = [row['var3'] for row in table.where('var2 &lt; 20')]</screen>
              <para>which selects the <literal>var3</literal> field for all
              the rows that fulfil the condition. Or:</para>
              <screen>res = [row[4] for row in table if row[1] &lt; 20]</screen>
              <para>which selects the field in the <emphasis>4th</emphasis>
              position for all the rows that fulfil the
              condition. Or:</para>
              <screen>res = [row[:] for row in table if row['var2'] &lt; 20]</screen>
              <para>which selects the all the fields (in the form of a
              <emphasis>tuple</emphasis>) for all the rows that fulfil the
              condition. Or:</para>
              <screen>res = [row[1::2] for row in table.iterrows(2, 3000, 3)]</screen>
              <para>which selects all the fields in even positions (in the
              form of a <emphasis>tuple</emphasis>) for all the rows in the
              slice <literal>[2:3000:3]</literal>.</para>
            </section>

            <section>
              <title>__setitem__(key, value)</title>

              <para>Set the <literal>key</literal> row field to the specified
              <literal>value</literal>.</para>

              <para>Differently from its <literal>__getitem__()</literal>
              counterpart, in this case <literal>key</literal> can only be a
              string (the name of the field). The changes done via
              <literal>__setitem__()</literal> will not take effect on the
              data on disk until any of the <literal>Row.append()</literal>
              (see <xref linkend="Row.append" />) or
              <literal>Row.update()</literal> (see <xref linkend="Row.update"
              />) methods are called.</para>

              <para>Example of use:</para>

              <screen>for row in table.iterrows(step=10):
    row['col1'] = row.nrow
    row['col2'] = 'b'
    row['col3'] = 0.0
    row.update()
table.flush()</screen>

              <para>which modifies every tenth row in the table.</para>
            </section>
          </section>
        </section>

        <section id="ColsClassDescr">
          <title>The <literal>Cols</literal> class</title>

          <para>Container for columns in a table or nested column.</para>

          <para>This class is used as an <emphasis>accessor</emphasis> to the
          columns in a table or nested column.  It supports the
          <emphasis>natural naming</emphasis> convention, so that you can
          access the different columns as attributes which lead to
          <literal>Column</literal> instances (for non-nested columns) or
          other <literal>Cols</literal> instances (for nested columns).</para>

          <para>For instance, if <literal>table.cols</literal> is a
          <literal>Cols</literal> instance with a column named
          <literal>col1</literal> under it, the later can be accessed as
          <literal>table.cols.col1</literal>. If <literal>col1</literal> is
          nested and contains a <literal>col2</literal> column, this can be
          accessed as <literal>table.cols.col1.col2</literal> and so
          on. Because of natural naming, the names of members start with
          special prefixes, like in the <literal>Group</literal> class (see
          <xref linkend="GroupClassDescr" xrefstyle="select: label"
          />).</para>

          <para>Like the <literal>Column</literal> class (see <xref
          linkend="ColumnClassDescr" xrefstyle="select: label" />),
          <literal>Cols</literal> supports item access to read and write
          ranges of values in the table or nested column.</para>

          <section>
            <title><literal>Cols</literal> instance variables</title>

            <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_colnames</emphasis></glossterm>

                <glossdef>
                  <para>A list of the names of the columns hanging directly
                  from the associated table or nested column.  The order of
                  the names matches the order of their respective columns in
                  the containing table.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_colpathnames</emphasis></glossterm>

                <glossdef>
                  <para>A list of the pathnames of all the columns under the
                  associated table or nested column (in preorder).  If it does
                  not contain nested columns, this is exactly the same as the
                  <literal>Cols._v_colnames</literal> attribute.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_desc</emphasis></glossterm>

                <glossdef>
                  <para>The associated <literal>Description</literal> instance
                  (see <xref linkend="DescriptionClassDescr"
                  xrefstyle="select: label" />).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">_v_table</emphasis></glossterm>

                <glossdef>
                  <para>The parent <literal>Table</literal> instance (see
                  <xref linkend="TableClassDescr" xrefstyle="select: label"
                  />).</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title><literal>Cols</literal> methods</title>

            <section id="Cols._f_col" xreflabel="description">
              <title>_f_col(colname)</title>

              <para>Get an accessor to the column
              <literal>colname</literal>.</para>

              <para>This method returns a <literal>Column</literal> instance
              (see <xref linkend="ColumnClassDescr" xrefstyle="select: label"
              />) if the requested column is not nested, and a
              <literal>Cols</literal> instance (see <xref
              linkend="ColsClassDescr" xrefstyle="select: label" />) if it is.
              You may use full column pathnames in
              <literal>colname</literal>.</para>

              <para>Calling <literal>cols._f_col('col1/col2')</literal> is
              equivalent to using <literal>cols.col1.col2</literal>.  However,
              the first syntax is more intended for programmatic use.  It is
              also better if you want to access columns with names that are
              not valid Python identifiers.</para>
            </section>

            <section id="Cols.__getitem__" xreflabel="description">
              <title>__getitem__(key)</title>

              <para>Get a row or a range of rows from a table or nested
              column.</para>

              <para>If <literal>key</literal> argument is an integer, the
              corresponding nested type row is returned as a record of the
              current flavor. If <literal>key</literal> is a slice, the range
              of rows determined by it is returned as a record array of the
              current flavor.</para>

              <para>Example of use:</para>

              <screen>record = table.cols[4]  # equivalent to table[4]
recarray = table.cols.Info[4:1000:2]</screen>

              <para>Those statements are equivalent to:</para>

              <screen>nrecord = table.read(start=4)[0]
nrecarray = table.read(start=4, stop=1000, step=2).field('Info')</screen>

              <para>Here you can see how a mix of natural naming, indexing and
              slicing can be used as shorthands for the
              <literal>Table.read()</literal> (see <xref linkend="Table.read"
              />) method.</para>
            </section>

            <section id="Cols.__len__" xreflabel="description">
              <title>__len__()</title>

              <para>Get the number of top level columns in table.</para>

            </section>

            <section>
              <title>__setitem__(key)</title>

              <para>Set a row or a range of rows in a table or nested
              column.</para>

              <para>If <literal>key</literal> argument is an integer, the
              corresponding row is set to <literal>value</literal>. If
              <literal>key</literal> is a slice, the range of rows determined
              by it is set to <literal>value</literal>.</para>

              <para>Example of use:</para>

              <screen>table.cols[4] = record
table.cols.Info[4:1000:2] = recarray</screen>

              <para>Those statements are equivalent to:</para>

              <screen>table.modifyRows(4, rows=record)
table.modifyColumn(4, 1000, 2, colname='Info', column=recarray)</screen>

              <para>Here you can see how a mix of natural naming, indexing and
              slicing can be used as shorthands for the
              <literal>Table.modifyRows()</literal> (see <xref
              linkend="Table.modifyRows" />) and
              <literal>Table.modifyColumn()</literal> (see <xref
              linkend="Table.modifyColumn" />) methods.</para>
            </section>
          </section>
        </section>

        <section id="ColumnClassDescr">
          <title>The <literal>Column</literal> class</title>

          <para>Accessor for a non-nested column in a table.</para>

          <para>Each instance of this class is associated with one
          <emphasis>non-nested</emphasis> column of a table. These instances
          are mainly used to read and write data from the table columns using
          item access (like the <literal>Cols</literal> class —see <xref
          linkend="ColsClassDescr" xrefstyle="select: label" />), but there
          are a few other associated methods to deal with indexes.</para>

          <section>
            <title><literal>Column</literal> instance variables</title>

            <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis role="bold">descr</emphasis></glossterm>

                <glossdef>
                  <para>The <literal>Description</literal> (see <xref
                  linkend="DescriptionClassDescr" xrefstyle="select: label"
                  />) instance of the parent table or nested column.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">dtype</emphasis></glossterm>

                <glossdef>
                  <para>The NumPy <literal>dtype</literal> that most closely
                  matches this column.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">index</emphasis></glossterm>

                <glossdef>
                  <para>The <literal>Index</literal> instance (see <xref
                  linkend="IndexClassDescr" xrefstyle="select: label" />)
                  associated with this column (<literal>None</literal> if the
                  column is not indexed).
                  </para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">is_indexed</emphasis></glossterm>

                <glossdef>
                  <para>True if the column is indexed, false otherwise.
                  </para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">maindim</emphasis></glossterm>

                <glossdef>
                  <para>The dimension along which iterators work.</para>
                  <para>Its value is 0 (i.e. the first dimension).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">name</emphasis></glossterm>

                <glossdef>
                  <para>The name of the associated column.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">pathname</emphasis></glossterm>

                <glossdef>
                  <para>The complete pathname of the associated column (the
                  same as <literal>Column.name</literal> if the column is not
                  inside a nested column).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">shape</emphasis></glossterm>

                <glossdef>
                  <para>The shape of this column.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">table</emphasis></glossterm>

                <glossdef>
                  <para>The parent <literal>Table</literal> instance (see
                  <xref linkend="TableClassDescr" xrefstyle="select: label"
                  />).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">type</emphasis></glossterm>

                <glossdef>
                  <para>The PyTables type of the column (a string).</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title><literal>Column</literal> methods</title>

            <section id="Column.createIndex" xreflabel="description">
              <title><literal>createIndex(optlevel=6, kind="medium",
              filters=None, tmp_dir=None)</literal></title>

              <para>Create an index for this column.</para>

              <para>Keyword arguments:</para>

              <glosslist>
                <glossentry>
                  <glossterm><emphasis role="bold">optlevel</emphasis></glossterm>
                  <glossdef>
                    <para>The optimization level for building the index.  The
                    levels ranges from 0 (no optimization) up to 9 (maximum
                    optimization).  Higher levels of optimization mean better
                    chances for reducing the entropy of the index at the price
                    of using more CPU, memory and I/O resources for creating
                    the index.</para>
                  </glossdef>
                </glossentry>

                <glossentry>
                  <glossterm><emphasis role="bold">kind</emphasis></glossterm>

                  <glossdef>
                    <para>The kind of the index to be built.  It can take the
                    <literal>'ultralight'</literal>, <literal>'light'</literal>,
                    <literal>'medium'</literal> or <literal>'full'</literal>
                    values.  Lighter kinds (<literal>'ultralight'</literal>
                    and <literal>'light'</literal>) mean that the index takes
                    less space on disk, but will perform queries slower.
                    Heavier kinds (<literal>'medium'</literal>
                    and <literal>'full'</literal>) mean better chances for
                    reducing the entropy of the index (increasing the query
                    speed) at the price of using more disk space as well as
                    more CPU, memory and I/O resources for creating the
                    index.</para>

                    <para>Note that selecting a <literal>full</literal> kind
                    with an <literal>optlevel</literal> of 9 (the maximum)
                    guarantees the creation of an index with zero entropy,
                    that is, a completely sorted index (CSI) — provided that
                    the number of rows in the table does not exceed the 2**48
                    figure (that is more than 100 trillions of rows).  See
                    <literal>Column.createCSIndex()</literal>
                    (<xref linkend="Column.createCSIndex" />) method for a
                    more direct way to create a CSI index.</para>
                  </glossdef>
                </glossentry>

                <glossentry>
                  <glossterm><emphasis role="bold">filters</emphasis></glossterm>
                  <glossdef>
                    <para>Specify the <literal>Filters</literal> instance used
                     to compress the index.  If <literal>None</literal>,
                     default index filters will be used (currently, zlib level
                     1 with shuffling).</para>
                  </glossdef>
                </glossentry>

                <glossentry>
                  <glossterm><emphasis role="bold">tmp_dir</emphasis></glossterm>
                  <glossdef>
                    <para>When <literal>kind</literal> is other
                    than <literal>'ultralight'</literal>, a temporary file is
                    created during the index build process.  You can use the
                    <literal>tmp_dir</literal> argument to specify the
                    directory for this temporary file.  The default is to
                    create it in the same directory as the file containing the
                    original table.</para>
                  </glossdef>

                </glossentry>
              </glosslist>

              <warning>
                <para>In some situations it is useful to get a completely
                  sorted index (CSI).  For those cases, it is best to use the
                  <literal>createCSIndex()</literal> method instead (see
                  <xref linkend="Column.createCSIndex" />).</para>
              </warning>

            </section>

            <section id="Column.createCSIndex" xreflabel="description">
              <title><literal>createCSIndex(filters=None,
              tmp_dir=None)</literal></title>

              <para>Create a completely sorted index (CSI) for this
              column.</para>

              <para>This method guarantees the creation of an index with zero
              entropy, that is, a completely sorted index (CSI) -- provided
              that the number of rows in the table does not exceed the 2**48
              figure (that is more than 100 trillions of rows).  A CSI index
              is needed for some table methods (like
              <literal>Table.itersorted()</literal> or
              <literal>Table.readSorted()</literal>) in order to ensure
              completely sorted results.</para>

              <para>For the meaning of <literal>filters</literal> and
              <literal>tmp_dir</literal> arguments see
              <literal>Column.createIndex()</literal> (<xref
              linkend="Column.createIndex" />).</para>

              <note>
                <para>This method is equivalent to
                  <literal>Column.createIndex(optlevel=9, kind='full',
                  ...)</literal>.</para>
              </note>
            </section>

            <section id="Colum.reIndex" xreflabel="description">
              <title><literal>reIndex()</literal></title>

              <para>Recompute the index associated with this column.</para>

              <para>This can be useful when you suspect that, for any reason,
              the index information is no longer valid and you want to rebuild
              it.</para>

              <para>This method does nothing if the column is not
              indexed.</para>

            </section>

            <section id="Column.reIndexDirty" xreflabel="description">
              <title><literal>reIndexDirty()</literal></title>

              <para>Recompute the associated index only if it is dirty.</para>

              <para>This can be useful when you have set
              <literal>Table.autoIndex</literal> (see <xref
              linkend="TableInstanceVariablesDescr" xrefstyle="select: label"
              />) to false for the table and you want to update the column's
              index after an invalidating index operation
              (like. <literal>Table.removeRows()</literal> —see <xref
              linkend="Table.removeRows" />).</para>

              <para>This method does nothing if the column is not
              indexed.</para>

            </section>

            <section id="Column.removeIndex" xreflabel="description">
              <title><literal>removeIndex()</literal></title>

              <para>Remove the index associated with this column.</para>

              <para>This method does nothing if the column is not indexed. The
              removed index can be created again by calling the
              <literal>Column.createIndex()</literal> method (see <xref
              linkend="Column.createIndex" />).</para>

            </section>
          </section>

          <section>
            <title><literal>Column</literal> special methods</title>

            <section id="Column.__getitem__" xreflabel="description">
              <title><literal>__getitem__(key)</literal></title>

              <para>Get a row or a range of rows from a column.</para>

              <para>If <literal>key</literal> argument is an integer, the
              corresponding element in the column is returned as an object of
              the current flavor.  If <literal>key</literal> is a slice, the
              range of elements determined by it is returned as an array of
              the current flavor.</para>

              <para>Example of use:</para>

              <screen>print "Column handlers:"
for name in table.colnames:
    print table.cols._f_col(name)

print "Select table.cols.name[1]-->", table.cols.name[1]
print "Select table.cols.name[1:2]-->", table.cols.name[1:2]
print "Select table.cols.name[:]-->", table.cols.name[:]
print "Select table.cols._f_col('name')[:]-->", table.cols._f_col('name')[:]</screen>

              <para>The output of this for a certain arbitrary table
              is:</para>

              <screen>Column handlers:
/table.cols.name (Column(), string, idx=None)
/table.cols.lati (Column(), int32, idx=None)
/table.cols.longi (Column(), int32, idx=None)
/table.cols.vector (Column(2,), int32, idx=None)
/table.cols.matrix2D (Column(2, 2), float64, idx=None)
Select table.cols.name[1]--> Particle:     11
Select table.cols.name[1:2]--> ['Particle:     11']
Select table.cols.name[:]--> ['Particle:     10'
 'Particle:     11' 'Particle:     12'
 'Particle:     13' 'Particle:     14']
Select table.cols._f_col('name')[:]--> ['Particle:     10'
 'Particle:     11' 'Particle:     12'
 'Particle:     13' 'Particle:     14']</screen>

              <para>See the <literal>examples/table2.py</literal> file for a
              more complete example.</para>
            </section>

            <section id="Column.__len__" xreflabel="description">
              <title>__len__()</title>

              <para>Get the number of elements in the column.</para>

              <para>This matches the length in rows of the parent
              table.</para>
            </section>

            <section id="Column.__setitem__" xreflabel="description">
              <title>__setitem__(key, value)</title>

              <para>Set a row or a range of rows in a column.</para>

              <para>If <literal>key</literal> argument is an integer, the
              corresponding element is set to <literal>value</literal>.  If
              <literal>key</literal> is a slice, the range of elements
              determined by it is set to <literal>value</literal>.</para>

              <para>Example of use:</para>

              <screen># Modify row 1
table.cols.col1[1] = -1
# Modify rows 1 and 3
table.cols.col1[1::2] = [2,3]</screen>

              <para>Which is equivalent to:</para>

              <screen># Modify row 1
table.modifyColumns(start=1, columns=[[-1]], names=['col1'])
# Modify rows 1 and 3
columns = numpy.rec.fromarrays([[2,3]], formats='i4')
table.modifyColumns(start=1, step=2, columns=columns, names=['col1'])</screen>
            </section>
          </section>
        </section>
      </section>

      <section id="ArrayClassDescr">
        <title>The <literal>Array</literal> class</title>

        <para>This class represents homogeneous datasets in an HDF5
        file.</para>

        <para>This class provides methods to write or read data to or from
        array objects in the file. This class does not allow you neither to
        enlarge nor compress the datasets on disk; use the
        <literal>EArray</literal> class (see <xref linkend="EArrayClassDescr"
        xrefstyle="select: label" />) if you want enlargeable dataset support
        or compression features, or <literal>CArray</literal> (see <xref
        linkend="CArrayClassDescr" xrefstyle="select: label" />) if you just
        want compression.</para>

        <para>An interesting property of the <literal>Array</literal> class is
        that it remembers the <emphasis>flavor</emphasis> of the object that
        has been saved so that if you saved, for example, a
        <literal>list</literal>, you will get a <literal>list</literal> during
        readings afterwards; if you saved a NumPy array, you will get a NumPy
        object, and so forth.</para>

        <para>Note that this class inherits all the public attributes and
        methods that <literal>Leaf</literal> (see <xref
        linkend="LeafClassDescr" xrefstyle="select: label" />) already
        provides. However, as <literal>Array</literal> instances have no
        internal I/O buffers, it is not necessary to use the
        <literal>flush()</literal> method they inherit from
        <literal>Leaf</literal> in order to save their internal state to disk.
        When a writing method call returns, all the data is already on
        disk.</para>

        <section id="ArrayClassInstanceVariables">
          <title><literal>Array</literal> instance variables</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">atom</emphasis></glossterm>

              <glossdef>
                <para>An <literal>Atom</literal> (see <xref
                linkend="AtomClassDescr" xrefstyle="select: label" />)
                instance representing the <emphasis>type</emphasis> and
                <emphasis>shape</emphasis> of the atomic objects to be
                saved.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">rowsize</emphasis></glossterm>

              <glossdef>
                <para>The size of the rows in dimensions orthogonal to
                <emphasis>maindim</emphasis>.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">nrow</emphasis></glossterm>

              <glossdef>
                <para>On iterators, this is the index of the current
                row.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title><literal>Array</literal> methods</title>

          <section id="Array.getEnum" xreflabel="description">
            <title>getEnum()</title>

            <para>Get the enumerated type associated with this array.</para>

            <para>If this array is of an enumerated type, the corresponding
            <literal>Enum</literal> instance (see <xref
            linkend="EnumClassDescr" xrefstyle="select: label" />) is
            returned. If it is not of an enumerated type, a
            <literal>TypeError</literal> is raised.</para>
          </section>

          <section id="Array.iterrows" xreflabel="description">
            <title>iterrows(start=None, stop=None, step=None)</title>

            <para>Iterate over the rows of the array.</para>

            <para>This method returns an iterator yielding an object of the
            current flavor for each selected row in the array.  The returned
            rows are taken from the <emphasis>main
            dimension</emphasis>.</para>

            <para>If a range is not supplied, <emphasis>all the
            rows</emphasis> in the array are iterated upon —you can also use
            the <literal>Array.__iter__()</literal> special method (see <xref
            linkend="Array.__iter__" />) for that purpose.  If you only want
            to iterate over a given <emphasis>range of rows</emphasis> in the
            array, you may use the <literal>start</literal>,
            <literal>stop</literal> and <literal>step</literal> parameters,
            which have the same meaning as in <literal>Array.read()</literal>
            (see <xref linkend="Array.read" />).</para>

            <para>Example of use:</para>

            <screen>result = [row for row in arrayInstance.iterrows(step=4)]</screen>
          </section>

          <section id="Array.next" xreflabel="description">
            <title>next()</title>

            <para>Get the next element of the array during an
            iteration.</para>

            <para>The element is returned as an object of the current
            flavor.</para>
          </section>

          <section id="Array.read" xreflabel="description">
            <title>read(start=None, stop=None, step=None)</title>

            <para>Get data in the array as an object of the current
            flavor.</para>

            <para>The <literal>start</literal>, <literal>stop</literal> and
            <literal>step</literal> parameters can be used to select only a
            <emphasis>range of rows</emphasis> in the array.  Their meanings
            are the same as in the built-in <literal>range()</literal> Python
            function, except that negative values of <literal>step</literal>
            are not allowed yet. Moreover, if only <literal>start</literal> is
            specified, then <literal>stop</literal> will be set to
            <literal>start+1</literal>. If you do not specify neither
            <literal>start</literal> nor <literal>stop</literal>, then
            <emphasis>all the rows</emphasis> in the array are
            selected.</para>
          </section>
        </section>

        <section id="ArraySpecialMethods">
          <title><literal>Array</literal> special methods</title>

          <para>The following methods automatically trigger actions when an
          <literal>Array</literal> instance is accessed in a special way
          (e.g. <literal>array[2:3,...,::2]</literal> will be equivalent to a
          call to <literal>array.__getitem__((slice(2, 3, None), Ellipsis,
          slice(None, None, 2)))</literal>).</para>

          <section id="Array.__getitem__" xreflabel="description">
            <title>__getitem__(key)</title>

            <para>Get a row, a range of rows or a slice from the array.</para>

            <para>The set of tokens allowed for the <literal>key</literal> is
            the same as that for extended slicing in Python (including the
            <literal>Ellipsis</literal> or <literal>...</literal> token).  The
            result is an object of the current flavor; its shape depends on
            the kind of slice used as <literal>key</literal> and the shape of
            the array itself.</para>

            <para>Furthermore, NumPy-style fancy indexing, where a list of
            indices in a certain axis is specified, is also supported.  Note
            that only one list per selection is supported right now.  Finally,
            NumPy-style point and boolean selections are supported as
            well.</para>

            <para>Example of use:</para>

            <screen>array1 = array[4]                       # simple selection
array2 = array[4:1000:2]                # slice selection
array3 = array[1, ..., ::2, 1:4, 4:]    # general slice selection
array4 = array[1, [1,5,10], ..., -1]    # fancy selection
array5 = array[np.where(array[:] > 4)]  # point selection
array6 = array[array[:] > 4]            # boolean selection</screen>
          </section>

          <section id="Array.__iter__" xreflabel="description">
            <title>__iter__()</title>

            <para>Iterate over the rows of the array.</para>

            <para>This is equivalent to calling
            <literal>Array.iterrows()</literal> (see <xref
            linkend="Array.iterrows" />) with default arguments, i.e. it
            iterates over <emphasis>all the rows</emphasis> in the
            array.</para>

            <para>Example of use:</para>

            <screen>result = [row[2] for row in array]</screen>

            <para>Which is equivalent to:</para>

            <screen>result = [row[2] for row in array.iterrows()]</screen>
          </section>

          <section id="Array.__setitem__" xreflabel="description">
            <title>__setitem__(key, value)</title>

            <para>Set a row, a range of rows or a slice in the array.</para>

            <para>It takes different actions depending on the type of the
            <literal>key</literal> parameter: if it is an integer, the
            corresponding array row is set to <literal>value</literal> (the
            value is broadcast when needed).  If <literal>key</literal> is a
            slice, the row slice determined by it is set to
            <literal>value</literal> (as usual, if the slice to be updated
            exceeds the actual shape of the array, only the values in the
            existing range are updated).</para>

            <para>If <literal>value</literal> is a multidimensional object,
            then its shape must be compatible with the shape determined by
            <literal>key</literal>, otherwise, a <literal>ValueError</literal>
            will be raised.</para>

            <para>Furthermore, NumPy-style fancy indexing, where a list of
            indices in a certain axis is specified, is also supported.  Note
            that only one list per selection is supported right now.  Finally,
            NumPy-style point and boolean selections are supported as well.
            </para>

            <para>Example of use:</para>

            <screen>a1[0] = 333        # assign an integer to a Integer Array row
a2[0] = 'b'        # assign a string to a string Array row
a3[1:4] = 5        # broadcast 5 to slice 1:4
a4[1:4:2] = 'xXx'  # broadcast 'xXx' to slice 1:4:2
# General slice update (a5.shape = (4,3,2,8,5,10).
a5[1, ..., ::2, 1:4, 4:] = numpy.arange(1728, shape=(4,3,2,4,3,6))
a6[1, [1,5,10], ..., -1] = arr    # fancy selection
a7[np.where(a6[:] > 4)] = 4       # point selection + broadcast
a8[arr > 4] = arr2                # boolean selection</screen>
          </section>
        </section>
      </section>

      <section id="CArrayClassDescr">
        <title>The <literal>CArray</literal> class</title>

        <para>This class represents homogeneous datasets in an HDF5
        file.</para>

        <para>The difference between a <literal>CArray</literal> and a normal
        <literal>Array</literal> (see <xref linkend="ArrayClassDescr"
        xrefstyle="select: label" />), from which it inherits, is that a
        <literal>CArray</literal> has a chunked layout and, as a consequence,
        it supports compression.  You can use datasets of this class to easily
        save or load arrays to or from disk, with compression support
        included.</para>

        <section>
          <title>Example of use</title>

          <para>See below a small example of the use of the
          <literal>CArray</literal> class.  The code is available in
          <literal>examples/carray1.py</literal>:</para>

          <screen>import numpy
import tables

fileName = 'carray1.h5'
shape = (200, 300)
atom = tables.UInt8Atom()
filters = tables.Filters(complevel=5, complib='zlib')

h5f = tables.openFile(fileName, 'w')
ca = h5f.createCArray(h5f.root, 'carray', atom, shape, filters=filters)
# Fill a hyperslab in ``ca``.
ca[10:60, 20:70] = numpy.ones((50, 50))
h5f.close()

# Re-open and read another hyperslab
h5f = tables.openFile(fileName)
print h5f
print h5f.root.carray[8:12, 18:22]
h5f.close()</screen>

          <para>The output for the previous script is something like:</para>

          <screen>carray1.h5 (File) ''
Last modif.: 'Thu Apr 12 10:15:38 2007'
Object Tree:
/ (RootGroup) ''
/carray (CArray(200, 300), shuffle, zlib(5)) ''

[[0 0 0 0]
 [0 0 0 0]
 [0 0 1 1]
 [0 0 1 1]]</screen>
        </section>
      </section>

      <section id="EArrayClassDescr">
        <title>The <literal>EArray</literal> class</title>

        <para>This class represents extendable, homogeneous datasets in an
        HDF5 file.</para>

        <para>The main difference between an <literal>EArray</literal> and a
        <literal>CArray</literal> (see <xref linkend="CArrayClassDescr"
        xrefstyle="select: label" />), from which it inherits, is that the
        former can be enlarged along one of its dimensions, the
        <emphasis>enlargeable dimension</emphasis>.  That means that the
        <literal>Leaf.extdim</literal> attribute (see <xref
        linkend="LeafInstanceVariables" xrefstyle="select: label" />) of any
        <literal>EArray</literal> instance will always be non-negative.
        Multiple enlargeable dimensions might be supported in the
        future.</para>

        <para>New rows can be added to the end of an enlargeable array by
        using the <literal>EArray.append()</literal> method (see <xref
        linkend="EArray.append" />).</para>

        <section id="EArrayMethodsDescr">
          <title><literal>EArray</literal> methods</title>

          <section id="EArray.append" xreflabel="description">
            <title>append(sequence)</title>

            <para>Add a <literal>sequence</literal> of data to the end of the
            dataset.</para>

            <para>The sequence must have the same type as the array; otherwise
            a <literal>TypeError</literal> is raised. In the same way, the
            dimensions of the <literal>sequence</literal> must conform to the
            shape of the array, that is, all dimensions must match, with the
            exception of the enlargeable dimension, which can be of any length
            (even 0!).  If the shape of the <literal>sequence</literal> is
            invalid, a <literal>ValueError</literal> is raised.</para>
          </section>
        </section>

        <section>
          <title>Example of use</title>

          <para>See below a small example of the use of the
          <literal>EArray</literal> class.  The code is available in
          <literal>examples/earray1.py</literal>:</para>

          <screen>import tables
import numpy

fileh = tables.openFile('earray1.h5', mode='w')
a = tables.StringAtom(itemsize=8)
# Use ``a`` as the object type for the enlargeable array.
array_c = fileh.createEArray(fileh.root, 'array_c', a, (0,), "Chars")
array_c.append(numpy.array(['a'*2, 'b'*4], dtype='S8'))
array_c.append(numpy.array(['a'*6, 'b'*8, 'c'*10], dtype='S8'))

# Read the string ``EArray`` we have created on disk.
for s in array_c:
    print 'array_c[%s] => %r' % (array_c.nrow, s)
# Close the file.
fileh.close()</screen>

          <para>The output for the previous script is something like:</para>

          <screen>array_c[0] => 'aa'
array_c[1] => 'bbbb'
array_c[2] => 'aaaaaa'
array_c[3] => 'bbbbbbbb'
array_c[4] => 'cccccccc'</screen>
        </section>
      </section>

      <section id="VLArrayClassDescr">
        <title>The <literal>VLArray</literal> class</title>

        <para>This class represents variable length (ragged) arrays in an HDF5
        file.</para>

        <para>Instances of this class represent array objects in the object
        tree with the property that their rows can have a
        <emphasis>variable</emphasis> number of homogeneous elements, called
        <emphasis>atoms</emphasis>. Like <literal>Table</literal> datasets
        (see <xref linkend="TableClassDescr" xrefstyle="select: label" />),
        variable length arrays can have only one dimension, and the elements
        (atoms) of their rows can be fully multidimensional.
        <literal>VLArray</literal> objects do also support compression.</para>

        <para>When reading a range of rows from a <literal>VLArray</literal>,
        you will <emphasis>always</emphasis> get a Python list of objects of
        the current flavor (each of them for a row), which may have different
        lengths.</para>

        <para>This class provides methods to write or read data to or from
        variable length array objects in the file. Note that it also inherits
        all the public attributes and methods that <literal>Leaf</literal>
        (see <xref linkend="LeafClassDescr" xrefstyle="select: label" />)
        already provides.</para>

        <section>
          <title><literal>VLArray</literal> instance variables</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">atom</emphasis></glossterm>

              <glossdef>
                <para>An <literal>Atom</literal> (see <xref
                linkend="AtomClassDescr" xrefstyle="select: label" />)
                instance representing the <emphasis>type</emphasis> and
                <emphasis>shape</emphasis> of the atomic objects to be
                saved. You may use a <emphasis>pseudo-atom</emphasis> for
                storing a serialized object or variable length string per
                row.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">flavor</emphasis></glossterm>

              <glossdef>
                <para>The type of data object read from this leaf.</para>

                <para>Please note that when reading several rows of
                <literal>VLArray</literal> data, the flavor only applies to
                the <emphasis>components</emphasis> of the returned Python
                list, not to the list itself.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">nrow</emphasis></glossterm>

              <glossdef>
                <para>On iterators, this is the index of the current
                row.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title><literal>VLArray</literal> methods</title>

          <section id="VLArray.append" xreflabel="description">
            <title>append(sequence)</title>

            <para>Add a <literal>sequence</literal> of data to the end of the
            dataset.</para>

            <para>This method appends the objects in the
            <literal>sequence</literal> to a <emphasis>single row</emphasis>
            in this array. The type and shape of individual objects must be
            compliant with the atoms in the array. In the case of serialized
            objects and variable length strings, the object or string to
            append is itself the <literal>sequence</literal>.</para>
          </section>

          <section id="VLArray.getEnum" xreflabel="description">
            <title>getEnum()</title>

            <para>Get the enumerated type associated with this array.</para>

            <para>If this array is of an enumerated type, the corresponding
            <literal>Enum</literal> instance (see <xref
            linkend="EnumClassDescr" xrefstyle="select: label" />) is
            returned. If it is not of an enumerated type, a
            <literal>TypeError</literal> is raised.</para>
          </section>

          <section id="VLArray.iterrows" xreflabel="description">
            <title>iterrows(start=None, stop=None, step=None)</title>

            <para>Iterate over the rows of the array.</para>

            <para>This method returns an iterator yielding an object of the
            current flavor for each selected row in the array.</para>

            <para>If a range is not supplied, <emphasis>all the
            rows</emphasis> in the array are iterated upon —you can also use
            the <literal>VLArray.__iter__()</literal> (see <xref
            linkend="VLArray.__iter__" />) special method for that purpose.
            If you only want to iterate over a given <emphasis>range of
            rows</emphasis> in the array, you may use the
            <literal>start</literal>, <literal>stop</literal> and
            <literal>step</literal> parameters, which have the same meaning as
            in <literal>VLArray.read()</literal> (see <xref
            linkend="VLArray.read" />).</para>

            <para>Example of use:</para>

            <screen>for row in vlarray.iterrows(step=4):
    print '%s[%d]--> %s' % (vlarray.name, vlarray.nrow, row)</screen>
          </section>

          <section id="VLArray.next" xreflabel="description">
            <title>next()</title>

            <para>Get the next element of the array during an
            iteration.</para>

            <para>The element is returned as a list of objects of the current
            flavor.</para>
          </section>

          <section id="VLArray.read" xreflabel="description">
            <title>read(start=None, stop=None, step=1)</title>

            <para>Get data in the array as a list of objects of the current
            flavor.</para>

            <para>Please note that, as the lengths of the different rows are
            variable, the returned value is a <emphasis>Python list</emphasis>
            (not an array of the current flavor), with as many entries as
            specified rows in the range parameters.</para>

            <para>The <literal>start</literal>, <literal>stop</literal> and
            <literal>step</literal> parameters can be used to select only a
            <emphasis>range of rows</emphasis> in the array.  Their meanings
            are the same as in the built-in <literal>range()</literal> Python
            function, except that negative values of <literal>step</literal>
            are not allowed yet. Moreover, if only <literal>start</literal> is
            specified, then <literal>stop</literal> will be set to
            <literal>start+1</literal>. If you do not specify neither
            <literal>start</literal> nor <literal>stop</literal>, then
            <emphasis>all the rows</emphasis> in the array are
            selected.</para>
          </section>
        </section>

        <section id="VLArraySpecialMethods">
          <title><literal>VLArray</literal> special methods</title>

          <para>The following methods automatically trigger actions when a
          <literal>VLArray</literal> instance is accessed in a special way
          (e.g., <literal>vlarray[2:5]</literal> will be equivalent to a call
          to <literal>vlarray.__getitem__(slice(2, 5, None)</literal>).</para>

          <section id="VLArray.__getitem__" xreflabel="description">
            <title>__getitem__(key)</title>

            <para>Get a row or a range of rows from the array.</para>

            <para>If <literal>key</literal> argument is an integer, the
            corresponding array row is returned as an object of the current
            flavor.  If <literal>key</literal> is a slice, the range of rows
            determined by it is returned as a list of objects of the current
            flavor.</para>

            <para>In addition, NumPy-style point selections are supported.  In
            particular, if <literal>key</literal> is a list of row
            coordinates, the set of rows determined by it is returned.
            Furthermore, if <literal>key</literal> is an array of boolean
            values, only the coordinates where <literal>key</literal>
            is <literal>True</literal> are returned.  Note that for the latter
            to work it is necessary that <literal>key</literal> list would
            contain exactly as many rows as the array has.</para>

            <para>Example of use:</para>

            <screen>a_row = vlarray[4]
a_list = vlarray[4:1000:2]
a_list2 = vlarray[[0,2]]   # get list of coords
a_list3 = vlarray[[0,-2]]  # negative values accepted
a_list4 = vlarray[numpy.array([True,...,False])]  # array of bools</screen>
          </section>

          <section id="VLArray.__iter__" xreflabel="description">
            <title>__iter__()</title>

            <para>Iterate over the rows of the array.</para>

            <para>This is equivalent to calling
            <literal>VLArray.iterrows()</literal> (see <xref
            linkend="VLArray.iterrows" />) with default arguments, i.e. it
            iterates over <emphasis>all the rows</emphasis> in the
            array.</para>
            <para>Example of use:</para>

            <screen>result = [row for row in vlarray]</screen>

            <para>Which is equivalent to:</para>

            <screen>result = [row for row in vlarray.iterrows()]</screen>
          </section>

          <section id="VLArray.__setitem__" xreflabel="description">
            <title>__setitem__(key, value)</title>

            <para>Set a row, or set of rows, in the array.</para>

            <para>It takes different actions depending on the type of the
            <literal>key</literal> parameter: if it is an integer, the
            corresponding table row is set to <literal>value</literal> (a
            record or sequence capable of being converted to the table
            structure). If <literal>key</literal> is a slice, the row slice
            determined by it is set to <literal>value</literal> (a record
            array or sequence capable of being converted to the table
            structure).</para>

            <para>In addition, NumPy-style point selections are supported.  In
            particular, if <literal>key</literal> is a list of row
            coordinates, the set of rows determined by it is set
            to <literal>value</literal>.  Furthermore,
            if <literal>key</literal> is an array of boolean values, only the
            coordinates where <literal>key</literal>
            is <literal>True</literal> are set to values
            from <literal>value</literal>.  Note that for the latter to work
            it is necessary that <literal>key</literal> list would contain
            exactly as many rows as the table has.</para>

            <note>
              <para>When updating the rows of a <literal>VLArray</literal>
              object which uses a pseudo-atom, there is a problem: you can
              only update values with <emphasis>exactly</emphasis> the same
              size in bytes than the original row.  This is very difficult to
              meet with <literal>object</literal> pseudo-atoms, because
              <literal>cPickle</literal> applied on a Python object does not
              guarantee to return the same number of bytes than over another
              object, even if they are of the same class.  This effectively
              limits the kinds of objects than can be updated in
              variable-length arrays.</para>
            </note>

            <para>Example of use:</para>

            <screen>vlarray[0] = vlarray[0] * 2 + 3
vlarray[99] = arange(96) * 2 + 3
# Negative values for the index are supported.
vlarray[-99] = vlarray[5] * 2 + 3
vlarray[1:30:2] = list_of_rows
vlarray[[1,3]] = new_1_and_3_rows</screen>
          </section>
        </section>

        <section>
          <title>Example of use</title>

          <para>See below a small example of the use of the
          <literal>VLArray</literal> class.  The code is available in
          <literal>examples/vlarray1.py</literal>:</para>

          <screen>import tables
from numpy import *

# Create a VLArray:
fileh = tables.openFile('vlarray1.h5', mode='w')
vlarray = fileh.createVLArray(fileh.root, 'vlarray1',
                              tables.Int32Atom(shape=()),
                              "ragged array of ints",
                              filters=tables.Filters(1))
# Append some (variable length) rows:
vlarray.append(array([5, 6]))
vlarray.append(array([5, 6, 7]))
vlarray.append([5, 6, 9, 8])

# Now, read it through an iterator:
print '-->', vlarray.title
for x in vlarray:
    print '%s[%d]--> %s' % (vlarray.name, vlarray.nrow, x)

# Now, do the same with native Python strings.
vlarray2 = fileh.createVLArray(fileh.root, 'vlarray2',
                              tables.StringAtom(itemsize=2),
                              "ragged array of strings",
                              filters=tables.Filters(1))
vlarray2.flavor = 'python'
# Append some (variable length) rows:
print '-->', vlarray2.title
vlarray2.append(['5', '66'])
vlarray2.append(['5', '6', '77'])
vlarray2.append(['5', '6', '9', '88'])

# Now, read it through an iterator:
for x in vlarray2:
    print '%s[%d]--> %s' % (vlarray2.name, vlarray2.nrow, x)

# Close the file.
fileh.close()</screen>

          <para>The output for the previous script is something like:</para>

          <screen>--> ragged array of ints
vlarray1[0]--> [5 6]
vlarray1[1]--> [5 6 7]
vlarray1[2]--> [5 6 9 8]
--> ragged array of strings
vlarray2[0]--> ['5', '66']
vlarray2[1]--> ['5', '6', '77']
vlarray2[2]--> ['5', '6', '9', '88']</screen>
        </section>
      </section>

      <section id="LinkClassDescr">
        <title>The <literal>Link</literal> class</title>

        <para>Abstract base class for all PyTables links.</para>

        <para>A link is a node that refers to another node.
          The <literal>Link</literal> class inherits
          from <literal>Node</literal> class and the links that inherits
          from <literal>Link</literal> are <literal>SoftLink</literal>
          and <literal>ExternalLink</literal>.  There is not
          a <literal>HardLink</literal> subclass because hard links behave
          like a regular <literal>Group</literal> or <literal>Leaf</literal>.
          Contrarily to other nodes, links cannot have HDF5 attributes.  This
          is an HDF5 library limitation that might be solved in future
          releases.
        </para>

        <para>See <xref linkend="LinksTutorial"/> for a small tutorial on how
        to work with links.</para>

        <section>
          <title><literal>Link</literal> instance variables</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">_v_attrs</emphasis></glossterm>

              <glossdef>
                <para>A <literal>NoAttrs</literal> instance replacing the
                  typical <emphasis>AttributeSet</emphasis> instance of other
                  node objects.  The purpose of <literal>NoAttrs</literal> is
                  to make clear that HDF5 attributes are not supported in link
                  nodes.
                </para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">target</emphasis></glossterm>

              <glossdef>
                <para>The path string to the pointed node.</para>
              </glossdef>
            </glossentry>

          </glosslist>
        </section>

        <section>
          <title><literal>Link</literal> methods</title>

          <para>The following methods are useful for copying, moving, renaming
            and removing links.</para>

          <section id="Link.copy" xreflabel="description">
            <title>copy(newparent=None, newname=None, overwrite=False,
             createparents=False)</title>

            <para>Copy this link and return the new one.</para>

            <para>See <literal>Node._f_copy()</literal>
            (<xref linkend="Node._f_copy" />) for a complete explanation of
            the arguments.  Please note that there is no
            <literal>recursive</literal> flag since links do not have child
            nodes.</para>

          </section>

          <section id="Link.move" xreflabel="description">
            <title>move(newparent=None, newname=None, overwrite=False)</title>

            <para>Move or rename this link.</para>

            <para>See <literal>Node._f_move()</literal>
            (<xref linkend="Node._f_move" />) for a complete explanation of
            the arguments.</para>

          </section>

          <section id="Link.remove" xreflabel="description">
            <title>remove()</title>

            <para>Remove this link from the hierarchy.</para>

          </section>

          <section id="Link.rename" xreflabel="description">
            <title>rename(newname=None)</title>

            <para>Rename this link in place.</para>

            <para>See <literal>Node._f_rename()</literal>
            (<xref linkend="Node._f_rename" />) for a complete explanation of
            the arguments.</para>

          </section>
        </section>
      </section>


      <section id="SoftLinkClassDescr">
        <title>The <literal>SoftLink</literal> class</title>

        <para>Represents a soft link (aka symbolic link).</para>

        <para> A soft link is a reference to another node in
          the <emphasis>same</emphasis> file hierarchy.  Getting access to the
          pointed node (this action is
          called <emphasis>dereferrencing</emphasis>) is done via
          the <literal>__call__</literal> special method (see below).
        </para>

        <section>
          <title><literal>SoftLink</literal> special methods</title>

          <para>The following methods are specific for dereferrencing and
          representing soft links.</para>

          <section id="Link.__call__" xreflabel="description">
            <title>__call__()</title>

            <para>Dereference <literal>self.target</literal> and return the
            pointed object.</para>

            <para>Example of use:</para>

            <screen>>>> f=tables.openFile('data/test.h5')
>>> print f.root.link0
/link0 (SoftLink) -> /another/path
>>> print f.root.link0()
/another/path (Group) ''</screen>

          </section>

          <section id="Link.__str__" xreflabel="description">
            <title>__str__()</title>

            <para>Return a short string representation of the link.
            </para>

            <para>Example of use:</para>

            <screen>>>> f=tables.openFile('data/test.h5')
>>> print f.root.link0
/link0 (SoftLink) -> /path/to/node</screen>

          </section>
        </section>
      </section>


      <section id="ExternalLinkClassDescr">
        <title>The <literal>ExternalLink</literal> class</title>

        <para>Represents an external link.</para>

        <para>An external link is a reference to a node
        in <emphasis>another</emphasis> file.  Getting access to the pointed
        node (this action is called <emphasis>dereferrencing</emphasis>) is
        done via the <literal>__call__</literal> special method (see
        below).</para>

        <warning>
          <para>External links are only supported when PyTables is compiled
            against HDF5 1.8.x series.  When using PyTables with HDF5 1.6.x,
            the <emphasis>parent</emphasis> group containing external link
            objects will be mapped to an <literal>Unknown</literal> instance
            (see <xref linkend="UnknownClassDescr" xrefstyle="select: label"
            />) and you won't be able to access <emphasis>any</emphasis> node
            hanging of this parent group.  It follows that if the parent group
            containing the external link is the root group, you won't be able
            to read <emphasis>any</emphasis> information contained in the file
            when using HDF5 1.6.x.</para>
        </warning>

        <section>
          <title><literal>ExternalLink</literal> instance variables</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">extfile</emphasis></glossterm>

              <glossdef>
                <para>The external file handler, if the link has been
                  dereferenced.  In case the link has not been dereferenced
                  yet, its value is <literal>None</literal>.
                </para>

              </glossdef>
            </glossentry>

          </glosslist>
        </section>

        <section>
          <title><literal>ExternalLink</literal> methods</title>

          <section id="ExternalLink.umount" xreflabel="description">
            <title>umount()</title>

            <para>Safely unmount <literal>self.extfile</literal>, if
            opened.</para>

          </section>
        </section>

        <section>
          <title><literal>ExternalLink</literal> special methods</title>

          <para>The following methods are specific for dereferrencing and
            representing external links.</para>

          <section id="ExternalLink.__call__" xreflabel="description">
            <title>__call__(**kwargs)</title>

            <para>Dereference <literal>self.target</literal> and return the
            pointed object.</para>

            <para>You can pass all the arguments supported by the
              <literal>openFile()</literal> function
              (except <literal>filename</literal>, of course) so as to open
              the referenced external file.</para>

            <para>Example of use:</para>

            <screen>>>> f=tables.openFile('data1/test1.h5')
>>> print f.root.link2
/link2 (ExternalLink) -> data2/test2.h5:/path/to/node
>>> plink2 = f.root.link2('a')  # open in 'a'ppend mode
>>> print plink2
/path/to/node (Group) ''
>>> print plink2._v_filename
'data2/test2.h5'        # belongs to referenced file</screen>

          </section>

          <section id="ExternalLink.__str__" xreflabel="description">
            <title>__str__()</title>

            <para>Return a short string representation of the link.
            </para>

            <para>Example of use:</para>

            <screen>>>> f=tables.openFile('data1/test1.h5')
>>> print f.root.link2
/link2 (ExternalLink) -> data2/test2.h5:/path/to/node</screen>

          </section>
        </section>
      </section>


      <section id="UnImplementedClassDescr">
        <title>The <literal>UnImplemented</literal> class</title>

        <para>This class represents datasets not supported by PyTables in an
        HDF5 file.</para>

        <para>When reading a generic HDF5 file (i.e. one that has not been
        created with PyTables, but with some other HDF5 library based tool),
        chances are that the specific combination of datatypes or dataspaces
        in some dataset might not be supported by PyTables yet. In such a
        case, this dataset will be mapped into an
        <literal>UnImplemented</literal> instance and the user will still be
        able to access the complete object tree of the generic HDF5 file. The
        user will also be able to <emphasis>read and write the
        attributes</emphasis> of the dataset, <emphasis>access some of its
        metadata</emphasis>, and perform <emphasis>certain hierarchy
        manipulation operations</emphasis> like deleting or moving (but not
        copying) the node. Of course, the user will not be able to read the
        actual data on it.</para>

        <para>This is an elegant way to allow users to work with generic HDF5
        files despite the fact that some of its datasets are not supported by
        PyTables. However, if you are really interested in having full access
        to an unimplemented dataset, please get in contact with the developer
        team.</para>

        <para>This class does not have any public instance variables or
        methods, except those inherited from the <literal>Leaf</literal> class
        (see <xref linkend="LeafClassDescr" xrefstyle="select: label"
        />).</para>
      </section>


      <section id="UnknownClassDescr">
        <title>The <literal>Unknown</literal> class</title>

        <para>This class represents nodes reported
          as <emphasis>unknown</emphasis> by the underlying HDF5 library.
        </para>

        <para>This class does not have any public instance variables or
          methods, except those inherited from the <literal>Node</literal>
          class.</para>

      </section>


      <section id="AttributeSetClassDescr">
        <title>The <literal>AttributeSet</literal> class</title>

        <para>Container for the HDF5 attributes of a <literal>Node</literal>
        (see <xref linkend="NodeClassDescr" xrefstyle="select: label"
        />).</para>

        <para>This class provides methods to create new HDF5 node attributes,
        and to get, rename or delete existing ones.</para>

        <para>Like in <literal>Group</literal> instances (see <xref
        linkend="GroupClassDescr" xrefstyle="select: label" />),
        <literal>AttributeSet</literal> instances make use of the
        <emphasis>natural naming</emphasis> convention, i.e. you can access
        the attributes on disk as if they were normal Python attributes of the
        <literal>AttributeSet</literal> instance.</para>

        <para>This offers the user a very convenient way to access HDF5 node
        attributes. However, for this reason and in order not to pollute the
        object namespace, one can not assign <emphasis>normal</emphasis>
        attributes to <literal>AttributeSet</literal> instances, and their
        members use names which start by special prefixes as happens with
        <literal>Group</literal> objects.</para>

        <section>
          <title>Notes on native and pickled attributes</title>

          <para>The values of most basic types are saved as HDF5 native data
          in the HDF5 file.  This includes Python <literal>bool</literal>,
          <literal>int</literal>, <literal>float</literal>,
          <literal>complex</literal> and <literal>str</literal> (but not
          <literal>long</literal> nor <literal>unicode</literal>) values, as
          well as their NumPy scalar versions and homogeneous or
          <emphasis>structured</emphasis> NumPy arrays of them.  When read,
          these values are always loaded as NumPy scalar or array objects, as
          needed.</para>

          <para>For that reason, attributes in native HDF5 files will be
          always mapped into NumPy objects.  Specifically, a multidimensional
          attribute will be mapped into a multidimensional
          <literal>ndarray</literal> and a scalar will be mapped into a NumPy
          scalar object (for example, a scalar
          <literal>H5T_NATIVE_LLONG</literal> will be read and returned as a
          <literal>numpy.int64</literal> scalar).</para>

          <para>However, other kinds of values are serialized using
          <literal>cPickle</literal>, so you only will be able to correctly
          retrieve them using a Python-aware HDF5 library.  Thus, if you want
          to save Python scalar values and make sure you are able to read them
          with generic HDF5 tools, you should make use of <emphasis>scalar or
          homogeneous/structured array NumPy objects</emphasis> (for example,
          <literal>numpy.int64(1)</literal> or <literal>numpy.array([1, 2, 3],
          dtype='int16')</literal>).</para>

          <para>One more advice: because of the various potential difficulties
          in restoring a Python object stored in an attribute, you may end up
          getting a <literal>cPickle</literal> string where a Python object is
          expected. If this is the case, you may wish to run
          <literal>cPickle.loads()</literal> on that string to get an idea of
          where things went wrong, as shown in this example:</para>

          <screen>>>> import os, tempfile
>>> import tables
>>>
>>> class MyClass(object):
...   foo = 'bar'
...
>>> myObject = MyClass()  # save object of custom class in HDF5 attr
>>> h5fname = tempfile.mktemp(suffix='.h5')
>>> h5f = tables.openFile(h5fname, 'w')
>>> h5f.root._v_attrs.obj = myObject  # store the object
>>> print h5f.root._v_attrs.obj.foo  # retrieve it
bar
>>> h5f.close()
>>>
>>> del MyClass, myObject  # delete class of object and reopen file
>>> h5f = tables.openFile(h5fname, 'r')
>>> print repr(h5f.root._v_attrs.obj)
'ccopy_reg\n_reconstructor...
>>> import cPickle  # let's unpickle that to see what went wrong
>>> cPickle.loads(h5f.root._v_attrs.obj)
Traceback (most recent call last):
  ...
AttributeError: 'module' object has no attribute 'MyClass'
>>> # So the problem was not in the stored object,
... # but in the *environment* where it was restored.
... h5f.close()
>>> os.remove(h5fname)</screen>
        </section>

        <section id="AttributeSetClassInstanceVariables">
          <title><literal>AttributeSet</literal> instance variables</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis
              role="bold">_v_attrnames</emphasis></glossterm>

              <glossdef>
                <para>A list with all attribute names.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">_v_attrnamessys</emphasis></glossterm>

              <glossdef>
                <para>A list with system attribute names.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">_v_attrnamesuser</emphasis></glossterm>

              <glossdef>
                <para>A list with user attribute names.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis role="bold">_v_node</emphasis></glossterm>

              <glossdef>
                <para>The <literal>Node</literal> instance (see <xref
                linkend="NodeClassDescr" xrefstyle="select: label" />) this
                attribute set is associated with.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">_v_unimplemented</emphasis></glossterm>

              <glossdef>
                <para>A list of attribute names with unimplemented native HDF5
                types.</para>
              </glossdef>
            </glossentry>

          </glosslist>
        </section>

        <section>
          <title><literal>AttributeSet</literal> methods</title>

          <para>Note that this class overrides the
          <literal>__getattr__()</literal>, <literal>__setattr__()</literal>
          and <literal>__delattr__()</literal> special methods.  This allows
          you to read, assign or delete attributes on disk by just using the
          next constructs:</para>

          <screen>leaf.attrs.myattr = 'str attr'    # set a string (native support)
leaf.attrs.myattr2 = 3            # set an integer (native support)
leaf.attrs.myattr3 = [3, (1, 2)]  # a generic object (Pickled)
attrib = leaf.attrs.myattr        # get the attribute ``myattr``
del leaf.attrs.myattr             # delete the attribute ``myattr``</screen>

          <para>In addition, the dictionary-like
          <literal>__getitem__()</literal>, <literal>__setitem__()</literal>
          and <literal>__delitem__()</literal> methods are available, so you
          may write things like this:</para>

          <screen>for name in node._v_attrs._f_list():
    print "name: %s, value: %s" % (name, node._v_attrs[name])</screen>

         <para>Use whatever idiom you prefer to access the attributes.</para>

          <para>If an attribute is set on a target node that already has a
          large number of attributes, a <literal>PerformanceWarning</literal>
          will be issued.</para>

          <section>
            <title>_f_copy(where)</title>

            <para>Copy attributes to the <literal>where</literal> node.</para>

            <para>Copies all user and certain system attributes to the given
            <literal>where</literal> node (a <literal>Node</literal> instance
            —see <xref linkend="NodeClassDescr" xrefstyle="select: label" />),
            replacing the existing ones.</para>
          </section>

          <section>
            <title>_f_list(attrset='user')</title>

            <para>Get a list of attribute names.</para>

            <para>The <literal>attrset</literal> string selects the attribute
            set to be used.  A <literal>'user'</literal> value returns only
            user attributes (this is the default).  A <literal>'sys'</literal>
            value returns only system attributes.  Finally,
            <literal>'all'</literal> returns both system and user
            attributes.</para>
          </section>

          <section>
            <title>_f_rename(oldattrname, newattrname)</title>

            <para>Rename an attribute from <literal>oldattrname</literal> to
            <literal>newattrname</literal>.</para>
          </section>

          <section>
            <title>__contains__(name)</title>

            <para>Is there an attribute with that
            <literal>name</literal>?</para>

            <para>A true value is returned if the attribute set has an
            attribute with the given name, false otherwise.</para>
          </section>
        </section>
      </section>

      <section id="declarativeClasses">
        <title>Declarative classes</title>

        <para>In this section a series of classes that are meant to
        <emphasis>declare</emphasis> datatypes that are required for creating
        primary PyTables datasets are described.</para>

        <section id="AtomClassDescr">
          <title>The <literal>Atom</literal> class and its
          descendants.</title>

          <para>Defines the type of atomic cells stored in a dataset.</para>

          <para>The meaning of <emphasis>atomic</emphasis> is that individual
          elements of a cell can not be extracted directly by indexing (i.e.
          <literal>__getitem__()</literal>) the dataset; e.g. if a dataset has
          shape (2, 2) and its atoms have shape (3,), to get the third element
          of the cell at (1, 0) one should use
          <literal>dataset[1,0][2]</literal> instead of
          <literal>dataset[1,0,2]</literal>.</para>

          <para>The <literal>Atom</literal> class is meant to declare the
          different properties of the <emphasis>base element</emphasis> (also
          known as <emphasis>atom</emphasis>) of <literal>CArray</literal>,
          <literal>EArray</literal> and <literal>VLArray</literal> datasets,
          although they are also used to describe the base elements of
          <literal>Array</literal> datasets. Atoms have the property that
          their length is always the same.  However, you can grow datasets
          along the extensible dimension in the case of
          <literal>EArray</literal> or put a variable number of them on a
          <literal>VLArray</literal> row. Moreover, they are not
          restricted to scalar values, and they can be <emphasis>fully
          multidimensional objects</emphasis>.</para>

          <para>A series of descendant classes are offered in order to make
          the use of these element descriptions easier. You should use a
          particular <literal>Atom</literal> descendant class whenever you
          know the exact type you will need when writing your code. Otherwise,
          you may use one of the <literal>Atom.from_*()</literal> factory
          Methods.</para>

          <section>
            <title><literal>Atom</literal> instance variables</title>

            <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis role="bold">dflt</emphasis></glossterm>

                <glossdef>
                  <para>The default value of the atom.</para>

                  <para>If the user does not supply a value for an element
                  while filling a dataset, this default value will be written
                  to disk. If the user supplies a scalar value for a
                  multidimensional atom, this value is automatically
                  <emphasis>broadcast</emphasis> to all the items in the atom
                  cell. If <literal>dflt</literal> is not supplied, an
                  appropriate zero value (or <emphasis>null</emphasis> string)
                  will be chosen by default.  Please note that default values
                  are kept internally as NumPy objects.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">dtype</emphasis></glossterm>

                <glossdef>
                  <para>The NumPy <literal>dtype</literal> that most closely
                  matches this atom.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">itemsize</emphasis></glossterm>

                <glossdef>
                  <para>Size in bytes of a single item in the atom.</para>

                  <para>Specially useful for atoms of the
                  <literal>string</literal> kind.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">kind</emphasis></glossterm>

                <glossdef>
                  <para>The PyTables kind of the atom (a string).
                  <!-- manual-only -->
                  For a relation of the data kinds supported by PyTables and
                  more information about them, see <xref
                  linkend="datatypesSupported" xrefstyle="select: label"
                  />.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">recarrtype</emphasis></glossterm>

                <glossdef>
                  <para>String type to be used in
                  <literal>numpy.rec.array()</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">shape</emphasis></glossterm>

                <glossdef>
                  <para>The shape of the atom (a tuple, <literal>()</literal>
                  for scalar atoms).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">size</emphasis></glossterm>

                <glossdef>
                  <para>Total size in bytes of the atom.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">type</emphasis></glossterm>

                <glossdef>
                  <para>The PyTables type of the atom (a string).
                  <!-- manual-only -->
                  For a relation of the data types supported by PyTables and
                  more information about them, see <xref
                  linkend="datatypesSupported" xrefstyle="select: label"
                  />.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <para>Atoms can be compared with atoms and other objects for
            strict (in)equality without having to compare individual
            attributes:</para>

            <screen>>>> atom1 = StringAtom(itemsize=10)  # same as ``atom2``
>>> atom2 = Atom.from_kind('string', 10)  # same as ``atom1``
>>> atom3 = IntAtom()
>>> atom1 == 'foo'
False
>>> atom1 == atom2
True
>>> atom2 != atom1
False
>>> atom1 == atom3
False
>>> atom3 != atom2
True</screen>
          </section>

          <section>
            <title><literal>Atom</literal> methods</title>

            <section>
              <title>copy(**override)</title>

              <para>Get a copy of the atom, possibly overriding some
              arguments.</para>

              <para>Constructor arguments to be overridden must be passed as
              keyword arguments.</para>

              <screen>>>> atom1 = StringAtom(itemsize=12)
>>> atom2 = atom1.copy()
>>> print atom1
StringAtom(itemsize=12, shape=(), dflt='')
>>> print atom2
StringAtom(itemsize=12, shape=(), dflt='')
>>> atom1 is atom2
False
>>> atom3 = atom1.copy(itemsize=100, shape=(2, 2))
>>> print atom3
StringAtom(itemsize=100, shape=(2, 2), dflt='')
>>> atom1.copy(foobar=42)
Traceback (most recent call last):
  ...
TypeError: __init__() got an unexpected keyword argument 'foobar'</screen>
            </section>
          </section>

          <section>
            <title><literal>Atom</literal> factory methods</title>

            <section>
              <title>from_dtype(dtype, dflt=None)</title>

              <para>Create an <literal>Atom</literal> from a NumPy
              <literal>dtype</literal>.</para>

              <para>An optional default value may be specified as the
              <literal>dflt</literal> argument. Information in the
              <literal>dtype</literal> not represented in an
              <literal>Atom</literal> is ignored.</para>

              <screen>>>> import numpy
>>> Atom.from_dtype(numpy.dtype((numpy.int16, (2, 2))))
Int16Atom(shape=(2, 2), dflt=0)
>>> Atom.from_dtype(numpy.dtype('S5'), dflt='hello')
StringAtom(itemsize=5, shape=(), dflt='hello')
>>> Atom.from_dtype(numpy.dtype('Float64'))
Float64Atom(shape=(), dflt=0.0)</screen>
            </section>

            <section>
              <title>from_kind(kind, itemsize=None, shape=(),
              dflt=None)</title>

              <para>Create an <literal>Atom</literal> from a PyTables
              <literal>kind</literal>.</para>

              <para>Optional item size, shape and default value may be
              specified as the <literal>itemsize</literal>,
              <literal>shape</literal> and <literal>dflt</literal>
              arguments, respectively. Bear in mind that not all atoms support
              a default item size.</para>

              <screen>>>> Atom.from_kind('int', itemsize=2, shape=(2, 2))
Int16Atom(shape=(2, 2), dflt=0)
>>> Atom.from_kind('int', shape=(2, 2))
Int32Atom(shape=(2, 2), dflt=0)
>>> Atom.from_kind('int', shape=1)
Int32Atom(shape=(1,), dflt=0)
>>> Atom.from_kind('string', itemsize=5, dflt='hello')
StringAtom(itemsize=5, shape=(), dflt='hello')
>>> Atom.from_kind('string', dflt='hello')
Traceback (most recent call last):
  ...
ValueError: no default item size for kind ``string``
>>> Atom.from_kind('Float')
Traceback (most recent call last):
  ...
ValueError: unknown kind: 'Float'</screen>

              <para>Moreover, some kinds with atypical constructor signatures
              are not supported; you need to use the proper
              constructor:</para>

              <screen>>>> Atom.from_kind('enum')
Traceback (most recent call last):
  ...
ValueError: the ``enum`` kind is not supported...</screen>
            </section>

            <section>
              <title>from_sctype(sctype, shape=(), dflt=None)</title>

              <para>Create an <literal>Atom</literal> from a NumPy scalar type
              <literal>sctype</literal>.</para>

              <para>Optional shape and default value may be specified as the
              <literal>shape</literal> and <literal>dflt</literal>
              arguments, respectively. Information in the
              <literal>sctype</literal> not represented in an
              <literal>Atom</literal> is ignored.</para>

              <screen>>>> import numpy
>>> Atom.from_sctype(numpy.int16, shape=(2, 2))
Int16Atom(shape=(2, 2), dflt=0)
>>> Atom.from_sctype('S5', dflt='hello')
Traceback (most recent call last):
  ...
ValueError: unknown NumPy scalar type: 'S5'
>>> Atom.from_sctype('Float64')
Float64Atom(shape=(), dflt=0.0)</screen>
            </section>

            <section>
              <title>from_type(type, shape=(), dflt=None)</title>

              <para>Create an <literal>Atom</literal> from a PyTables
              <literal>type</literal>.</para>

              <para>Optional shape and default value may be specified as the
              <literal>shape</literal> and <literal>dflt</literal>
              arguments, respectively.</para>

              <screen>>>> Atom.from_type('bool')
BoolAtom(shape=(), dflt=False)
>>> Atom.from_type('int16', shape=(2, 2))
Int16Atom(shape=(2, 2), dflt=0)
>>> Atom.from_type('string40', dflt='hello')
Traceback (most recent call last):
  ...
ValueError: unknown type: 'string40'
>>> Atom.from_type('Float64')
Traceback (most recent call last):
  ...
ValueError: unknown type: 'Float64'</screen>
            </section>
          </section>

          <section id="AtomConstructors">
            <title><literal>Atom</literal> constructors</title>

            <para>There are some common arguments for most
            <literal>Atom</literal>-derived constructors:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis
                role="bold">itemsize</emphasis></glossterm>

                <glossdef>
                  <para>For types with a non-fixed size, this sets the size in
                  bytes of individual items in the atom.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">shape</emphasis></glossterm>

                <glossdef>
                  <para>Sets the shape of the atom. An integer shape of
                  <literal>N</literal> is equivalent to the tuple
                  <literal>(N,)</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">dflt</emphasis></glossterm>

                <glossdef>
                  <para>Sets the default value for the atom.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <!-- manual-only -->
            <para>A relation of the different constructors with their
            parameters follows.</para>

            <section>
              <title>StringAtom(itemsize, shape=(), dflt='')</title>

              <para>Defines an atom of type <literal>string</literal>.</para>

              <para>The item size is the <emphasis>maximum</emphasis> length
              in characters of strings.</para>
            </section>

            <section>
              <title>BoolAtom(shape=(), dflt=False)</title>

              <para>Defines an atom of type <literal>bool</literal>.</para>
            </section>

            <section>
              <title>IntAtom(itemsize=4, shape=(), dflt=0)</title>

              <para>Defines an atom of a signed integral type
              (<literal>int</literal> kind).</para>
            </section>

            <section>
              <title>Int8Atom(shape=(), dflt=0)</title>

              <para>Defines an atom of type <literal>int8</literal>.</para>
            </section>

            <section>
              <title>Int16Atom(shape=(), dflt=0)</title>

              <para>Defines an atom of type <literal>int16</literal>.</para>
            </section>

            <section>
              <title>Int32Atom(shape=(), dflt=0)</title>

              <para>Defines an atom of type <literal>int32</literal>.</para>
            </section>

            <section>
              <title>Int64Atom(shape=(), dflt=0)</title>

              <para>Defines an atom of type <literal>int64</literal>.</para>
            </section>

            <section>
              <title>UIntAtom(itemsize=4, shape=(), dflt=0)</title>

              <para>Defines an atom of an unsigned integral type
              (<literal>uint</literal> kind).</para>
            </section>

            <section>
              <title>UInt8Atom(shape=(), dflt=0)</title>

              <para>Defines an atom of type <literal>uint8</literal>.</para>
            </section>

            <section>
              <title>UInt16Atom(shape=(), dflt=0)</title>

              <para>Defines an atom of type <literal>uint16</literal>.</para>
            </section>

            <section>
              <title>UInt32Atom(shape=(), dflt=0)</title>

              <para>Defines an atom of type <literal>uint32</literal>.</para>
            </section>

            <section>
              <title>UInt64Atom(shape=(), dflt=0)</title>

              <para>Defines an atom of type <literal>uint64</literal>.</para>
            </section>

            <section>
              <title>Float32Atom(shape=(), dflt=0.0)</title>

              <para>Defines an atom of type <literal>float32</literal>.</para>
            </section>

            <section>
              <title>Float64Atom(shape=(), dflt=0.0)</title>

              <para>Defines an atom of type <literal>float64</literal>.</para>
            </section>

            <section>
              <title>ComplexAtom(itemsize, shape=(), dflt=0j)</title>

              <para>Defines an atom of kind <literal>complex</literal>.</para>

              <para>Allowed item sizes are 8 (single precision) and 16 (double
              precision). This class must be used instead of more concrete
              ones to avoid confusions with <literal>numarray</literal>-like
              precision specifications used in PyTables 1.X.</para>
            </section>

            <section>
              <title>TimeAtom(itemsize=4, shape=(), dflt=0)</title>

              <para>Defines an atom of time type (<literal>time</literal>
              kind).</para>

              <para>There are two distinct supported types of time: a 32 bit
              integer value and a 64 bit floating point value. Both of them
              reflect the number of seconds since the Unix epoch. This atom
              has the property of being stored using the HDF5 time
              datatypes.</para>
            </section>

            <section>
              <title>Time32Atom(shape=(), dflt=0)</title>

              <para>Defines an atom of type <literal>time32</literal>.</para>
            </section>

            <section>
              <title>Time64Atom(shape=(), dflt=0.0)</title>

              <para>Defines an atom of type <literal>time64</literal>.</para>
            </section>

            <section>
              <title>EnumAtom(enum, dflt, base, shape=())</title>

              <para>Description of an atom of an enumerated type.</para>

              <para>Instances of this class describe the atom type used to
              store enumerated values. Those values belong to an enumerated
              type, defined by the first argument (<literal>enum</literal>) in
              the constructor of the atom, which accepts the same kinds of
              arguments as the <literal>Enum</literal> class (see <xref
              linkend="EnumClassDescr" xrefstyle="select: label" />).  The
              enumerated type is stored in the <literal>enum</literal>
              attribute of the atom.</para>

              <para>A default value must be specified as the second argument
              (<literal>dflt</literal>) in the constructor; it must be the
              <emphasis>name</emphasis> (a string) of one of the enumerated
              values in the enumerated type. When the atom is created, the
              corresponding concrete value is broadcast and stored in the
              <literal>dflt</literal> attribute (setting different default
              values for items in a multidimensional atom is not supported
              yet). If the name does not match any value in the enumerated
              type, a <literal>KeyError</literal> is raised.</para>

              <para>Another atom must be specified as the
              <literal>base</literal> argument in order to determine the base
              type used for storing the values of enumerated values in memory
              and disk. This <emphasis>storage atom</emphasis> is kept in the
              <literal>base</literal> attribute of the created atom. As a
              shorthand, you may specify a PyTables type instead of the
              storage atom, implying that this has a scalar shape.</para>

              <para>The storage atom should be able to represent each and
              every concrete value in the enumeration. If it is not, a
              <literal>TypeError</literal> is raised. The default value of the
              storage atom is ignored.</para>

              <para>The <literal>type</literal> attribute of enumerated atoms
              is always <literal>enum</literal>.</para>

              <para>Enumerated atoms also support comparisons with other
              objects:</para>

              <screen>>>> enum = ['T0', 'T1', 'T2']
>>> atom1 = EnumAtom(enum, 'T0', 'int8')  # same as ``atom2``
>>> atom2 = EnumAtom(enum, 'T0', Int8Atom())  # same as ``atom1``
>>> atom3 = EnumAtom(enum, 'T0', 'int16')
>>> atom4 = Int8Atom()
>>> atom1 == enum
False
>>> atom1 == atom2
True
>>> atom2 != atom1
False
>>> atom1 == atom3
False
>>> atom1 == atom4
False
>>> atom4 != atom1
True</screen>

              <section>
                <title>Examples</title>

                <para>The next C <literal>enum</literal> construction:</para>

                <screen>enum myEnum {
  T0,
  T1,
  T2
};</screen>

                <para>would correspond to the following PyTables
                declaration:</para>

                <screen>>>> myEnumAtom = EnumAtom(['T0', 'T1', 'T2'], 'T0', 'int32')</screen>

                <para>Please note the <literal>dflt</literal> argument with a
                value of <literal>'T0'</literal>. Since the concrete value
                matching <literal>T0</literal> is unknown right now (we have
                not used explicit concrete values), using the name is the only
                option left for defining a default value for the atom.</para>

                <para>The chosen representation of values for this enumerated
                atom uses unsigned 32-bit integers, which surely wastes quite
                a lot of memory. Another size could be selected by using the
                <literal>base</literal> argument (this time with a full-blown
                storage atom):</para>

                <screen>>>> myEnumAtom = EnumAtom(['T0', 'T1', 'T2'], 'T0', UInt8Atom())</screen>

                <para>You can also define multidimensional arrays for data
                elements:</para>

                <screen>>>> myEnumAtom = EnumAtom(
...    ['T0', 'T1', 'T2'], 'T0', base='uint32', shape=(3,2))</screen>

                <para>for 3x2 arrays of <literal>uint32</literal>.</para>
              </section>
            </section>
          </section>

          <section id="PseudoAtomsDescr">
            <title>Pseudo atoms</title>

            <para>Now, there come three special classes,
            <literal>ObjectAtom</literal>, <literal>VLStringAtom</literal> and
            <literal>VLUnicodeAtom</literal>, that actually do not descend
            from <literal>Atom</literal>, but which goal is so similar that
            they should be described here. Pseudo-atoms can only be used with
            <literal>VLArray</literal> datasets (see <xref
            linkend="VLArrayClassDescr" xrefstyle="select: label" />), and
            they do not support multidimensional values, nor multiple values
            per row.</para>

            <para>They can be recognised because they also have
            <literal>kind</literal>, <literal>type</literal> and
            <literal>shape</literal> attributes, but no
            <literal>size</literal>, <literal>itemsize</literal> or
            <literal>dflt</literal> ones. Instead, they have a
            <literal>base</literal> atom which defines the elements used for
            storage.</para>

            <para>See <literal>examples/vlarray1.py</literal> and
            <literal>examples/vlarray2.py</literal> for further examples on
            <literal>VLArray</literal> datasets, including object
            serialization and string management.</para>

            <section>
              <title>ObjectAtom()</title>

              <para>Defines an atom of type <literal>object</literal>.</para>

              <para>This class is meant to fit <emphasis>any</emphasis> kind
              of Python object in a row of a <literal>VLArray</literal>
              dataset by using <literal>cPickle</literal> behind the
              scenes. Due to the fact that you can not foresee how long will
              be the output of the <literal>cPickle</literal> serialization
              (i.e. the atom already has a <emphasis>variable</emphasis>
              length), you can only fit <emphasis>one object per
              row</emphasis>. However, you can still group several objects in
              a single tuple or list and pass it to the
              <literal>VLArray.append()</literal> method (see <xref
              linkend="VLArray.append"/>).</para>

              <para>Object atoms do not accept parameters and they cause the
              reads of rows to always return Python objects. You can regard
              <literal>object</literal> atoms as an easy way to save an
              arbitrary number of generic Python objects in a
              <literal>VLArray</literal> dataset.</para>
            </section>

            <section id="VLStringAtom" xreflabel="description">
              <title>VLStringAtom()</title>

              <para>Defines an atom of type
              <literal>vlstring</literal>.</para>

              <para>This class describes a <emphasis>row</emphasis> of the
              <literal>VLArray</literal> class, rather than an atom. It
              differs from the <literal>StringAtom</literal> class in that you
              can only add <emphasis>one instance of it to one specific
              row</emphasis>, i.e. the <literal>VLArray.append()</literal>
              method (see <xref linkend="VLArray.append"/>) only accepts one
              object when the base atom is of this type.</para>

              <para>Like <literal>StringAtom</literal>, this class does not
              make assumptions on the encoding of the string, and raw bytes
              are stored as is.  Unicode strings are supported as long as no
              character is out of the ASCII set; otherwise, you will need to
              <emphasis>explicitly</emphasis> convert them to strings before
              you can save them.  For full Unicode support, using
              <literal>VLUnicodeAtom</literal> (see <xref
              linkend="VLUnicodeAtom" />) is recommended.</para>

              <para>Variable-length string atoms do not accept parameters and
              they cause the reads of rows to always return Python strings.
              You can regard <literal>vlstring</literal> atoms as an easy way
              to save generic variable length strings.</para>
            </section>

            <section id="VLUnicodeAtom" xreflabel="description">
              <title>VLUnicodeAtom()</title>

              <para>Defines an atom of type
              <literal>vlunicode</literal>.</para>

              <para>This class describes a <emphasis>row</emphasis> of the
              <literal>VLArray</literal> class, rather than an atom.  It is
              very similar to <literal>VLStringAtom</literal> (see <xref
              linkend="VLStringAtom" />), but it stores Unicode strings (using
              32-bit characters a la UCS-4, so all strings of the same length
              also take up the same space).</para>

              <para>This class does not make assumptions on the encoding of
              plain input strings.  Plain strings are supported as long as no
              character is out of the ASCII set; otherwise, you will need to
              <emphasis>explicitly</emphasis> convert them to Unicode before
              you can save them.</para>

              <para>Variable-length Unicode atoms do not accept parameters and
              they cause the reads of rows to always return Python Unicode
              strings.  You can regard <literal>vlunicode</literal> atoms as
              an easy way to save variable length Unicode strings.</para>
            </section>
          </section>
        </section>

        <section id="ColClassDescr">
          <title>The <literal>Col</literal> class and its descendants</title>

          <para>Defines a non-nested column.</para>

          <para><literal>Col</literal> instances are used as a means to
          declare the different properties of a non-nested column in a table
          or nested column.  <literal>Col</literal> classes are descendants of
          their equivalent <literal>Atom</literal> classes (see <xref
          linkend="AtomClassDescr" xrefstyle="select: label" />), but their
          instances have an additional <literal>_v_pos</literal> attribute
          that is used to decide the position of the column inside its parent
          table or nested column (see the <literal>IsDescription</literal>
          class in <xref linkend="IsDescriptionClassDescr" xrefstyle="select:
          label" /> for more information on column positions).</para>

          <para>In the same fashion as <literal>Atom</literal>, you should use
          a particular <literal>Col</literal> descendant class whenever you
          know the exact type you will need when writing your code. Otherwise,
          you may use one of the <literal>Col.from_*()</literal> factory
          methods.</para>

          <section>
            <title><literal>Col</literal> instance variables</title>

            <para>In addition to the variables that they inherit from the
            <literal>Atom</literal> class, <literal>Col</literal> instances
            have the following attributes:</para>

            <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis role="bold">_v_pos</emphasis></glossterm>

                <glossdef>
                  <para>The <emphasis>relative</emphasis> position of this
                  column with regard to its column siblings.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title><literal>Col</literal> factory methods</title>

            <para>Each factory method inherited from
            the <literal>Atom</literal> class is available with the same
            signature, plus an additional <literal>pos</literal> parameter
            (placed in last position) which defaults
            to <literal>None</literal> and that may take an integer value.
            This parameter might be used to specify the position of the column
            in the table.</para>

            <para>Besides, there are the next additional factory methods,
            available only for <literal>Col</literal> objects:</para>

            <section>
              <title>from_atom(atom, pos=None)</title>

              <para>Create a <literal>Col</literal> definition from a PyTables
              <literal>atom</literal>.</para>

              <para>An optional position may be specified as the
              <literal>pos</literal> argument.</para>
            </section>
          </section>

          <section id="ColConstructors">
            <title><literal>Col</literal> constructors</title>

            <para>There are some common arguments for most
            <literal>Col</literal>-derived constructors:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis
                role="bold">itemsize</emphasis></glossterm>

                <glossdef>
                  <para>For types with a non-fixed size, this sets the size in
                  bytes of individual items in the column.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">shape</emphasis></glossterm>

                <glossdef>
                  <para>Sets the shape of the column. An integer shape of
                  <literal>N</literal> is equivalent to the tuple
                  <literal>(N,)</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">dflt</emphasis></glossterm>

                <glossdef>
                  <para>Sets the default value for the column.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">pos</emphasis></glossterm>

                <glossdef>
                  <para>Sets the position of column in table.  If unspecified,
                  the position will be randomly selected.</para>
                </glossdef>
              </glossentry>
            </glosslist>

            <!-- manual-only -->
            <para>A relation of the different constructors with their
            parameters follows.  For more information about them, see
            its <literal>Atom</literal> ancestors documentation in
            <xref linkend="AtomConstructors"/>.
            </para>

            <section>
              <title>StringCol(itemsize, shape=(), dflt='', pos=None)</title>

              <para>Defines an column of type <literal>string</literal>.</para>
            </section>

            <section>
              <title>BoolCol(shape=(), dflt=False, pos=None)</title>

              <para>Defines an column of type <literal>bool</literal>.</para>
            </section>

            <section>
              <title>IntCol(itemsize=4, shape=(), dflt=0, pos=None)</title>

              <para>Defines an column of a signed integral type
              (<literal>int</literal> kind).</para>
            </section>

            <section>
              <title>Int8Col(shape=(), dflt=0, pos=None)</title>

              <para>Defines an column of type <literal>int8</literal>.</para>
            </section>

            <section>
              <title>Int16Col(shape=(), dflt=0, pos=None)</title>

              <para>Defines an column of type <literal>int16</literal>.</para>
            </section>

            <section>
              <title>Int32Col(shape=(), dflt=0, pos=None)</title>

              <para>Defines an column of type <literal>int32</literal>.</para>
            </section>

            <section>
              <title>Int64Col(shape=(), dflt=0, pos=None)</title>

              <para>Defines an column of type <literal>int64</literal>.</para>
            </section>

            <section>
              <title>UIntCol(itemsize=4, shape=(), dflt=0, pos=None)</title>

              <para>Defines an column of an unsigned integral type
              (<literal>uint</literal> kind).</para>
            </section>

            <section>
              <title>UInt8Col(shape=(), dflt=0, pos=None)</title>

              <para>Defines an column of type <literal>uint8</literal>.</para>
            </section>

            <section>
              <title>UInt16Col(shape=(), dflt=0, pos=None)</title>

              <para>Defines an column of type <literal>uint16</literal>.</para>
            </section>

            <section>
              <title>UInt32Col(shape=(), dflt=0, pos=None)</title>

              <para>Defines an column of type <literal>uint32</literal>.</para>
            </section>

            <section>
              <title>UInt64Col(shape=(), dflt=0, pos=None)</title>

              <para>Defines an column of type <literal>uint64</literal>.</para>
            </section>

            <section>
              <title>Float32Col(shape=(), dflt=0.0, pos=None)</title>

              <para>Defines an column of type <literal>float32</literal>.</para>
            </section>

            <section>
              <title>Float64Col(shape=(), dflt=0.0, pos=None)</title>

              <para>Defines an column of type <literal>float64</literal>.</para>
            </section>

            <section>
              <title>ComplexCol(itemsize, shape=(), dflt=0j, pos=None)</title>

              <para>Defines an column of kind <literal>complex</literal>.</para>
            </section>

            <section>
              <title>TimeCol(itemsize=4, shape=(), dflt=0, pos=None)</title>

              <para>Defines an column of time type (<literal>time</literal>
              kind).</para>
            </section>

            <section>
              <title>Time32Col(shape=(), dflt=0, pos=None)</title>

              <para>Defines an column of type <literal>time32</literal>.</para>
            </section>

            <section>
              <title>Time64Col(shape=(), dflt=0.0, pos=None)</title>

              <para>Defines an column of type <literal>time64</literal>.</para>
            </section>

            <section>
              <title>EnumCol(enum, dflt, base, shape=(), pos=None)</title>

              <para>Description of an column of an enumerated type.</para>
            </section>

          </section>
        </section>

        <section id="IsDescriptionClassDescr">
          <title>The <literal>IsDescription</literal> class</title>

          <para>Description of the structure of a table or nested
          column.</para>

          <para>This class is designed to be used as an easy, yet meaningful
          way to describe the structure of new <literal>Table</literal> (see
          <xref linkend="TableClassDescr" xrefstyle="select: label" />)
          datasets or nested columns through the definition of
          <emphasis>derived classes</emphasis>. In order to define such a
          class, you must declare it as descendant of
          <literal>IsDescription</literal>, with as many attributes as columns
          you want in your table. The name of each attribute will become the
          name of a column, and its value will hold a description of
          it.</para>

          <para>Ordinary columns can be described using instances of the
          <literal>Col</literal> class (see <xref linkend="ColClassDescr"
          xrefstyle="select: label" />). Nested columns can be described by
          using classes derived from <literal>IsDescription</literal>,
          instances of it, or name-description dictionaries. Derived classes
          can be declared in place (in which case the column takes the name of
          the class) or referenced by name.</para>

          <para>Nested columns can have a <literal>_v_pos</literal> special
          attribute which sets the <emphasis>relative</emphasis> position of
          the column among sibling columns <emphasis>also having explicit
          positions</emphasis>.  The <literal>pos</literal> constructor
          argument of <literal>Col</literal> instances is used for the same
          purpose.  Columns with no explicit position will be placed
          afterwards in alphanumeric order.</para>

          <para>Once you have created a description object, you can pass it to
          the <literal>Table</literal> constructor, where all the information
          it contains will be used to define the table structure.
          <!-- manual-only -->
          See the <xref linkend="secondExample" xrefstyle="select: label" />
          for an example on how that works.</para>

          <section>
            <title><literal>IsDescription</literal> special attributes</title>

            <para>These are the special attributes that the user can specify
            <emphasis>when declaring</emphasis> an
            <literal>IsDescription</literal> subclass to complement its
            <emphasis>metadata</emphasis>.</para>

            <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis role="bold">_v_pos</emphasis></glossterm>

                <glossdef>
                  <para>Sets the position of a possible nested column
                  description among its sibling columns.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title><literal>IsDescription</literal> class variables</title>

            <para>The following attributes are <emphasis>automatically
            created</emphasis> when an <literal>IsDescription</literal>
            subclass is declared.  Please note that declared columns can no
            longer be accessed as normal class variables after its
            creation.</para>

            <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis role="bold">columns</emphasis></glossterm>

                <glossdef>
                  <para>Maps the name of each column in the description to its
                  own descriptive object.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

        </section>
      </section>

      <section id="helperClasses">
        <title>Helper classes</title>

        <para>This section describes some classes that do not fit in any other
        section and that mainly serve for ancillary purposes.</para>

        <section id="FiltersClassDescr">
          <title>The <literal>Filters</literal> class</title>

          <para>Container for filter properties.</para>

          <para>This class is meant to serve as a container that keeps
          information about the filter properties associated with the chunked
          leaves, that is <literal>Table</literal>, <literal>CArray</literal>,
          <literal>EArray</literal> and <literal>VLArray</literal>.</para>

          <para>Instances of this class can be directly compared for
          equality.</para>

          <section>
            <title><literal>Filters</literal> instance variables</title>

            <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis
                role="bold">fletcher32</emphasis></glossterm>

                <glossdef>
                  <para>Whether the <emphasis>Fletcher32</emphasis> filter is
                  active or not.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">complevel</emphasis></glossterm>

                <glossdef>
                  <para>The compression level (0 disables compression).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">complib</emphasis></glossterm>

                <glossdef>
                  <para>The compression filter used (irrelevant when
                  compression is not enabled).</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">shuffle</emphasis></glossterm>

                <glossdef>
                  <para>Whether the <emphasis>Shuffle</emphasis> filter is
                  active or not.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title>Example of use</title>

            <para>This is a small example on using the
            <literal>Filters</literal> class:</para>

            <screen>import numpy
from tables import *

fileh = openFile('test5.h5', mode='w')
atom = Float32Atom()
filters = Filters(complevel=1, complib='blosc', fletcher32=True)
arr = fileh.createEArray(fileh.root, 'earray', atom, (0,2),
                         "A growable array", filters=filters)
# Append several rows in only one call
arr.append(numpy.array([[1., 2.],
                       [2., 3.],
                       [3., 4.]], dtype=numpy.float32))

# Print information on that enlargeable array
print "Result Array:"
print repr(arr)

fileh.close()</screen>

            <para>This enforces the use of the Blosc library, a compression
            level of 1 and a Fletcher32 checksum filter as well. See the
            output of this example:</para>

            <screen>Result Array:
/earray (EArray(3, 2), fletcher32, shuffle, blosc(1)) 'A growable array'
  type = float32
  shape = (3, 2)
  itemsize = 4
  nrows = 3
  extdim = 0
  flavor = 'numpy'
  byteorder = 'little'</screen>
          </section>

          <section id="FiltersInitDescr">
            <title><literal>Filters(complevel=0, complib='zlib', shuffle=True,
            fletcher32=False)</literal></title>

            <para>Create a new <literal>Filters</literal> instance.</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis
                role="bold">complevel</emphasis></glossterm>

                <glossdef>
                  <para>Specifies a compression level for data. The allowed
                  range is 0-9. A value of 0 (the default) disables
                  compression.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">complib</emphasis></glossterm>

                <glossdef>
                  <para>Specifies the compression library to be used. Right
                  now, <literal>'zlib'</literal> (the
                  default), <literal>'lzo'</literal>, <literal>'bzip2'</literal>
                  and <literal>'blosc'</literal> are supported.  Specifying a
                  compression library which is not available in the system
                  issues a <literal>FiltersWarning</literal> and sets the
                  library to the default one.</para>

                  <!-- manual-only -->
                  <para>See <xref linkend="compressionIssues"
                  xrefstyle="select: label" /> for some advice on which
                  library is better suited to your needs.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">shuffle</emphasis></glossterm>

                <glossdef>
                  <para>Whether or not to use the <emphasis>Shuffle</emphasis>
                  filter in the HDF5 library. This is normally used to improve
                  the compression ratio. A false value disables shuffling and
                  a true one enables it. The default value depends on whether
                  compression is enabled or not; if compression is enabled,
                  shuffling defaults to be enabled, else shuffling is
                  disabled. Shuffling can only be used when compression is
                  enabled.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">fletcher32</emphasis></glossterm>

                <glossdef>
                  <para>Whether or not to use the
                  <emphasis>Fletcher32</emphasis> filter in the HDF5 library.
                  This is used to add a checksum on each data chunk. A false
                  value (the default) disables the checksum.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title><literal>copy(override)</literal></title>

            <para>Get a copy of the filters, possibly overriding some
            arguments.</para>

            <para>Constructor arguments to be overridden must be passed as
            keyword arguments.</para>

            <para>Using this method is recommended over replacing the
            attributes of an instance, since instances of this class may
            become immutable in the future.</para>

            <screen>>>> filters1 = Filters()
>>> filters2 = filters1.copy()
>>> filters1 == filters2
True
>>> filters1 is filters2
False
>>> filters3 = filters1.copy(complevel=1)
Traceback (most recent call last):
  ...
ValueError: compression library ``None`` is not supported...
>>> filters3 = filters1.copy(complevel=1, complib='zlib')
>>> print filters1
Filters(complevel=0, shuffle=False, fletcher32=False)
>>> print filters3
Filters(complevel=1, complib='zlib', shuffle=False, fletcher32=False)
>>> filters1.copy(foobar=42)
Traceback (most recent call last):
  ...
TypeError: __init__() got an unexpected keyword argument 'foobar'</screen>
          </section>
        </section>

        <section id="IndexClassDescr">
          <title>The <literal>Index</literal> class</title>

          <para>Represents the index of a column in a table.</para>

          <para>This class is used to keep the indexing information for
          columns in a <literal>Table</literal> dataset (see <xref
          linkend="TableClassDescr" xrefstyle="select: label" />). It is
          actually a descendant of the <literal>Group</literal> class (see
          <xref linkend="GroupClassDescr" xrefstyle="select: label" />), with
          some added functionality. An <literal>Index</literal> is always
          associated with one and only one column in the table.</para>

          <note>
            <para>This class is mainly intended for internal use, but some of
            its documented attributes and methods may be interesting for the
            programmer.</para>
          </note>

          <section id="IndexClassInstanceVariables">
            <title><literal>Index</literal> instance variables</title>

            <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis role="bold">column</emphasis></glossterm>

                <glossdef>
                  <para>The <literal>Column</literal> (see <xref
                  linkend="ColumnClassDescr" xrefstyle="select: label" />)
                  instance for the indexed column.</para>

                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">dirty</emphasis></glossterm>

                <glossdef>
                  <para>Whether the index is dirty or not.</para>
                  <para>Dirty indexes are out of sync with column data, so
                  they exist but they are not usable.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">filters</emphasis></glossterm>

                <glossdef>
                  <para>Filter properties for this index —see
                  <literal>Filters</literal> in <xref
                  linkend="FiltersClassDescr" xrefstyle="select: label"
                  />.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">nelements</emphasis></glossterm>

                <glossdef>
                  <para>The number of currently indexed row for this
                  column.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">is_CSI</emphasis></glossterm>

                <glossdef>
                  <para>Whether the index is completely sorted or not.</para>
                </glossdef>
              </glossentry>

            </glosslist>
          </section>

          <section>
            <title><literal>Index</literal> methods</title>

            <section id="Index.readSorted" xreflabel="description">
              <title>readSorted(start=None, stop=None, step=None)</title>

              <para>Return the sorted values of index in the specified
              range.</para>

              <para>The meaning of the <literal>start</literal>,
              <literal>stop</literal> and <literal>step</literal> arguments is
              the same as in <literal>Table.readSorted()</literal> (see <xref
              linkend="Table.readSorted" />).</para>
            </section>

            <section id="Index.readIndices" xreflabel="description">
              <title>readIndices(start=None, stop=None, step=None)</title>

              <para>Return the indices values of index in the specified
              range.</para>

              <para>The meaning of the <literal>start</literal>,
              <literal>stop</literal> and <literal>step</literal> arguments is
              the same as in <literal>Table.readSorted()</literal> (see <xref
              linkend="Table.readSorted" />).</para>
            </section>
          </section>

          <section>
            <title><literal>Index</literal> special methods</title>

            <section id="Index.__getitem__" xreflabel="description">
              <title>__getitem__(key)</title>

              <para>Return the indices values of index in the specified
              range.</para>

              <para>If <literal>key</literal> argument is an integer, the
              corresponding index is returned.  If <literal>key</literal> is a
              slice, the range of indices determined by it is returned.  A
              negative value of <literal>step</literal> in slice is supported,
              meaning that the results will be returned in reverse
              order.</para>

              <para>This method is equivalent to
              <literal>Index.readIndices()</literal> (see <xref
              linkend="Index.readIndices" />).</para>
            </section>

          </section>
        </section>

        <section id="EnumClassDescr">
          <title>The <literal>Enum</literal> class</title>

          <para>Enumerated type.</para>

          <para>Each instance of this class represents an enumerated type. The
          values of the type must be declared
          <emphasis>exhaustively</emphasis> and named with
          <emphasis>strings</emphasis>, and they might be given explicit
          concrete values, though this is not compulsory. Once the type is
          defined, it can not be modified.</para>

          <para>There are three ways of defining an enumerated type. Each one
          of them corresponds to the type of the only argument in the
          constructor of <literal>Enum</literal>:</para>

          <itemizedlist>
            <listitem>
              <para><emphasis>Sequence of names</emphasis>: each enumerated
              value is named using a string, and its order is determined by
              its position in the sequence; the concrete value is assigned
              automatically:</para>

              <screen>>>> boolEnum = Enum(['True', 'False'])</screen>
            </listitem>

            <listitem>
              <para><emphasis>Mapping of names</emphasis>: each enumerated
              value is named by a string and given an explicit concrete value.
              All of the concrete values must be different, or a
              <literal>ValueError</literal> will be raised.</para>

              <screen>>>> priority = Enum({'red': 20, 'orange': 10, 'green': 0})
>>> colors = Enum({'red': 1, 'blue': 1})
Traceback (most recent call last):
  ...
ValueError: enumerated values contain duplicate concrete values: 1</screen>
            </listitem>

            <listitem>
              <para><emphasis>Enumerated type</emphasis>: in that case, a copy
              of the original enumerated type is created. Both enumerated
              types are considered equal.</para>

              <screen>>>> prio2 = Enum(priority)
>>> priority == prio2
True</screen>
            </listitem>
          </itemizedlist>

          <para>Please note that names starting with <literal>_</literal> are
          not allowed, since they are reserved for internal usage:</para>

          <screen>>>> prio2 = Enum(['_xx'])
Traceback (most recent call last):
  ...
ValueError: name of enumerated value can not start with ``_``: '_xx'</screen>

          <para>The concrete value of an enumerated value is obtained by
          getting its name as an attribute of the <literal>Enum</literal>
          instance (see <literal>__getattr__()</literal>) or as an item (see
          <literal>__getitem__()</literal>). This allows comparisons between
          enumerated values and assigning them to ordinary Python
          variables:</para>

          <screen>>>> redv = priority.red
>>> redv == priority['red']
True
>>> redv &gt; priority.green
True
>>> priority.red == priority.orange
False</screen>

          <para>The name of the enumerated value corresponding to a concrete
          value can also be obtained by using the
          <literal>__call__()</literal> method of the enumerated type. In this
          way you get the symbolic name to use it later with
          <literal>__getitem__()</literal>:</para>

          <screen>>>> priority(redv)
'red'
>>> priority.red == priority[priority(priority.red)]
True</screen>

          <para>(If you ask, the <literal>__getitem__()</literal> method is
          not used for this purpose to avoid ambiguity in the case of using
          strings as concrete values.)</para>

          <section>
            <title><literal>Enum</literal> special methods</title>

            <section id="Enum.__call__" xreflabel="description">
              <title>__call__(value, *default)</title>

              <para>Get the name of the enumerated value with that concrete
              <literal>value</literal>.</para>

              <para>If there is no value with that concrete value in the
              enumeration and a second argument is given as a
              <literal>default</literal>, this is returned. Else, a
              <literal>ValueError</literal> is raised.</para>

              <para>This method can be used for checking that a concrete value
              belongs to the set of concrete values in an enumerated
              type.</para>
            </section>

            <section>
              <title>__contains__(name)</title>

              <para>Is there an enumerated value with that
              <literal>name</literal> in the type?</para>

              <para>If the enumerated type has an enumerated value with that
              <literal>name</literal>, <literal>True</literal> is returned.
              Otherwise, <literal>False</literal> is returned. The
              <literal>name</literal> must be a string.</para>

              <para>This method does <emphasis>not</emphasis> check for
              concrete values matching a value in an enumerated type. For
              that, please use the <literal>Enum.__call__()</literal>
              method (see <xref linkend="Enum.__call__" />).</para>
            </section>

            <section>
              <title>__eq__(other)</title>

              <para>Is the <literal>other</literal> enumerated type equivalent
              to this one?</para>

              <para>Two enumerated types are equivalent if they have exactly
              the same enumerated values (i.e. with the same names and
              concrete values).</para>
            </section>

            <section>
              <title>__getattr__(name)</title>

              <para>Get the concrete value of the enumerated value with that
              <literal>name</literal>.</para>

              <para>The <literal>name</literal> of the enumerated value must
              be a string. If there is no value with that
              <literal>name</literal> in the enumeration, an
              <literal>AttributeError</literal> is raised.</para>
            </section>

            <section>
              <title>__getitem__(name)</title>

              <para>Get the concrete value of the enumerated value with that
              <literal>name</literal>.</para>

              <para>The <literal>name</literal> of the enumerated value must
              be a string. If there is no value with that
              <literal>name</literal> in the enumeration, a
              <literal>KeyError</literal> is raised.</para>
            </section>

            <section>
              <title>__iter__()</title>

              <para>Iterate over the enumerated values.</para>

              <para>Enumerated values are returned as <literal>(name,
              value)</literal> pairs <emphasis>in no particular
              order</emphasis>.</para>
            </section>

            <section>
              <title>__len__()</title>

              <para>Return the number of enumerated values in the enumerated
              type.</para>
            </section>

            <section>
              <title>__repr__()</title>

              <para>Return the canonical string representation of the
              enumeration. The output of this method can be evaluated to give
              a new enumeration object that will compare equal to this
              one.</para>
            </section>
          </section>
        </section>
      </section>

      <section id="ExprClassDescr">
        <title>The <literal>Expr</literal> class - a general-purpose
        expression evaluator</title>

        <para><literal>Expr</literal> is a class for evaluating expressions
        containing array-like objects.  With it, you can evaluate expressions
        (like <literal>"3*a+4*b"</literal>) that operate on arbitrary large
        arrays while optimizing the resources required to perform them
        (basically main memory and CPU cache memory).  It is similar to the
        Numexpr package (see <biblioref linkend="Numexpr" />), but in addition
        to NumPy objects, it also accepts disk-based homogeneous arrays, like
        the <literal>Array</literal>, <literal>CArray</literal>,
        <literal>EArray</literal> and <literal>Column</literal> PyTables
        objects.</para>

        <para>All the internal computations are performed via the Numexpr
        package, so all the broadcast and upcasting rules of Numexpr applies
        here too.  These rules are very similar to the NumPy ones, but with
        some exceptions due to the particularities of having to deal with
        potentially very large disk-based arrays.  Be sure to read the
        documentation of the
        <literal>Expr</literal> constructor and methods as well as that of
        Numexpr, if you want to fully grasp these particularities.
        </para>

        <section id="ExprClassInstanceVariables">
          <title><literal>Expr</literal> instance variables</title>

          <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis role="bold">append_mode</emphasis></glossterm>

                <glossdef>
                  <para>The appending mode for user-provided output
                  containers.</para>

                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">maindim</emphasis></glossterm>

                <glossdef>
                  <para>Common main dimension for inputs in expression.</para>

                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">names</emphasis></glossterm>

                <glossdef>
                  <para>The names of variables in expression (list).</para>

                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">out</emphasis></glossterm>

                <glossdef>
                  <para>The user-provided container (if any) for the
                  expression outcome.</para>

                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">o_start, o_stop,
                o_step</emphasis></glossterm>

                <glossdef>
                  <para>The range selection for the user-provided
                  output.</para>

                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">shape</emphasis></glossterm>

                <glossdef>
                  <para>Common shape for the arrays in expression.</para>

                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">start, stop,
                step</emphasis></glossterm>

                <glossdef>
                  <para>The range selection for all the inputs.</para>

                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">values</emphasis></glossterm>

                <glossdef>
                  <para>The values of variables in expression (list).</para>

                </glossdef>
              </glossentry>
          </glosslist>
        </section>

        <section id="ExprClassInstanceIOVariables">
          <title><literal>Expr</literal> special tuning variables for
          input/output</title>

          <warning><para>The next parameters are meant only for advanced
              users.  Please do not touch them if you don't know what you are
              doing.</para>
          </warning>

          <glosslist>
              <?dbfo glosslist-presentation="list" ?>

              <glossentry>
                <glossterm><emphasis role="bold">BUFFERTIMES</emphasis></glossterm>

                <glossdef>
                  <para>The maximum buffersize/rowsize ratio before issuing a
                    <emphasis>PerformanceWarning</emphasis>.  The default is
                    1000.</para>

                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">CHUNKTIMES</emphasis></glossterm>

                <glossdef>
                  <para>The number of chunks in the input buffer per each
                  variable in expression.  The default is 16.</para>

                </glossdef>
              </glossentry>

          </glosslist>

        </section>

        <section id="ExprClassMethods">
          <title><literal>Expr</literal> methods</title>

          <section id="Expr.__init__" xreflabel="description">
            <title>__init__(expr, uservars=None, **kwargs)</title>

            <para>Compile the expression and initialize internal
              structures.</para>

            <para><literal>expr</literal> must be specified as a string like
              <literal>"2*a+3*b"</literal>.</para>

            <para>The <literal>uservars</literal> mapping may be used to
              define the variable names appearing in <literal>expr</literal>.
              This mapping should consist of identifier-like strings pointing
              to any <literal>Array</literal>,
              <literal>CArray</literal>, <literal>EArray</literal>,
              <literal>Column</literal> or NumPy ndarray instances (or even
              others which will be tried to be converted to ndarrays).</para>

            <para>When <literal>uservars</literal> is not provided
              or <literal>None</literal>, the current local and global
              namespace is sought instead of <literal>uservars</literal>.  It
              is also possible to pass just some of the variables in
              expression via the <literal>uservars</literal> mapping, and the
              rest will be retrieved from the current local and global
              namespaces.</para>

            <para><literal>**kwargs</literal> is meant to pass additional
              parameters to the Numexpr kernel.  This is basically the same as
              the <literal>**kwargs</literal> argument
              in <literal>Numexpr.evaluate()</literal>, and is mainly meant
              for advanced use.</para>

            <para>After initialized, an <literal>Expr</literal> instance can
              be evaluated via its <literal>eval()</literal> method.  This
              class also provides an <literal>__iter__()</literal> method that
              iterates over all the resulting rows in expression.</para>

            <para>Example of use:</para>

            <screen>>>> a = f.createArray('/', 'a', np.array([1,2,3]))
>>> b = f.createArray('/', 'b', np.array([3,4,5]))
>>> c = np.array([4,5,6])
>>> expr = tb.Expr("2*a+b*c")   # initialize the expression
>>> expr.eval()                 # evaluate it
array([14, 24, 36])
>>> sum(expr)                   # use as an iterator
74
            </screen>

            <para>where you can see that you can mix different containers in
              the expression (whenever shapes are consistent).</para>

            <para>You can also work with multidimensional arrays:</para>

            <screen>>>> a2 = f.createArray('/', 'a2', np.array([[1,2],[3,4]]))
>>> b2 = f.createArray('/', 'b2', np.array([[3,4],[5,6]]))
>>> c2 = np.array([4,5])           # This will be broadcasted
>>> expr = tb.Expr("2*a2+b2-c2")
>>> expr.eval()
array([[1, 3],
       [7, 9]])
>>> sum(expr)
array([ 8, 12])
            </screen>
          </section>

          <section id="Expr.eval" xreflabel="description">
            <title>eval()</title>

            <para>Evaluate the expression and return the outcome.</para>

            <para>Because of performance reasons, the computation order tries
              to go along the common main dimension of all inputs.  If not
              such a common main dimension is found, the iteration will go
              along the leading dimension instead.</para>

            <para>For non-consistent shapes in inputs (i.e. shapes having a
              different number of dimensions), the regular NumPy broadcast
              rules applies.  There is one exception to this rule though: when
              the dimensions orthogonal to the main dimension of the
              expression are consistent, but the main dimension itself differs
              among the inputs, then the shortest one is chosen for doing the
              computations.  This is so because trying to expand very large
              on-disk arrays could be too expensive or simply not
              possible.</para>

            <para>Also, the regular Numexpr casting rules (which are similar
              to those of NumPy, although you should check the Numexpr manual
              for the exceptions) are applied to determine the output
              type.</para>

            <para>Finally, if the <literal>setOuput()</literal> method
              specifying a user container has already been called, the output
              is sent to this user-provided container.  If not, a fresh NumPy
              container is returned instead.</para>

            <warning><para>When dealing with large on-disk inputs, failing to
                specify an on-disk container may consume all your available
                memory.</para>
            </warning>

            <para>For some examples of use see the
              <xref linkend="Expr.__init__" xrefstyle="select: label"/>
              docs.</para>

          </section>

          <section id="Expr.setInputsRange" xreflabel="description">
            <title>setInputsRange(start=None, stop=None, step=None)</title>

            <para>Define a range for all inputs in expression.</para>

            <para>The computation will only take place for the range defined
              by the <literal>start</literal>, <literal>stop</literal>
              and <literal>step</literal> parameters in the main dimension of
              inputs (or the leading one, if the object lacks the concept of
              main dimension, like a NumPy container).  If not a common main
              dimension exists for all inputs, the leading dimension will be
              used instead.</para>

          </section>

          <section id="Expr.setOutput" xreflabel="description">
            <title>setOutput(out, append_mode=False)</title>

            <para>Set <literal>out</literal> as container for output as well
              as the <literal>append_mode</literal>.</para>

            <para>The <literal>out</literal> must be a container that is meant
              to keep the outcome of the expression.  It should be an
              homogeneous type container and can typically be
              an <literal>Array</literal>, <literal>CArray</literal>,
              <literal>EArray</literal>, <literal>Column</literal> or a NumPy
              ndarray.</para>

            <para>The <literal>append_mode</literal> specifies the way of
              which the output is filled.  If true, the rows of the outcome
              are <emphasis>appended</emphasis> to the <literal>out</literal>
              container.  Of course, for doing this it is necessary
              that <literal>out</literal> would have
              an <literal>append()</literal> method (like
              an <literal>EArray</literal>, for example).</para>

            <para>If <literal>append_mode</literal> is false, the output is
              set via the <literal>__setitem__()</literal> method (see
              the <literal>Expr.setOutputRange()</literal> for info on how to
              select the rows to be updated).  If <literal>out</literal> is
              smaller than what is required by the expression, only the
              computations that are needed to fill up the container are
              carried out.  If it is larger, the excess elements are
              unaffected.</para>

          </section>

          <section id="Expr.setOutputRange" xreflabel="description">
            <title>setOutputRange(start=None, stop=None, step=None)</title>

            <para>Define a range for user-provided output object.</para>

            <para>The output object will only be modified in the range
              specified by the <literal>start</literal>,
              <literal>stop</literal> and <literal>step</literal> parameters
              in the main dimension of output (or the leading one, if the
              object does not have the concept of main dimension, like a NumPy
              container).</para>

          </section>

        </section>

        <section id="ExprClassSpecialMethods">
          <title><literal>Expr</literal> special methods</title>

          <section id="Expr.__iter__" xreflabel="description">
            <title>__iter__()</title>

            <para>Iterate over the rows of the outcome of the
              expression.</para>

            <para>This iterator always returns rows as NumPy objects, so a
              possible <literal>out</literal> container specified
              in <literal>Expr.setOutput()</literal> method is ignored
              here.</para>

            <para>The <literal>Expr.eval()</literal> documentation (see
              <xref linkend="Expr.eval" xrefstyle="select: label"/>) for
              details on how the computation is carried out.  Also, for some
              examples of use see the <xref linkend="Expr.__init__"
                                            xrefstyle="select: label"/> docs.</para>

          </section>
        </section>

      </section>

    </chapter>

    <chapter id="optimizationTips">
      <title>Optimization tips</title>

      <epigraph>
        <attribution>Johann Karl Friedrich Gauss <citetitle>[asked how he came
        upon his theorems]</citetitle></attribution>

        <literallayout>
      ... durch planmässiges Tattonieren.
      [... through systematic, palpable experimentation.]
      </literallayout>
      </epigraph>

      <para></para>

      <para>On this chapter, you will get deeper knowledge of PyTables
      internals. PyTables has many tunable features so that you can improve
      the performance of your application.  If you are planning to deal with
      really large data, you should read carefully this section in order to
      learn how to get an important efficiency boost for your code.  But if
      your datasets are small (say, up to 10 MB) or your number of nodes is
      contained (up to 1000), you should not worry about that as the default
      parameters in PyTables are already tuned for those sizes (although you
      may want to adjust them further anyway).  At any rate, reading this
      chapter will help you in your life with PyTables.</para>

      <section id="chunksizeOptim">
        <title>Understanding chunking</title>

        <para>The underlying HDF5 library that is used by PyTables allows for
        certain datasets (the so-called <emphasis>chunked</emphasis> datasets)
        to take the data in bunches of a certain length, named
        <emphasis>chunks</emphasis>, and write them on disk as a whole, i.e.
        the HDF5 library treats chunks as atomic objects and disk I/O is
        always made in terms of complete chunks.  This allows data filters to
        be defined by the application to perform tasks such as compression,
        encryption, check-summing, etc. on entire chunks.</para>

        <para>HDF5 keeps a B-tree in memory that is used to map chunk
        structures on disk.  The more chunks that are allocated for a dataset
        the larger the B-tree.  Large B-trees take memory and cause file
        storage overhead as well as more disk I/O and higher contention for
        the metadata cache.  Consequently, it's important to balance between
        memory and I/O overhead (small B-trees) and time to access data (big
        B-trees).</para>

        <para>In the next couple of sections, you will discover how to inform
        PyTables about the expected size of your datasets for allowing a
        sensible computation of the chunk sizes.  Also, you will be presented
        some experiments so that you can get a feeling on the consequences of
        manually specifying the chunk size.  Although doing this latter is
        only reserved to experienced people, these benchmarks may allow you to
        understand more deeply the chunk size implications and let you quickly
        start with the fine-tuning of this important parameter.</para>

        <section id="expectedRowsOptim">
          <title>Informing PyTables about expected number of rows in
          tables or arrays</title>

          <para>PyTables can determine a sensible chunk size to your dataset
          size if you helps it by providing an estimation of the final number
          of rows for an extensible
          leaf<footnote><para><literal>CArray</literal> nodes, though not
          extensible, are chunked and have their optimum chunk size
          automatically computed at creation time, since their final shape is
          known.</para></footnote>.  You should provide this information at
          leaf creation time by passing this value to the
          <literal>expectedrows</literal> argument of the
          <literal>createTable()</literal> method (see <xref
          linkend="createTableDescr" />) or
          <literal>createEArray()</literal> method (see <xref
          linkend="createEArrayDescr" />).  For
          <literal>VLArray</literal> leaves, you must pass the expected size
          in MBytes by using the argument <literal>expectedsizeinMB</literal>
          of <literal>createVLArray()</literal> (see <xref
          linkend="createVLArrayDescr" />)
          instead.</para>

          <para>When your leaf size is bigger than 10 MB (take this figure
          only as a reference, not strictly), by providing this guess you will
          be optimizing the access to your data.  When the table or array size
          is larger than, say 100MB, you are <emphasis>strongly</emphasis>
          suggested to provide such a guess; failing to do that may cause your
          application to do very slow I/O operations and to demand
          <emphasis>huge</emphasis> amounts of memory. You have been
          warned!</para>
        </section>

        <section id="chunksizeFineTune">
          <title>Fine-tuning the chunksize</title>

          <warning><para>This section is mostly meant for experts.  If you are
          a beginner, you must know that setting manually the chunksize is a
          potentially dangerous action.</para>
          </warning>

          <para>Most of the time, informing PyTables about the extent of your
          dataset is enough.  However, for more sophisticated applications,
          when one has special requirements for doing the I/O or when dealing
          with really large datasets, you should really understand the
          implications of the chunk size in order to be able to find the best
          value for your own application.
          </para>

          <para>You can specify the chunksize for every chunked dataset in
          PyTables by passing the <literal>chunkshape</literal> argument to
          the corresponding constructors. It is important to point out that
          <literal>chunkshape</literal> is not exactly the same thing than a
          chunksize; in fact, the chunksize of a dataset can be computed
          multiplying all the dimensions of the chunkshape among them and
          multiplying the outcome by the size of the atom.</para>

          <para>We are going to describe a series of experiments where an
          EArray of 15 GB is written with different chunksizes, and then it is
          accessed in both sequential (i.e. first element 0, then element 1
          and so on and so forth until the data is exhausted) and random mode
          (i.e. single elements are read randomly all through the dataset).
          These benchmarks have been carried out with PyTables 2.1 on a
          machine with an Intel Core2 processor @ 3 GHz and a RAID-0 made of
          two SATA disks spinning at 7200 RPM, and using GNU/Linux with an XFS
          filesystem.  The script used for the benchmarks is available in
          <literal>bench/optimal-chunksize.py</literal>.</para>

          <para>In figures <xref linkend="createTime-chunksize"
          xrefstyle="select: labelnumber" />,
          <xref linkend="fileSizes-chunksize" xrefstyle="select: labelnumber"
          />, <xref linkend="seqTime-chunksize" xrefstyle="select:
          labelnumber" /> and <xref linkend="randomTime-chunksize"
          xrefstyle="select: labelnumber" />, you can see how the chunksize
          affects different aspects, like creation time, file sizes,
          sequential read time and random read time.  So, if you properly
          inform PyTables about the extent of your datasets, you will get an
          automatic chunksize value (256 KB in this case) that is pretty
          optimal for most of uses.  However, if what you want is, for
          example, optimize the creation time when using the Zlib compressor,
          you may want to reduce the chunksize to 32 KB (see
          <xref linkend="createTime-chunksize" xrefstyle="select: label" />).
          Or, if your goal is to optimize the sequential access time for an
          dataset compressed with Blosc, you may want to increase the
          chunksize to 512 KB (see <xref linkend="seqTime-chunksize"
          xrefstyle="select: label"/>).
          </para>

          <para>You will notice that, by manually specifying the chunksize of
          a leave you will not normally get a drastic increase in performance,
          but at least, you have the opportunity to fine-tune such an
          important parameter for improve performance.</para>

          <figure id="createTime-chunksize">
            <title>Creation time per element for a 15 GB EArray and different
            chunksizes.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center"
                           fileref="create-chunksize-15GB.svg"
                           format="SVG"
                           scale="60" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="create-chunksize-15GB.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>


          <figure id="fileSizes-chunksize">
            <title>File sizes for a 15 GB EArray and different
            chunksizes.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center"
                           fileref="filesizes-chunksize-15GB.svg"
                           format="SVG"
                           scale="60" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="filesizes-chunksize-15GB.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>


          <figure id="seqTime-chunksize">
            <title>Sequential access time per element for a 15 GB EArray and
            different chunksizes.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center"
                           fileref="seq-chunksize-15GB.svg"
                           format="SVG"
                           scale="60" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="seq-chunksize-15GB.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>


          <figure id="randomTime-chunksize">
            <title>Random access time per element for a 15 GB EArray and
            different chunksizes.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center"
                           fileref="random-chunksize-15GB.svg"
                           format="SVG"
                           scale="60" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="random-chunksize-15GB.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>

          <para>Finally, it is worth noting that adjusting the chunksize can
          be specially important if you want to access your dataset by blocks
          of certain dimensions.  In this case, it is normally a good idea to
          set your <literal>chunkshape</literal> to be the same than these
          dimensions; you only have to be careful to not end with a too small
          or too large chunksize.  As always, experimenting prior to pass your
          application into production is your best ally.
          </para>

        </section>
      </section>

      <section id="searchOptim">
        <title>Accelerating your searches</title>

        <note>
          <para>Many of the explanations and plots in this section and the
            forthcoming ones still need to be updated to include Blosc (see
            <biblioref linkend="BloscRef" />), the new and powerful compressor
            added in PyTables 2.2 series.  You should expect it to be the
            fastest compressor among all the described here, and its use is
            strongly recommended whenever you need extreme speed and not a
            very high compression ratio.
          </para>
        </note>

        <para>Searching in tables is one of the most common and time consuming
        operations that a typical user faces in the process of mining through
        his data.  Being able to perform queries as fast as possible will
        allow more opportunities for finding the desired information quicker
        and also allows to deal with larger datasets.</para>

        <para>PyTables offers many sort of techniques so as to speed-up the
        search process as much as possible and, in order to give you hints to
        use them based, a series of benchmarks have been designed and carried
        out.  All the results presented in this section have been obtained
        with synthetic, random data and using PyTables 2.1.  Also, the tests
        have been conducted on a machine with an Intel Core2 (64-bit) @ 3 GHz
        processor with RAID-0 disk storage (made of four spinning disks @ 7200
        RPM), using GNU/Linux with an XFS filesystem.  The script used for the
        benchmarks is available in <literal>bench/indexed_search.py</literal>.
        As your data, queries and platform may be totally different for your
        case, take this just as a guide because your mileage may vary (and
        will vary).</para>

        <para>In order to be able to play with tables with a number of rows as
        large as possible, the record size has been chosen to be rather small
        (24 bytes). Here it is its definition:</para>

        <screen>class Record(tables.IsDescription):
            col1 = tables.Int32Col()
            col2 = tables.Int32Col()
            col3 = tables.Float64Col()
            col4 = tables.Float64Col()</screen>

        <para>In the next sections, we will be optimizing the times for a
        relatively complex query like this:

        <screen>result = [row['col2'] for row in table
   if (((row['col4'] &gt;= lim1 and row['col4'] &lt; lim2) or
       ((row['col2'] &gt; lim3 and row['col2'] &lt; lim4])) and
       ((row['col1']+3.1*row['col2']+row['col3']*row['col4']) &gt; lim5))]</screen>

        (for future reference, we will call this sort of queries
        <emphasis>regular</emphasis> queries).  So, if you want to see how to
        greatly improve the time taken to run queries like this, keep
        reading.</para>

        <section id="inkernelSearch">
          <title>In-kernel searches</title>

          <para>PyTables provides a way to accelerate data selections inside
          of a single table, through the use of the
          <literal>Table.where()</literal> iterator and related query methods
          (see <xref linkend="TableMethods_querying" xrefstyle="select: label"
          />).  This mode of selecting data is called
          <emphasis>in-kernel</emphasis>.  Let's see an example of an
          <emphasis>in-kernel</emphasis> query based on the
          <emphasis>regular</emphasis> one mentioned above:</para>

          <screen>result = [row['col2'] for row in table.where(
    '(((col4 &gt;= lim1) &amp; (col4 &lt; lim2)) |
      ((col2 &gt; lim3) &amp; (col2 &lt; lim4)) &amp;
      ((col1+3.1*col2+col3*col4) > lim5))')]</screen>

          <para>This simple change of mode selection can improve search times
          quite a lot and actually make PyTables very competitive when
          compared against typical relational databases as you can see in
          <xref linkend="sequentialTimes-10m" xrefstyle="select: label" /> and
          <xref linkend="sequentialTimes-1g" xrefstyle="select: label"
          />.</para>

          <figure id="sequentialTimes-10m">
            <title>Times for non-indexed complex queries in a small table with
            10 millions of rows: the data fits in memory.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center"
                           fileref="Q7-10m-noidx.svg"
                           format="SVG"
                           scale="60" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="Q7-10m-noidx.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>

          <para>By looking at <xref linkend="sequentialTimes-10m"
          xrefstyle="select: label" /> you can see how in the case that table
          data fits easily in memory, in-kernel searches on uncompressed
          tables are generally much faster (10x) than standard queries as well
          as PostgreSQL (5x).  Regarding compression, we can see how Zlib
          compressor actually slows down the performance of in-kernel queries
          by a factor 3.5x; however, it remains faster than PostgreSQL (40%).
          On his hand, LZO compressor only decreases the performance by a 75%
          with respect to uncompressed in-kernel queries and is still a lot
          faster than PostgreSQL (3x).  Finally, one can observe that, for low
          selectivity queries (large number of hits), PostgreSQL performance
          degrades quite steadily, while in PyTables this slow down rate is
          significantly smaller.  The reason of this behaviour is not entirely
          clear to the authors, but the fact is clearly reproducible in our
          benchmarks.
          </para>

          <para>But, why in-kernel queries are so fast when compared with
          regular ones?.  The answer is that in regular selection mode the
          data for all the rows in table has to be brought into Python space
          so as to evaluate the condition and decide if the corresponding
          field should be added to the <literal>result</literal> list.  On the
          contrary, in the in-kernel mode, the condition is passed to the
          PyTables kernel (hence the name), written in C, and evaluated there
          at full C speed (with the help of the integrated Numexpr package,
          see <biblioref linkend="Numexpr" />), so that the only values that
          are brought to Python space are the rows that fulfilled the
          condition.  Hence, for selections that only have a relatively small
          number of hits (compared with the total amount of rows), the savings
          are very large.  It is also interesting to note the fact that,
          although for queries with a large number of hits the speed-up is not
          as high, it is still very important.</para>

          <para>On the other hand, when the table is too large to fit in
          memory (see <xref linkend="sequentialTimes-1g" xrefstyle="select:
          label" />), the difference in speed between regular and in-kernel is
          not so important, but still significant (2x).  Also, and curiously
          enough, large tables compressed with Zlib offers slightly better
          performance (around 20%) than uncompressed ones; this is because the
          additional CPU spent by the uncompressor is compensated by the
          savings in terms of net I/O (one has to read less actual data from
          disk).  However, when using the extremely fast LZO compressor, it
          gives a clear advantage over Zlib, and is up to 2.5x faster than not
          using compression at all.  The reason is that LZO decompression
          speed is much faster than Zlib, and that allows PyTables to read the
          data at full disk speed (i.e. the bottleneck is in the I/O
          subsystem, not in the CPU).  In this case the compression rate is
          around 2.5x, and this is why the data can be read 2.5x faster.  So,
          in general, using the LZO compressor is the best way to ensure best
          reading/querying performance for out-of-core datasets (more about
          how compression affects performance in <xref
          linkend="compressionIssues"/>).
          </para>

          <figure id="sequentialTimes-1g">
            <title>Times for non-indexed complex queries in a large table with
            1 billion of rows: the data does not fit in memory.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center"
                           fileref="Q8-1g-noidx.svg"
                           format="SVG"
                           scale="60" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="Q8-1g-noidx.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>

          <para>Furthermore, you can mix the <emphasis>in-kernel</emphasis>
          and <emphasis>regular</emphasis> selection modes for evaluating
          arbitrarily complex conditions making use of external functions.
          Look at this example:</para>

          <screen>result = [ row['var2']
           for row in table.where('(var3 == "foo") &amp; (var1 &lt;= 20)')
           if your_function(row['var2']) ]</screen>

          <para>Here, we use an <emphasis>in-kernel</emphasis> selection to
          choose rows according to the values of the <literal>var3</literal>
          and <literal>var1</literal> fields.  Then, we apply a
          <emphasis>regular</emphasis> selection to complete the query. Of
          course, when you mix the <emphasis>in-kernel</emphasis> and
          <emphasis>regular</emphasis> selection modes you should pass the
          most restrictive condition to the <emphasis>in-kernel</emphasis>
          part, i.e. to the <literal>where()</literal> iterator.  In situations
          where it is not clear which is the most restrictive condition, you
          might want to experiment a bit in order to find the best
          combination.</para>

          <para>However, since in-kernel condition strings allow rich
          expressions allowing the coexistence of multiple columns, variables,
          arithmetic operations and many typical functions, it is unlikely
          that you will be forced to use external regular selections in
          conditions of small to medium complexity. See <xref
          linkend="conditionSyntax" xrefstyle="select: label" /> for more
          information on in-kernel condition syntax.</para>
        </section>

        <section id="indexedSearches">
          <title>Indexed searches</title>

          <para>When you need more speed than <emphasis>in-kernel</emphasis>
          selections can offer you, PyTables offers a third selection method,
          the so-called <emphasis>indexed</emphasis> mode (based on the highly
          efficient OPSI indexing engine <biblioref linkend="OPSI" />).  In
          this mode, you have to decide which column(s) you are going to apply
          your selections over, and index them.  Indexing is just a kind of
          sorting operation over a column, so that searches along such a
          column (or columns) will look at this sorted information by using a
          <emphasis>binary search</emphasis> which is much faster than the
          <emphasis>sequential search</emphasis> described in the previous
          section.</para>

          <para>You can index the columns you want by calling the
          <literal>Column.createIndex()</literal> method (see <xref
          linkend="Column.createIndex" />) on an already created table.  For
          example:

 <screen>indexrows = table.cols.var1.createIndex()
indexrows = table.cols.var2.createIndex()
indexrows = table.cols.var3.createIndex()</screen>

          will create indexes for all <literal>var1</literal>,
          <literal>var2</literal> and <literal>var3</literal> columns.</para>

          <para>After you have indexed a series of columns, the PyTables query
          optimizer will try hard to discover the usable indexes in a
          potentially complex expression. However, there are still places
          where it cannot determine that an index can be used. See below for
          examples where the optimizer can safely determine if an index, or
          series of indexes, can be used or not.
          </para>

          <para>Example conditions where an index can be used: <itemizedlist>
              <listitem>
                <para><literal>var1 &gt;= "foo"</literal> (var1 is
                used)</para>
              </listitem>

              <listitem>
                <para><literal>var1 &gt;= mystr</literal> (var1 is
                used)</para>
              </listitem>

              <listitem>
                <para><literal>(var1 &gt;= "foo") &amp; (var4 &gt;
                0.0)</literal> (var1 is used)</para>
              </listitem>

              <listitem>
                <para><literal>("bar" &lt;= var1) &amp; (var1 &lt;
                "foo")</literal> (var1 is used)</para>
              </listitem>

              <listitem>
                <para><literal>(("bar" &lt;= var1) &amp; (var1 &lt; "foo"))
                &amp; (var4 &gt; 0.0)</literal> (var1 is used)</para>
              </listitem>

              <listitem>
                <para><literal>(var1 &gt;= "foo") &amp; (var3 &gt;
                10)</literal> (var1 and var3 are used)</para>
              </listitem>

              <listitem>
                <para><literal>(var1 &gt;= "foo") | (var3 &gt; 10)</literal>
                (var1 and var3 are used)</para>
              </listitem>

              <listitem>
                <para><literal>~(var1 &gt;= "foo") | ~(var3 &gt; 10)</literal>
                (var1 and var3 are used)</para>
              </listitem>

            </itemizedlist></para>

          <para>Example conditions where an index can <emphasis>not</emphasis>
          be used: <itemizedlist>
              <listitem>
                <para><literal>var4 &gt; 0.0</literal> (var4 is not
                indexed)</para>
              </listitem>

              <listitem>
                <para><literal>var1 != 0.0</literal> (range has two
                pieces)</para>
              </listitem>

              <listitem>
                <para><literal>~(("bar" &lt;= var1) &amp; (var1 &lt; "foo"))
                &amp; (var4 &gt; 0.0)</literal> (negation of a complex boolean
                expression)</para>
              </listitem>

            </itemizedlist></para>

          <note>
            <para>From PyTables 2.3 on, several indexes can be used in a
            single query.</para>
          </note>

          <note>
            <para>If you want to know for sure whether a particular query will
            use indexing or not (without actually running it), you are advised
            to use the <literal>Table.willQueryUseIndexing()</literal> method
            (see <xref linkend="Table.willQueryUseIndexing" xrefstyle="select:
            label" />).</para>
          </note>

          <para>One important aspect of the new indexing in PyTables (>= 2.3)
          is that it has been designed from the ground up with the goal of
          being capable to effectively manage very large tables.  To this goal,
          it sports a wide spectrum of different quality levels (also called
          optimization levels) for its indexes so that the user can choose the
          best one that suits her needs (more or less size, more or less
          performance).
          </para>

          <para>In <xref linkend="createIndexTimes" xrefstyle="select: label"
          />, you can see that the times to index columns in tables can be
          really short.  In particular, the time to index a column with 1
          billion rows (1 Gigarow) with the lowest optimization level is less
          than 4 minutes while indexing the same column with full optimization
          (so as to get a completely sorted index or CSI) requires around 1
          hour.  These are rather competitive figures compared with a
          relational database (in this case, PostgreSQL 8.3.1, which takes
          around 1.5 hours for getting the index done).  This is because
          PyTables is geared towards read-only or append-only tables and takes
          advantage of this fact to optimize the indexes properly.  On the
          contrary, most relational databases have to deliver decent
          performance in other scenarios as well (specially updates and
          deletions), and this fact leads not only to slower index creation
          times, but also to indexes taking much more space on disk, as you
          can see in <xref linkend="indexSizes" xrefstyle="select: label"
          />.</para>

          <figure id="createIndexTimes">
            <title>Times for indexing an <literal>Int32</literal> and
            <literal>Float64</literal> column.</title>

            <mediaobject>

              <imageobject role="fo">
                <imagedata align="center" fileref="create-index-time-int32-float64.svg"
                           format="SVG" scale="50" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" fileref="create-index-time-int32-float64.png"
                           format="PNG" />
              </imageobject>

            </mediaobject>
          </figure>

          <figure id="indexSizes">
            <title>Sizes for an index of a <literal>Float64</literal> column
            with 1 billion of rows.</title>

            <mediaobject>

              <imageobject role="fo">
                <imagedata align="center" fileref="indexes-sizes2.svg"
                           format="SVG" scale="50" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" fileref="indexes-sizes2.png"
                           format="PNG" />
              </imageobject>

            </mediaobject>
          </figure>

          <para>The user can select the index quality by passing the desired
          <literal>optlevel</literal> and <literal>kind</literal> arguments to
          the <literal>createIndex()</literal> method (see <xref
          linkend="Column.createIndex" />).  We can see in figures <xref
          linkend="createIndexTimes" xrefstyle="select: labelnumber" /> and
          <xref linkend="indexSizes" xrefstyle="select: labelnumber" /> how
          the different optimization levels affects index time creation and
          index sizes.

          So, which is the effect of the different optimization levels in
          terms of query times?  You can see that in <xref
          linkend="queryTimes-indexed-optlevels" xrefstyle="select: label"
          />.</para>

          <figure id="queryTimes-indexed-optlevels">
            <title>Times for complex queries with a cold cache (mean of 5
            first random queries) for different optimization levels. Benchmark
            made on a machine with Intel Core2 (64-bit) @ 3 GHz processor with
            RAID-0 disk storage.</title>

            <mediaobject>

              <imageobject role="fo">
                <imagedata align="center"
                           fileref="Q8-1g-idx-optlevels.svg"
                           format="SVG" scale="60" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="Q8-1g-idx-optlevels.png"
                           format="PNG" />
              </imageobject>

            </mediaobject>
          </figure>

          <para>Of course, compression also has an effect when doing indexed
          queries, although not very noticeable, as can be seen in <xref
          linkend="queryTimes-indexed-compress" xrefstyle="select: label"/>.
          As you can see, the difference between using no compression and
          using Zlib or LZO is very little, although LZO achieves relatively
          better performance generally speaking.</para>

          <figure id="queryTimes-indexed-compress">
            <title>Times for complex queries with a cold cache (mean of 5
            first random queries) for different compressors.</title>

            <mediaobject>

              <imageobject role="fo">
                <imagedata align="center"
                           fileref="Q8-1g-idx-compress.svg"
                           format="SVG" scale="60" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="Q8-1g-idx-compress.png"
                           format="PNG" />
              </imageobject>

            </mediaobject>
          </figure>

          <para>You can find a more complete description and benchmarks about
          OPSI, the indexing system of PyTables (>= 2.3) in <biblioref
          linkend="OPSI" />.</para>

        </section>

        <section id="SSD-indexes">
          <title>Indexing and Solid State Disks (SSD)</title>

          <para>Lately, the long promised Solid State Disks (SSD for brevity)
          with decent capacities and affordable prices have finally hit the
          market and will probably stay in coexistence with the traditional
          spinning disks for the foreseeable future (separately or forming
          <emphasis>hybrid</emphasis> systems).  SSD have many advantages over
          spinning disks, like much less power consumption and better
          throughput.  But of paramount importance, specially in the context
          of accelerating indexed queries, is its very reduced latency during
          disk seeks, which is typically 100x better than traditional disks.
          Such a huge improvement has to have a clear impact in reducing the
          query times, specially when the selectivity is high (i.e. the number
          of hits is small).
          </para>

          <para>In order to offer an estimate on the performance improvement
          we can expect when using a low-latency SSD instead of traditional
          spinning disks, the benchmark in the previous section has been
          repeated, but this time using a single SSD disk instead of the four
          spinning disks in RAID-0.  The result can be seen in <xref
          linkend="queryTimes-indexed-SSD" xrefstyle="select: label"/>.  There
          one can see how a query in a table of 1 billion of rows with 100
          hits took just 1 tenth of second when using a SSD, instead of 1
          second that needed the RAID made of spinning disks.  This factor of
          10x of speed-up for high-selectivity queries is nothing to sneeze
          at, and should be kept in mind when really high performance in
          queries is needed.  It is also interesting that using compression
          with LZO does have a clear advantage over when no compression is
          done.
          </para>

          <figure id="queryTimes-indexed-SSD">
            <title>Times for complex queries with a cold cache (mean of 5
            first random queries) for different disk storage (SSD vs spinning
            disks).</title>

            <mediaobject>

              <imageobject role="fo">
                <imagedata align="center"
                           fileref="Q8-1g-idx-SSD.svg"
                           format="SVG" scale="60" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="Q8-1g-idx-SSD.png"
                           format="PNG" />
              </imageobject>

            </mediaobject>
          </figure>

          <para>Finally, we should remark that SSD can't compete with
          traditional spinning disks in terms of capacity as they can only
          provide, for a similar cost, between 1/10th and 1/50th of the size
          of traditional disks.  It is here where the compression capabilities
          of PyTables can be very helpful because both tables and indexes can
          be compressed and the final space can be reduced by typically 2x to
          5x (4x to 10x when compared with traditional relational databases).
          Best of all, as already mentioned, performance is not degraded when
          compression is used, but actually <emphasis>improved</emphasis>.
          So, by using PyTables and SSD you can query larger datasets that
          otherwise would require spinning disks when using other databases
          <footnote><para>In fact, we were unable to run the PostgreSQL
          benchmark in this case because the space needed exceeded the
          capacity of our SSD.</para></footnote>, while allowing improvements
          in the speed of indexed queries between 2x (for medium to low
          selectivity queries) and 10x (for high selectivity queries).
          </para>

        </section>

        <section id="Sorting-indexes">
          <title>Achieving ultimate speed: sorted tables and beyond</title>

          <warning>
            <para>Sorting a large table is a costly operation.  The next
            procedure should only be performed when your dataset is mainly
            read-only and meant to be queried many times.</para>
          </warning>

          <para>When querying large tables, most of the query time is spent in
          locating the interesting rows to be read from disk.  In some
          occasions, you may have queries whose result depends
          <emphasis>mainly</emphasis> of one single column (a query with only
          one single condition is the trivial example), so we can guess that
          sorting the table by this column would lead to locate the
          interesting rows in a much more efficient way (because they would be
          mostly <emphasis>contiguous</emphasis>).  We are going to confirm
          this guess.
          </para>

          <para>For the case of the query that we have been using in the
          previous sections:</para>

          <screen>result = [row['col2'] for row in table.where(
    '(((col4 &gt;= lim1) &amp; (col4 &lt; lim2)) |
      ((col2 &gt; lim3) &amp; (col2 &lt; lim4)) &amp;
      ((col1+3.1*col2+col3*col4) > lim5))')]</screen>

          <para>it is possible to determine, by analysing the data
          distribution and the query limits, that <literal>col4</literal> is
          such a <emphasis>main column</emphasis>.  So, by ordering the table
          by the <literal>col4</literal> column (for example, by specifying
          setting the column to sort by in the <literal>sortby</literal>
          parameter in the <literal>Table.copy()</literal> method, see
          <xref linkend="Table.copy" />), and re-indexing
          <literal>col2</literal> and <literal>col4</literal> afterwards, we
          should get much faster performance for our query.  This is
          effectively demonstrated in <xref
          linkend="queryTimes-indexed-sorted" xrefstyle="select: label"/>,
          where one can see how queries with a low to medium (up to 10000)
          number of hits can be done in around 1 tenth of second for a RAID-0
          setup and in around 1 hundredth of second for a SSD disk.  This
          represents up to more that 100x improvement in speed with respect to
          the times with unsorted tables.  On the other hand, when the number
          of hits is large (&gt; 1 million), the query times grow almost
          linearly, showing a near-perfect scalability for both RAID-0 and SSD
          setups (the sequential access to disk becomes the bottleneck in this
          case).
          </para>

          <figure id="queryTimes-indexed-sorted">
            <title>Times for complex queries with a cold cache (mean of 5
            first random queries) for unsorted and sorted tables.</title>

            <mediaobject>

              <imageobject role="fo">
                <imagedata align="center"
                           fileref="Q8-1g-idx-sorted.svg"
                           format="SVG" scale="60" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="Q8-1g-idx-sorted.png"
                           format="PNG" />
              </imageobject>

            </mediaobject>
          </figure>

          <para>Even though we have shown many ways to improve query times
          that should fulfill the needs of most of people, for those needing
          more, you can for sure discover new optimization opportunities.  For
          example, querying against sorted tables is limited mainly by
          sequential access to data on disk and data compression capability,
          so you may want to read <xref linkend="chunksizeFineTune"
          xrefstyle="select: label" />, for ways on improving this aspect.
          Reading the other sections of this chapter will help in finding new
          roads for increasing the performance as well.  You know, the limit
          for stopping the optimization process is basically your imagination
          (but, most plausibly, your available time ;-).
          </para>

        </section>
      </section>

      <section id="compressionIssues">
        <title>Compression issues</title>

        <para>One of the beauties of PyTables is that it supports compression
        on tables and arrays<footnote><para>Except for
        <literal>Array</literal> objects.</para></footnote>, although it is
        not used by default. Compression of big amounts of data might be a bit
        controversial feature, because it has a legend of being a very big
        consumer of CPU time resources. However, if you are willing to check
        if compression can help not only by reducing your dataset file size
        but <emphasis>also</emphasis> by improving I/O efficiency, specially
        when dealing with very large datasets, keep reading.</para>

        <section id="compressionStudy">
          <title>A study on supported compression libraries</title>

          <para>The compression library used by default is the
          <emphasis>Zlib</emphasis> (see <biblioref linkend="zlibRef"
          />). Since HDF5 <emphasis>requires</emphasis> it, you can safely use
          it and expect that your HDF5 files will be readable on any other
          platform that has HDF5 libraries installed. Zlib provides good
          compression ratio, although somewhat slow, and reasonably fast
          decompression.  Because of that, it is a good candidate to be used
          for compressing you data.</para>

          <para>However, in some situations it is critical to have a
          <emphasis>very good decompression speed</emphasis> (at the expense
          of lower compression ratios or more CPU wasted on compression, as we
          will see soon). In others, the emphasis is put in achieving the
          <emphasis>maximum compression ratios</emphasis>, no matter which
          reading speed will result. This is why support for two additional
          compressors has been added to PyTables: LZO (see <biblioref
          linkend="lzoRef" />) and bzip2 (see <biblioref linkend="bzip2Ref"
          />). Following the author of LZO (and checked by the author of this
          section, as you will see soon), LZO offers pretty fast compression
          and extremely fast decompression. In fact, LZO is so fast when
          compressing/decompressing that it may well happen (that depends on
          your data, of course) that writing or reading a compressed dataset
          is sometimes faster than if it is not compressed at all (specially
          when dealing with extremely large datasets). This fact is very
          important, specially if you have to deal with very large amounts of
          data. Regarding bzip2, it has a reputation of achieving excellent
          compression ratios, but at the price of spending much more CPU time,
          which results in very low compression/decompression speeds.</para>

          <para>Be aware that the LZO and bzip2 support in PyTables is not
          standard on HDF5, so if you are going to use your PyTables files in
          other contexts different from PyTables you will not be able to read
          them. Still, see the <xref linkend="ptrepackDescr"
          xrefstyle="select: label" /> (where the <literal>ptrepack</literal>
          utility is described) to find a way to free your files from LZO or
          bzip2 dependencies, so that you can use these compressors locally
          with the warranty that you can replace them with Zlib (or even
          remove compression completely) if you want to use these files with
          other HDF5 tools or platforms afterwards.</para>

          <para>In order to allow you to grasp what amount of compression can
          be achieved, and how this affects performance, a series of
          experiments has been carried out. All the results presented in this
          section (and in the next one) have been obtained with synthetic data
          and using PyTables 1.3. Also, the tests have been conducted on a IBM
          OpenPower 720 (e-series) with a PowerPC G5 at 1.65 GHz and a hard
          disk spinning at 15K RPM. As your data and platform may be totally
          different for your case, take this just as a guide because your
          mileage may vary. Finally, and to be able to play with tables with a
          number of rows as large as possible, the record size has been chosen
          to be small (16 bytes). Here is its definition:</para>

          <screen>class Bench(IsDescription):
    var1 = StringCol(length=4)
    var2 = IntCol()
    var3 = FloatCol()</screen>

          <para>With this setup, you can look at the compression ratios that
          can be achieved in <xref linkend="comprTblComparison"
          xrefstyle="select: label" />. As you can see, LZO is the compressor
          that performs worse in this sense, but, curiously enough, there is
          not much difference between Zlib and bzip2.</para>

          <figure id="comprTblComparison">
            <title>Comparison between different compression libraries.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" fileref="compressed-recordsize.svg"
                           format="SVG" scale="65" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" fileref="compressed-recordsize.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>

          <para>Also, PyTables lets you select different compression levels for
          Zlib and bzip2, although you may get a bit disappointed by the small
          improvement that these compressors show when dealing with a
          combination of numbers and strings as in our example. As a reference,
          see plot <xref linkend="comprZlibComparison" xrefstyle="select:
          labelnumber" /> for a comparison of the compression achieved by
          selecting different levels of Zlib.  Very oddly, the best compression
          ratio corresponds to level 1 (!).  See later for an explanation and
          more figures on this subject.</para>

          <figure id="comprZlibComparison">
            <title>Comparison between different compression levels of
            Zlib.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center"
                           fileref="compressed-recordsize-zlib.svg" format="SVG"
                           scale="65" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="compressed-recordsize-zlib.png" format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>

          <para>Have also a look at <xref linkend="comprWriteComparison"
          xrefstyle="select: label" />. It shows how the speed of writing rows
          evolves as the size (number of rows) of the table grows. Even though
          in these graphs the size of one single row is 16 bytes, you can most
          probably extrapolate these figures to other row sizes.</para>

          <figure id="comprWriteComparison">
            <title>Writing tables with several compressors.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" fileref="compressed-writing.svg"
                           format="SVG" scale="65" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" fileref="compressed-writing.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>

          <para>In <xref linkend="comprReadNoCacheComparison"
          xrefstyle="select: label" /> you can see how compression affects the
          reading performance. In fact, what you see in the plot is an
          <emphasis>in-kernel selection</emphasis> speed, but provided that this
          operation is very fast (see <xref linkend="inkernelSearch"
          xrefstyle="select: label" />), we can accept it as an actual read
          test. Compared with the reference line without compression, the
          general trend here is that LZO does not affect too much the reading
          performance (and in some points it is actually better), Zlib makes
          speed drop to a half, while bzip2 is performing very slow (up to 8x
          slower).</para>

          <para>Also, in the same <xref linkend="comprReadNoCacheComparison"
          xrefstyle="select: label" /> you can notice some strange peaks in the
          speed that we might be tempted to attribute to libraries on which
          PyTables relies (HDF5, compressors...), or to PyTables itself.
          However, <xref linkend="comprReadCacheComparison" xrefstyle="select:
          label" /> reveals that, if we put the file in the filesystem cache (by
          reading it several times before, for example), the evolution of the
          performance is much smoother. So, the most probable explanation would
          be that such peaks are a consequence of the underlying OS filesystem,
          rather than a flaw in PyTables (or any other library behind
          it). Another consequence that can be derived from the aforementioned
          plot is that LZO decompression performance is much better than Zlib,
          allowing an improvement in overall speed of more than 2x, and perhaps
          more important, the read performance for really large datasets
          (i.e. when they do not fit in the OS filesystem cache) can be actually
          <emphasis>better</emphasis> than not using compression at
          all. Finally, one can see that reading performance is very badly
          affected when bzip2 is used (it is 10x slower than LZO and 4x than
          Zlib), but this was somewhat expected anyway.</para>

          <figure id="comprReadNoCacheComparison">
            <title>Selecting values in tables with several compressors. The file
            is not in the OS cache.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center"
                           fileref="compressed-select-nocache.svg" format="SVG"
                           scale="65" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="compressed-select-nocache.png" format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>

          <figure id="comprReadCacheComparison">
            <title>Selecting values in tables with several compressors. The file
            is in the OS cache.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" fileref="compressed-select-cache.svg"
                           format="SVG" scale="65" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" fileref="compressed-select-cache.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>

          <para>So, generally speaking and looking at the experiments above, you
          can expect that LZO will be the fastest in both compressing and
          decompressing, but the one that achieves the worse compression ratio
          (although that may be just OK for many situations, specially when used
          with shuffling —see <xref linkend="ShufflingOptim" xrefstyle="select:
          label" />).  bzip2 is the slowest, by large, in both compressing and
          decompressing, and besides, it does not achieve any better compression
          ratio than Zlib. Zlib represents a balance between them: it's somewhat
          slow compressing (2x) and decompressing (3x) than LZO, but it normally
          achieves better compression ratios.</para>

          <para>Finally, by looking at the plots <xref
          linkend="comprWriteZlibComparison" xrefstyle="select: labelnumber" />,
          <xref linkend="comprReadZlibComparison" xrefstyle="select:
                                                             labelnumber" />, and the aforementioned <xref
                                                             linkend="comprZlibComparison" xrefstyle="select: labelnumber" /> you
          can see why the recommended compression level to use for all
          compression libraries is 1.  This is the lowest level of compression,
          but as the size of the underlying HDF5 chunk size is normally rather
          small compared with the size of compression buffers, there is not much
          point in increasing the latter (i.e. increasing the compression
          level).  Nonetheless, in some situations (like for example, in
          extremely large tables or arrays, where the computed chunk size can be
          rather large) you may want to check, on your own, how the different
          compression levels do actually affect your application.</para>

          <para>You can select the compression library and level by setting the
          <literal>complib</literal> and <literal>complevel</literal> keywords
          in the <literal>Filters</literal> class (see <xref
          linkend="FiltersClassDescr" xrefstyle="select: label" />). A
          compression level of 0 will completely disable compression (the
          default), 1 is the less memory and CPU time demanding level, while 9
          is the maximum level and the most memory demanding and CPU
          intensive. Finally, have in mind that LZO is not accepting a
          compression level right now, so, when using LZO, 0 means that
          compression is not active, and any other value means that LZO is
          active.</para>

          <para>So, in conclusion, if your ultimate goal is writing and reading
          as fast as possible, choose LZO. If you want to reduce as much as
          possible your data, while retaining acceptable read speed, choose
          Zlib. Finally, if portability is important for you, Zlib is your best
          bet. So, when you want to use bzip2? Well, looking at the results, it
          is difficult to recommend its use in general, but you may want to
          experiment with it in those cases where you know that it is well
          suited for your data pattern (for example, for dealing with repetitive
          string datasets).</para>

          <figure id="comprWriteZlibComparison">
            <title>Writing in tables with different levels of
            compression.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" fileref="compressed-writing-zlib.svg"
                           format="SVG" scale="65" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" fileref="compressed-writing-zlib.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>

          <figure id="comprReadZlibComparison">
            <title>Selecting values in tables with different levels of
            compression. The file is in the OS cache.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center"
                           fileref="compressed-select-cache-zlib.svg"
                           format="SVG" scale="65" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="compressed-select-cache-zlib.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>

        <section id="ShufflingOptim">
          <title>Shuffling (or how to make the compression process more
          effective)</title>

          <para>The HDF5 library provides an interesting filter that can
          leverage the results of your favorite compressor. Its name is
          <emphasis>shuffle</emphasis>, and because it can greatly benefit
          compression and it does not take many CPU resources (see below for a
          justification), it is active <emphasis>by default</emphasis> in
          PyTables whenever compression is activated (independently of the
          chosen compressor). It is deactivated when compression is off (which
          is the default, as you already should know). Of course, you can
          deactivate it if you want, but this is not recommended.</para>

          <para>So, how does this mysterious filter exactly work? From the HDF5
          reference manual: <quote>The shuffle filter de-interlaces a block of
          data by reordering the bytes. All the bytes from one consistent byte
          position of each data element are placed together in one block; all
          bytes from a second consistent byte position of each data element are
          placed together a second block; etc. For example, given three data
          elements of a 4-byte datatype stored as 012301230123, shuffling will
          re-order data as 000111222333. This can be a valuable step in an
          effective compression algorithm because the bytes in each byte
          position are often closely related to each other and putting them
          together can increase the compression ratio.</quote></para>

          <para>In <xref linkend="comprShuffleComparison"
          xrefstyle="select: label" /> you can see a benchmark that shows how
          the <emphasis>shuffle</emphasis> filter can help the different
          libraries in compressing data. In this experiment, shuffle has made
          LZO compress almost 3x more (!), while Zlib and bzip2 are seeing
          improvements of 2x. Once again, the data for this experiment is
          synthetic, and <emphasis>shuffle</emphasis> seems to do a great work
          with it, but in general, the results will vary in each case<footnote>
          <para>Some users reported that the typical improvement with real
          data is between a factor 1.5x and 2.5x over the already compressed
          datasets.</para>
          </footnote>.</para>

          <figure id="comprShuffleComparison">
            <title>Comparison between different compression libraries with and
            without the <emphasis>shuffle</emphasis> filter.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center"
                           fileref="compressed-recordsize-shuffle.svg"
                           format="SVG" scale="65" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="compressed-recordsize-shuffle.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>

          <para>At any rate, the most remarkable fact about the
          <emphasis>shuffle</emphasis> filter is the relatively high level of
          compression that compressor filters can achieve when used in
          combination with it. A curious thing to note is that the Bzip2
          compression rate does not seem very much improved (less than a 40%),
          and what is more striking, Bzip2+shuffle does compress quite
          <emphasis>less</emphasis> than Zlib+shuffle or LZO+shuffle
          combinations, which is kind of unexpected. The thing that seems clear
          is that Bzip2 is not very good at compressing patterns that result of
          shuffle application. As always, you may want to experiment with your
          own data before widely applying the Bzip2+shuffle combination in order
          to avoid surprises.</para>

          <para>Now, how does shuffling affect performance? Well, if you look at
          plots <xref linkend="comprWriteShuffleComparison"
          xrefstyle="select: labelnumber" />, <xref
          linkend="comprReadNoCacheShuffleComparison"
          xrefstyle="select:         labelnumber" /> and <xref
          linkend="comprReadCacheShuffleComparison"
          xrefstyle="select: labelnumber" />, you will get a somewhat unexpected
          (but pleasant) surprise. Roughly, <emphasis>shuffle</emphasis> makes
          the writing process (shuffling+compressing) faster (approximately a 15%
          for LZO, 30% for Bzip2 and a 80% for Zlib), which is an interesting
          result by itself. But perhaps more exciting is the fact that the
          reading process (unshuffling+decompressing) is also accelerated by a
          similar extent (a 20% for LZO, 60% for Zlib and a 75% for Bzip2,
          roughly).</para>

          <figure id="comprWriteShuffleComparison">
            <title>Writing with different compression libraries with and without
            the <emphasis>shuffle</emphasis> filter.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center"
                           fileref="compressed-writing-shuffle.svg" format="SVG"
                           scale="65" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="compressed-writing-shuffle.png" format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>

          <figure id="comprReadNoCacheShuffleComparison">
            <title>Reading with different compression libraries with the
            <emphasis>shuffle</emphasis> filter. The file is not in OS
            cache.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center"
                           fileref="compressed-select-nocache-shuffle-only.svg"
                           format="SVG" scale="65" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="compressed-select-nocache-shuffle-only.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>

          <figure id="comprReadCacheShuffleComparison">
            <title>Reading with different compression libraries with and without
            the <emphasis>shuffle</emphasis> filter. The file is in OS
            cache.</title>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center"
                           fileref="compressed-select-cache-shuffle.svg"
                           format="SVG" scale="65" />
              </imageobject>

              <imageobject role="html">
                <imagedata align="center"
                           fileref="compressed-select-cache-shuffle.png"
                           format="PNG" />
              </imageobject>
            </mediaobject>
          </figure>

          <para>You may wonder why introducing another filter in the write/read
          pipelines does effectively accelerate the throughput. Well, maybe data
          elements are more similar or related column-wise than row-wise, i.e.
          contiguous elements in the same column are more alike, so shuffling
          makes the job of the compressor easier (faster) and more effective
          (greater ratios). As a side effect, compressed chunks do fit better in
          the CPU cache (at least, the chunks are smaller!) so that the process
          of unshuffle/decompress can make a better use of the cache (i.e.
          reducing the number of CPU cache faults).</para>

          <para>So, given the potential gains (faster writing and reading, but
          specially much improved compression level), it is a good thing to have
          such a filter enabled by default in the battle for discovering
          redundancy when you want to compress your data, just as PyTables
          does.</para>

        </section>
      </section>

      <section>
        <title>Using Psyco</title>

        <para>Psyco (see <biblioref linkend="psycoRef" />) is a kind of
        specialized compiler for Python that typically accelerates Python
        applications with no change in source code. You can think of Psyco as
        a kind of just-in-time (JIT) compiler, a little bit like Java's, that
        emits machine code on the fly instead of interpreting your Python
        program step by step. The result is that your unmodified Python
        programs run faster.</para>

        <para>Psyco is very easy to install and use, so in most scenarios it
        is worth to give it a try. However, it only runs on Intel 386
        architectures, so if you are using other architectures, you are out of
        luck (and, moreover, it seems that there are no plans to support other
        platforms).  Besides, with the addition of flexible (and very fast)
        in-kernel queries (by the way, they cannot be optimized at all by
        Psyco), the use of Psyco will only help in rather few scenarios.  In
        fact, the only important situation that you might benefit right now
        from using Psyco (I mean, in PyTables contexts) is for speeding-up the
        write speed in tables when using the Row interface (see
        <xref linkend="RowClassDescr" />).  But again, this latter case can
        also be accelerated by using the Table.append() (see
        <xref linkend="Table.append" />) method and building your own
        buffers <footnote><para>So, there is not much point in using Psyco
        with recent versions of PyTables anymore</para></footnote>.</para>

        <para>As an example, imagine that you have a small script that reads
        and selects data over a series of datasets, like this:</para>

        <screen>def readFile(filename):
    "Select data from all the tables in filename"

    fileh = openFile(filename, mode = "r")
    result = []
    for table in fileh("/", 'Table'):
    result = [p['var3'] for p in table if p['var2'] &lt;= 20]

    fileh.close()
    return result

if __name__=="__main__":
print readFile("myfile.h5")</screen>

        <para>In order to accelerate this piece of code, you can rewrite your
        main program to look like:</para>

        <screen>if __name__=="__main__":
import psyco
psyco.bind(readFile)
print readFile("myfile.h5")</screen>

        <para>That's all!  From now on, each time that you execute your Python
        script, Psyco will deploy its sophisticated algorithms so as to
        accelerate your calculations.</para>

        <para>You can see in the graphs <xref linkend="psycoWriteComparison"
        xrefstyle="select: labelnumber" /> and <xref
        linkend="psycoReadComparison"
        xrefstyle="select:         labelnumber" /> how much I/O speed
        improvement you can get by using Psyco. By looking at this figures you
        can get an idea if these improvements are of your interest or not. In
        general, if you are not going to use compression you will take
        advantage of Psyco if your tables are medium sized (from a thousand to
        a million rows), and this advantage will disappear progressively when
        the number of rows grows well over one million. However if you use
        compression, you will probably see improvements even beyond this limit
        (see <xref linkend="compressionIssues" xrefstyle="select: label" />).
        As always, there is no substitute for experimentation with your own
        dataset.</para>

        <figure id="psycoWriteComparison">
          <title>Writing tables with/without Psyco.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center"
                         fileref="write-medium-psyco-nopsyco-comparison.svg"
                         format="SVG" scale="35" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center"
                         fileref="write-medium-psyco-nopsyco-comparison.png"
                         format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>

        <figure id="psycoReadComparison">
          <title>Reading tables with/without Psyco.</title>

          <mediaobject>
            <imageobject role="fo">
              <imagedata align="center"
                         fileref="read-medium-psyco-nopsyco-comparison.svg"
                         format="SVG" scale="35" />
            </imageobject>

            <imageobject role="html">
              <imagedata align="center"
                         fileref="read-medium-psyco-nopsyco-comparison.png"
                         format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section id="LRUOptim">
        <title>Getting the most from the node LRU cache</title>

        <para>One limitation of the initial versions of PyTables was that they
        needed to load all nodes in a file completely before being ready to
        deal with them, making the opening times for files with a lot of nodes
        very high and unacceptable in many cases.</para>

        <para>Starting from PyTables 1.2 on, a new lazy node loading schema
        was setup that avoids loading all the nodes of the <emphasis>object
        tree</emphasis> in memory. In addition, a new LRU cache was introduced
        in order to accelerate the access to already visited nodes. This cache
        (one per file) is responsible for keeping up the most recently visited
        nodes in memory and discard the least recent used ones. This
        represents a big advantage over the old schema, not only in terms of
        memory usage (as there is no need to load <emphasis>every</emphasis>
        node in memory), but it also adds very convenient optimizations for
        working interactively like, for example, speeding-up the opening times
        of files with lots of nodes, allowing to open almost any kind of file
        in typically less than one tenth of second (compare this with the more
        than 10 seconds for files with more than 10000 nodes in PyTables
        pre-1.2 era) as well as optimizing the access to frequently visited
        nodes. See <biblioref linkend="NewObjectTreeCacheRef" /> for more info
        on the advantages (and also drawbacks) of this approach.</para>

        <para>One thing that deserves some discussion is the election of the
        parameter that sets the maximum amount of nodes to be kept in memory
        at any time. As PyTables is meant to be deployed in machines that can
        have potentially low memory, the default for it is quite conservative
        (you can look at its actual value in the
        <literal>NODE_CACHE_SLOTS</literal> parameter in module
        <literal>tables/parameters.py</literal>). However, if you usually need
        to deal with files that have many more nodes than the maximum default,
        and you have a lot of free memory in your system, then you may want to
        experiment in order to see which is the appropriate value of
        <literal>NODE_CACHE_SLOTS</literal> that fits better your
        needs.</para>

        <para>As an example, look at the next code:</para>

        <screen>def browse_tables(filename):
    fileh = openFile(filename,'a')
    group = fileh.root.newgroup
    for j in range(10):
        for tt in fileh.walkNodes(group, "Table"):
            title = tt.attrs.TITLE
            for row in tt:
                pass
    fileh.close()</screen>

        <para>We will be running the code above against a couple of files
        having a <literal>/newgroup</literal> containing 100 tables and 1000
        tables respectively.  In addition, this benchmark is run twice for two
        different values of the LRU cache size, specifically 256 and 1024. You
        can see the results in <xref linkend="LRUTblComparison"
        xrefstyle="select: label" />.</para>

        <table align="center" id="LRUTblComparison">
          <title>Retrieval speed and memory consumption depending on the
          number of nodes in LRU cache.</title>

          <tgroup cols="10">
            <colspec align="left" colname="c1" />

            <colspec align="left" colname="c2" />

            <colspec align="right" colname="c3" />

            <colspec align="right" colname="c4" />

            <colspec align="right" colname="c5" />

            <colspec align="right" colname="c6" />

            <colspec align="right" colname="c7" />

            <colspec align="right" colname="c8" />

            <colspec align="right" colname="c9" />

            <colspec align="right" colname="c10" />

            <thead>
              <row>
                <entry align="left" nameend="c2" namest="c1"></entry>

                <entry align="center" nameend="c6" namest="c3">100
                nodes</entry>

                <entry align="center" nameend="c10" namest="c7">1000
                nodes</entry>
              </row>

              <row>
                <entry align="left" nameend="c2" namest="c1"></entry>

                <entry align="center" nameend="c4" namest="c3">Memory
                (MB)</entry>

                <entry align="center" nameend="c6" namest="c5">Time
                (ms)</entry>

                <entry align="center" nameend="c8" namest="c7">Memory
                (MB)</entry>

                <entry align="center" nameend="c10" namest="c9">Time
                (ms)</entry>
              </row>

              <row>
                <entry align="left">Node is coming from...</entry>

                <entry align="left">Cache size</entry>

                <entry align="right">256</entry>

                <entry align="right">1024</entry>

                <entry align="right">256</entry>

                <entry align="right">1024</entry>

                <entry align="right">256</entry>

                <entry align="right">1024</entry>

                <entry align="right">256</entry>

                <entry align="right">1024</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry align="left">Disk</entry>

                <entry align="left"></entry>

                <entry align="right">14</entry>

                <entry align="right">14</entry>

                <entry align="right">1.24</entry>

                <entry align="right">1.24</entry>

                <entry align="right">51</entry>

                <entry align="right">66</entry>

                <entry align="right">1.33</entry>

                <entry align="right">1.31</entry>
              </row>

              <row>
                <entry align="left">Cache</entry>

                <entry align="left"></entry>

                <entry align="right">14</entry>

                <entry align="right">14</entry>

                <entry align="right">0.53</entry>

                <entry align="right">0.52</entry>

                <entry align="right">65</entry>

                <entry align="right">73</entry>

                <entry align="right">1.35</entry>

                <entry align="right">0.68</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para>From the data in <xref linkend="LRUTblComparison"
        xrefstyle="select: label" />, one can see that when the number of
        objects that you are dealing with does fit in cache, you will get
        better access times to them. Also, incrementing the node cache size
        effectively consumes more memory <emphasis>only</emphasis> if the
        total nodes exceeds the slots in cache; otherwise the memory
        consumption remains the same. It is also worth noting that
        incrementing the node cache size in the case you want to fit all your
        nodes in cache does not take much more memory than being too
        conservative. On the other hand, it might happen that the speed-up
        that you can achieve by allocating more slots in your cache is not
        worth the amount of memory used.</para>

        <para>Also worth noting is that if you have a lot of memory available
        and performance is absolutely critical, you may want to try out a
        negative value for <literal>NODE_CACHE_SLOTS</literal>.  This will
        cause that all the touched nodes will be kept in an internal
        dictionary and this is the faster way to load/retrieve nodes.
        However, and in order to avoid a large memory consumption, the user
        will be warned when the number of loaded nodes will reach the
        <literal>-NODE_CACHE_SLOTS</literal> value.</para>

        <para>Finally, a value of zero in <literal>NODE_CACHE_SLOTS</literal>
        means that any cache mechanism is disabled.</para>

        <para>At any rate, if you feel that this issue is important for you,
        there is no replacement for setting your own experiments up in order
        to proceed to fine-tune the <literal>NODE_CACHE_SLOTS</literal>
        parameter.</para>

        <note>
            <para>PyTables >= 2.3 sports an optimized LRU cache node written
            in C, so you should expect significantly faster LRU cache
            operations when working with it.</para>
        </note>
        <!-- TODO: Rewrite above section to include info on indexes -->
      </section>

      <section>
        <title>Compacting your PyTables files</title>

        <para>Let's suppose that you have a file where you have made a lot of
        row deletions on one or more tables, or deleted many leaves or even
        entire subtrees. These operations might leave
        <emphasis>holes</emphasis> (i.e. space that is not used anymore) in
        your files that may potentially affect not only the size of the files
        but, more importantly, the performance of I/O. This is because when
        you delete a lot of rows in a table, the space is not automatically
        recovered on the fly. In addition, if you add many more rows to a
        table than specified in the <literal>expectedrows</literal> keyword at
        creation time this may affect performance as well, as explained in
        <xref linkend="expectedRowsOptim" xrefstyle="select: label" />.</para>

        <para>In order to cope with these issues, you should be aware that
        PyTables includes a handy utility called <literal>ptrepack</literal>
        which can be very useful not only to compact
        <emphasis>fragmented</emphasis> files, but also to adjust some
        internal parameters in order to use better buffer and chunk sizes for
        optimum I/O speed. Please check the <xref linkend="ptrepackDescr"
        xrefstyle="select: label" /> for a brief tutorial on its use.</para>

        <para>Another thing that you might want to use
        <literal>ptrepack</literal> for is changing the compression filters or
        compression levels on your existing data for different goals, like
        checking how this can affect both final size and I/O performance, or
        getting rid of the optional compressors like <literal>LZO</literal> or
        <literal>bzip2</literal> in your existing files, in case you want to
        use them with generic HDF5 tools that do not have support for these
        filters.</para>
      </section>
    </chapter>
  </part>

  <part label="II">
    <title>Complementary modules</title>

    <chapter id="filenode">
      <title>filenode - simulating a filesystem with PyTables</title>

      <para></para>

      <section>
        <title>What is <literal>filenode</literal>?</title>

        <para><literal>filenode</literal> is a module which enables you to
        create a PyTables database of nodes which can be used like regular
        opened files in Python. In other words, you can store a file in a
        PyTables database, and read and write it as you would do with any
        other file in Python. Used in conjunction with PyTables hierarchical
        database organization, you can have your database turned into an open,
        extensible, efficient, high capacity, portable and metadata-rich
        filesystem for data exchange with other systems (including backup
        purposes).</para>

        <para>Between the main features of <literal>filenode</literal>, one
        can list:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis>Open:</emphasis> Since it relies on PyTables,
            which in turn, sits over HDF5 (see <biblioref
            linkend="HDFWhatIs" />), a standard hierarchical data format from
            NCSA.</para>
          </listitem>

          <listitem>
            <para><emphasis>Extensible:</emphasis> You can define new types of
            nodes, and their instances will be safely preserved (as are normal
            groups, leafs and attributes) by PyTables applications having no
            knowledge of their types. Moreover, the set of possible attributes
            for a node is not fixed, so you can define your own node
            attributes.</para>
          </listitem>

          <listitem>
            <para><emphasis>Efficient:</emphasis> Thanks to PyTables' proven
            extreme efficiency on handling huge amounts of data.
            <literal>filenode</literal> can make use of PyTables' on-the-fly
            compression and decompression of data.</para>
          </listitem>

          <listitem>
            <para><emphasis>High capacity:</emphasis> Since PyTables and HDF5
            are designed for massive data storage (they use 64-bit addressing
            even where the platform does not support it natively).</para>
          </listitem>

          <listitem>
            <para><emphasis>Portable:</emphasis> Since the HDF5 format has an
            architecture-neutral design, and the HDF5 libraries and PyTables
            are known to run under a variety of platforms. Besides that, a
            PyTables database fits into a single file, which poses no trouble
            for transportation.</para>
          </listitem>

          <listitem>
            <para><emphasis>Metadata-rich:</emphasis> Since PyTables can store
            arbitrary key-value pairs (even Python objects!) for every
            database node. Metadata may include authorship, keywords, MIME
            types and encodings, ownership information, access control lists
            (ACL), decoding functions and anything you can imagine!</para>
          </listitem>
        </itemizedlist>
      </section>

      <section>
        <title>Finding a <literal>filenode</literal> node</title>

        <para><literal>filenode</literal> nodes can be recognized because they
        have a <literal>NODE_TYPE</literal> system attribute with a
        <literal>'file'</literal> value. It is recommended that you use the
        <literal>getNodeAttr()</literal> method (see <xref
        linkend="File.getNodeAttr" />) of <literal>tables.File</literal> class
        to get the <literal>NODE_TYPE</literal> attribute independently of the
        nature (group or leaf) of the node, so you do not need to care
        about.</para>
      </section>

      <section>
        <title><literal>filenode</literal> - simulating files inside
        PyTables</title>

        <para>The <literal>filenode</literal> module is part of the
        <literal>nodes</literal> sub-package of PyTables. The recommended way
        to import the module is:</para>

        <screen>>>> from tables.nodes import filenode</screen>

        <para>However, <literal>filenode</literal> exports very few symbols,
        so you can import <literal>*</literal> for interactive usage. In fact,
        you will most probably only use the <literal>NodeType</literal>
        constant and the <literal>newNode()</literal> and
        <literal>openNode()</literal> calls.</para>

        <para>The <literal>NodeType</literal> constant contains the value that
        the <literal>NODE_TYPE</literal> system attribute of a node file is
        expected to contain (<literal>'file'</literal>, as we have seen).
        Although this is not expected to change, you should use
        <literal>filenode.NodeType</literal> instead of the literal
        <literal>'file'</literal> when possible.</para>

        <para><literal>newNode()</literal> and <literal>openNode()</literal>
        are the equivalent to the Python <literal>file()</literal> call (alias
        <literal>open()</literal>) for ordinary files. Their arguments differ
        from that of <literal>file()</literal>, but this is the only point
        where you will note the difference between working with a node file
        and working with an ordinary file.</para>

        <para>For this little tutorial, we will assume that we have a PyTables
        database opened for writing. Also, if you are somewhat lazy at typing
        sentences, the code that we are going to explain is included in the
        <literal>examples/filenodes1.py</literal> file.</para>

        <para>You can create a brand new file with these sentences:</para>

        <screen>>>> import tables
>>> h5file = tables.openFile('fnode.h5', 'w')</screen>

        <section>
          <title>Creating a new file node</title>

          <para>Creation of a new file node is achieved with the
          <literal>newNode()</literal> call. You must tell it in which
          PyTables file you want to create it, where in the PyTables hierarchy
          you want to create the node and which will be its name. The PyTables
          file is the first argument to <literal>newNode()</literal>; it will
          be also called the <literal>'host PyTables file'</literal>. The
          other two arguments must be given as keyword arguments
          <literal>where</literal> and <literal>name</literal>, respectively.
          As a result of the call, a brand new appendable and readable file
          node object is returned.</para>

          <para>So let us create a new node file in the previously opened
          <literal>h5file</literal> PyTables file, named
          <literal>'fnode_test'</literal> and placed right under the root of
          the database hierarchy. This is that command:</para>

          <screen>>>> fnode = filenode.newNode(h5file, where='/', name='fnode_test')</screen>

          <para>That is basically all you need to create a file node. Simple,
          isn't it? From that point on, you can use <literal>fnode</literal>
          as any opened Python file (i.e. you can write data, read data, lines
          of text and so on).</para>

          <para><literal>newNode()</literal> accepts some more keyword
          arguments. You can give a title to your file with the
          <literal>title</literal> argument. You can use PyTables' compression
          features with the <literal>filters</literal> argument. If you know
          beforehand the size that your file will have, you can give its final
          file size in bytes to the <literal>expectedsize</literal> argument
          so that the PyTables library would be able to optimize the data
          access.</para>

          <para><literal>newNode()</literal> creates a PyTables node where it
          is told to. To prove it, we will try to get the
          <literal>NODE_TYPE</literal> attribute from the newly created
          node.</para>

          <screen>>>> print h5file.getNodeAttr('/fnode_test', 'NODE_TYPE')
file</screen>
        </section>

        <section>
          <title>Using a file node</title>

          <para>As stated above, you can use the new node file as any other
          opened file. Let us try to write some text in and read it.</para>

          <screen>>>> print &gt;&gt; fnode, "This is a test text line."
>>> print &gt;&gt; fnode, "And this is another one."
>>> print &gt;&gt; fnode
>>> fnode.write("Of course, file methods can also be used.")
>>>
>>> fnode.seek(0)  # Go back to the beginning of file.
>>>
>>> for line in fnode:
...   print repr(line)
'This is a test text line.\n'
'And this is another one.\n'
'\n'
'Of course, file methods can also be used.'</screen>

          <para>This was run on a Unix system, so newlines are expressed as
          <literal>'\n'</literal>. In fact, you can override the line
          separator for a file by setting its <literal>lineSeparator</literal>
          property to any string you want.</para>

          <para>While using a file node, you should take care of closing it
          <emphasis>before</emphasis> you close the PyTables host file.
          Because of the way PyTables works, your data it will not be at a
          risk, but every operation you execute after closing the host file
          will fail with a <literal>ValueError</literal>. To close a file
          node, simply delete it or call its <literal>close()</literal>
          method.</para>

          <screen>>>> fnode.close()
>>> print fnode.closed
True</screen>
        </section>

        <section>
          <title>Opening an existing file node</title>

          <para>If you have a file node that you created using
          <literal>newNode()</literal>, you can open it later by calling
          <literal>openNode()</literal>. Its arguments are similar to that of
          <literal>file()</literal> or <literal>open()</literal>: the first
          argument is the PyTables node that you want to open (i.e. a node
          with a <literal>NODE_TYPE</literal> attribute having a
          <literal>'file'</literal> value), and the second argument is a mode
          string indicating how to open the file. Contrary to
          <literal>file()</literal>, <literal>openNode()</literal> can not be
          used to create a new file node.</para>

          <para>File nodes can be opened in read-only mode
          (<literal>'r'</literal>) or in read-and-append mode
          (<literal>'a+'</literal>). Reading from a file node is allowed in
          both modes, but appending is only allowed in the second one. Just
          like Python files do, writing data to an appendable file places it
          after the file pointer if it is on or beyond the end of the file, or
          otherwise after the existing data. Let us see an example:</para>

          <screen>>>> node = h5file.root.fnode_test
>>> fnode = filenode.openNode(node, 'a+')
>>> print repr(fnode.readline())
'This is a test text line.\n'
>>> print fnode.tell()
26
>>> print &gt;&gt; fnode, "This is a new line."
>>> print repr(fnode.readline())
''</screen>

          <para>Of course, the data append process places the pointer at the
          end of the file, so the last <literal>readline()</literal> call hit
          <literal>EOF</literal>. Let us seek to the beginning of the file to
          see the whole contents of our file.</para>

          <screen>>>> fnode.seek(0)
>>> for line in fnode:
...   print repr(line)
'This is a test text line.\n'
'And this is another one.\n'
'\n'
'Of course, file methods can also be used.This is a new line.\n'</screen>

          <para>As you can check, the last string we wrote was correctly
          appended at the end of the file, instead of overwriting the second
          line, where the file pointer was positioned by the time of the
          appending.</para>
        </section>

        <section>
          <title>Adding metadata to a file node</title>

          <para>You can associate arbitrary metadata to any open node file,
          regardless of its mode, as long as the host PyTables file is
          writable. Of course, you could use the
          <literal>setNodeAttr()</literal> method of
          <literal>tables.File</literal> to do it directly on the proper node,
          but <literal>filenode</literal> offers a much more comfortable way
          to do it. <literal>filenode</literal> objects have an
          <literal>attrs</literal> property which gives you direct access to
          their corresponding <literal>AttributeSet</literal> object.</para>

          <para>For instance, let us see how to associate MIME type metadata
          to our file node:</para>

          <screen>>>> fnode.attrs.content_type = 'text/plain; charset=us-ascii'</screen>

          <para>As simple as A-B-C. You can put nearly anything in an
          attribute, which opens the way to authorship, keywords, permissions
          and more. Moreover, there is not a fixed list of attributes.
          However, you should avoid names in all caps or starting with
          <literal>'_'</literal>, since PyTables and
          <literal>filenode</literal> may use them internally. Some valid
          examples:</para>

          <screen>>>> fnode.attrs.author = "Ivan Vilata i Balaguer"
>>> fnode.attrs.creation_date = '2004-10-20T13:25:25+0200'
>>> fnode.attrs.keywords_en = ["FileNode", "test", "metadata"]
>>> fnode.attrs.keywords_ca = ["FileNode", "prova", "metadades"]
>>> fnode.attrs.owner = 'ivan'
>>> fnode.attrs.acl = {'ivan': 'rw', '@users': 'r'}</screen>

          <para>You can check that these attributes get stored by running the
          <literal>ptdump</literal> command on the host PyTables file:</para>

          <screen>$ ptdump -a fnode.h5:/fnode_test
/fnode_test (EArray(113,)) ''
/fnode_test.attrs (AttributeSet), 14 attributes:
[CLASS := 'EARRAY',
EXTDIM := 0,
FLAVOR := 'numpy',
NODE_TYPE := 'file',
NODE_TYPE_VERSION := 2,
TITLE := '',
VERSION := '1.2',
acl := {'ivan': 'rw', '@users': 'r'},
author := 'Ivan Vilata i Balaguer',
content_type := 'text/plain; charset=us-ascii',
creation_date := '2004-10-20T13:25:25+0200',
keywords_ca := ['FileNode', 'prova', 'metadades'],
keywords_en := ['FileNode', 'test', 'metadata'],
owner := 'ivan']</screen>

          <para>Note that <literal>filenode</literal> makes no assumptions
          about the meaning of your metadata, so its handling is entirely left
          to your needs and imagination.</para>
        </section>
      </section>

      <section>
        <title>Complementary notes</title>

        <para>You can use file nodes and PyTables groups to mimic a filesystem
        with files and directories. Since you can store nearly anything you
        want as file metadata, this enables you to use a PyTables file as a
        portable compressed backup, even between radically different
        platforms. Take this with a grain of salt, since node files are
        restricted in their naming (only valid Python identifiers are valid);
        however, remember that you can use node titles and metadata to
        overcome this limitation. Also, you may need to devise some strategy
        to represent special files such as devices, sockets and such (not
        necessarily using <literal>filenode</literal>).</para>

        <para>We are eager to hear your opinion about
        <literal>filenode</literal> and its potential uses. Suggestions to
        improve <literal>filenode</literal> and create other node types are
        also welcome. Do not hesitate to contact us!</para>
      </section>

      <section>
        <title>Current limitations</title>

        <para><literal>filenode</literal> is still a young piece of software,
        so it lacks some functionality. This is a list of known current
        limitations:</para>

        <orderedlist>
          <listitem>
            <para>Node files can only be opened for read-only or read and
            append mode. This should be enhanced in the future.</para>
          </listitem>

          <!-- Near future? -->

          <listitem>
            <para>There is no universal newline support yet. This is likely to
            be implemented in a near future.</para>
          </listitem>

          <listitem>
            <para>Sparse files (files with lots of zeros) are not treated
            specially; if you want them to take less space, you should be
            better off using compression.</para>
          </listitem>
        </orderedlist>

        <para>These limitations still make <literal>filenode</literal>
        entirely adequate to work with most binary and text files. Of course,
        suggestions and patches are welcome.</para>
      </section>

      <section>
        <title><literal>filenode</literal> module reference</title>

        <section>
          <title>Global constants</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis role="bold">NodeType</emphasis></glossterm>

              <glossdef>
                <para>Value for <literal>NODE_TYPE</literal> node system
                attribute.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">NodeTypeVersions</emphasis></glossterm>

              <glossdef>
                <para>Supported values for
                <literal>NODE_TYPE_VERSION</literal> node system
                attribute.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title>Global functions</title>

          <section>
            <title>newNode(h5file, where, name, title="", filters=None,
            expectedsize=1000)</title>

            <para>Creates a new file node object in the specified PyTables
            file object. Additional named arguments <literal>where</literal>
            and <literal>name</literal> must be passed to specify where the
            file node is to be created. Other named arguments such as
            <literal>title</literal> and <literal>filters</literal> may also
            be passed. The special named argument
            <literal>expectedsize</literal>, indicating an estimate of the
            file size in bytes, may also be passed. It returns the file node
            object.</para>
          </section>

          <section>
            <title>openNode(node, mode = 'r')</title>

            <para>Opens an existing file node. Returns a file node object from
            the existing specified PyTables node. If mode is not specified or
            it is <literal>'r'</literal>, the file can only be read, and the
            pointer is positioned at the beginning of the file. If mode is
            <literal>'a+'</literal>, the file can be read and appended, and
            the pointer is positioned at the end of the file.</para>
          </section>
        </section>

        <section>
          <title>The <literal>FileNode</literal> abstract class</title>

          <para>This is the ancestor of <literal>ROFileNode</literal> and
          <literal>RAFileNode</literal> (see below). Instances of these
          classes are returned when <literal>newNode()</literal> or
          <literal>openNode()</literal> are called. It represents a new file
          node associated with a PyTables node, providing a standard Python
          file interface to it.</para>

          <para>This abstract class provides only an implementation of the
          reading methods needed to implement a file-like object over a
          PyTables node. The attribute set of the node becomes available via
          the <literal>attrs</literal> property. You can add attributes there,
          but try to avoid attribute names in all caps or starting with
          <literal>'_'</literal>, since they may clash with internal
          attributes.</para>

          <para>The node used as storage is also made available via the
          read-only attribute <literal>node</literal>. Please do not tamper
          with this object unless unavoidably, since you may break the
          operation of the file node object.</para>

          <para>The <literal>lineSeparator</literal> property contains the
          string used as a line separator, and defaults to
          <literal>os.linesep</literal>. It can be set to any reasonably-sized
          string you want.</para>

          <para>The constructor sets the <literal>closed</literal>,
          <literal>softspace</literal> and <literal>_lineSeparator</literal>
          attributes to their initial values, as well as the
          <literal>node</literal> attribute to <literal>None</literal>.
          Sub-classes should set the <literal>node</literal>,
          <literal>mode</literal> and <literal>offset</literal>
          attributes.</para>

          <para>Version 1 implements the file storage as a
          <literal>UInt8</literal> uni-dimensional
          <literal>EArray</literal>.</para>

          <section>
            <title><literal>FileNode</literal> methods</title>

            <section>
              <title><emphasis
              role="bold">getLineSeparator()</emphasis></title>

              <para>Returns the line separator string.</para>
            </section>

            <section>
              <title><emphasis
              role="bold">setLineSeparator()</emphasis></title>

              <para>Sets the line separator string.</para>
            </section>

            <section>
              <title><emphasis role="bold">getAttrs()</emphasis></title>

              <para>Returns the attribute set of the file node.</para>
            </section>

            <section>
              <title><emphasis role="bold">close()</emphasis></title>

              <para>Flushes the file and closes it. The
              <literal>node</literal> attribute becomes
              <literal>None</literal> and the <literal>attrs</literal>
              property becomes no longer available.</para>
            </section>

            <section>
              <title><emphasis role="bold">next()</emphasis></title>

              <para>Returns the next line of text. Raises
              <literal>StopIteration</literal> when lines are exhausted. See
              <literal>file.next.__doc__</literal> for more
              information.</para>
            </section>

            <section>
              <title><emphasis role="bold">read(size=None)</emphasis></title>

              <para>Reads at most <literal>size</literal> bytes. See
              <literal>file.read.__doc__</literal> for more information</para>
            </section>

            <section>
              <title><emphasis
              role="bold">readline(size=-1)</emphasis></title>

              <para>Reads the next text line. See
              <literal>file.readline.__doc__</literal> for more
              information</para>
            </section>

            <section>
              <title><emphasis
              role="bold">readlines(sizehint=-1)</emphasis></title>

              <para>Reads the text lines. See
              <literal>file.readlines.__doc__</literal> for more
              information.</para>
            </section>

            <section>
              <title><emphasis role="bold">seek(offset,
              whence=0)</emphasis></title>

              <para>Moves to a new file position. See
              <literal>file.seek.__doc__</literal> for more
              information.</para>
            </section>

            <section>
              <title><emphasis role="bold">tell()</emphasis></title>

              <para>Gets the current file position. See
              <literal>file.tell.__doc__</literal> for more
              information.</para>
            </section>

            <section>
              <title><emphasis role="bold">xreadlines()</emphasis></title>

              <para>For backward compatibility. See
              <literal>file.xreadlines.__doc__</literal> for more
              information.</para>
            </section>
          </section>
        </section>

        <section>
          <title>The <literal>ROFileNode</literal> class</title>

          <para>Instances of this class are returned when
          <literal>openNode()</literal> is called in read-only mode
          (<literal>'r'</literal>). This is a descendant of
          <literal>FileNode</literal> class, so it inherits all its methods.
          Moreover, it does not define any other useful method, just some
          protections against users intents to write on file.</para>
        </section>

        <section>
          <title>The <literal>RAFileNode</literal> class</title>

          <para>Instances of this class are returned when either
          <literal>newNode()</literal> is called or when
          <literal>openNode()</literal> is called in append mode
          (<literal>'a+'</literal>). This is a descendant of
          <literal>FileNode</literal> class, so it inherits all its methods.
          It provides additional methods that allow to write on file
          nodes.</para>

          <section>
            <title>flush()</title>

            <para>Flushes the file node. See
            <literal>file.flush.__doc__</literal> for more information.</para>
          </section>

          <section>
            <title>truncate(size=None)</title>

            <para>Truncates the file node to at most <literal>size</literal>
            bytes. Currently, this method only makes sense to grow the file
            node, since data can not be rewritten nor deleted. See
            <literal>file.truncate.__doc__</literal> for more
            information.</para>
          </section>

          <section>
            <title>write(string)</title>

            <para>Writes the string to the file. Writing an empty string does
            nothing, but requires the file to be open. See
            <literal>file.write.__doc__</literal> for more information.</para>
          </section>

          <section>
            <title>writelines(sequence)</title>

            <para>Writes the sequence of strings to the file. See
            <literal>file.writelines.__doc__</literal> for more
            information.</para>
          </section>
        </section>
      </section>
    </chapter>

    <chapter>
      <title>netcdf3 - a PyTables NetCDF3 emulation API (deprecated)</title>

      <warning>
        <para>The tables.netcdf3 module is not actively maintained anymore.
        It is deprecated and will be removed in the future versions.</para>
      </warning>

      <section>
        <title>What is <literal>netcdf3</literal>?</title>

        <para>The netCDF format is a popular format for binary files. It is
        portable between machines and self-describing, i.e. it contains the
        information necessary to interpret its contents. A free library
        provides convenient access to these files (see <biblioref
        linkend="NetCDFRef" />). A very nice python interface to that library
        is available in the <literal>Scientific Python NetCDF</literal> module
        (see <biblioref linkend="scientificpythonRef" />). Although it is
        somewhat less efficient and flexible than HDF5, netCDF is geared for
        storing gridded data and is quite easy to use. It has become a de
        facto standard for gridded data, especially in meteorology and
        oceanography. The next version of netCDF (netCDF 4) will actually be a
        software layer on top of HDF5 (see <biblioref
        linkend="NetCDF4Ref" />). The <literal>tables.netcdf3</literal>
        package does not create HDF5 files that are compatible with netCDF 4
        (although this is a long-term goal).</para>
      </section>

      <section>
        <title>Using the <literal>tables.netcdf3</literal> package</title>

        <para>The package <literal>tables.netcdf3</literal> emulates the
        <literal>Scientific.IO.NetCDF</literal> API using PyTables. It
        presents the data in the form of objects that behave very much like
        arrays. A <literal>tables.netcdf3</literal> file contains any number
        of dimensions and variables, both of which have unique names. Each
        variable has a shape defined by a set of dimensions, and optionally
        attributes whose values can be numbers, number sequences, or strings.
        One dimension of a file can be defined as
        <emphasis>unlimited</emphasis>, meaning that the file can grow along
        that direction. In the sections that follow, a step-by-step tutorial
        shows how to create and modify a <literal>tables.netcdf3</literal>
        file. All of the code snippets presented here are included in
        <literal>examples/netCDF_example.py</literal>. The
        <literal>tables.netcdf3</literal> package is designed to be used as a
        drop-in replacement for <literal>Scientific.IO.NetCDF</literal>, with
        only minor modifications to existing code. The differences between
        <literal>tables.netcdf3</literal> and
        <literal>Scientific.IO.NetCDF</literal> are summarized in the last
        section of this chapter.</para>

        <section>
          <title>Creating/Opening/Closing a <literal>tables.netcdf3</literal>
          file</title>

          <para>To create a <literal>tables.netcdf3</literal> file from
          python, you simply call the <literal>NetCDFFile</literal>
          constructor. This is also the method used to open an existing
          <literal>tables.netcdf3</literal> file. The object returned is an
          instance of the <literal>NetCDFFile</literal> class and all future
          access must be done through this object. If the file is open for
          write access (<literal>'w'</literal> or <literal>'a'</literal>), you
          may write any type of new data including new dimensions, variables
          and attributes. The optional <literal>history</literal> keyword
          argument can be used to set the <literal>history</literal>
          <literal>NetCDFFile</literal> global file attribute. Closing the
          <literal>tables.netcdf3</literal> file is accomplished via the
          <literal>close</literal> method of <literal>NetCDFFile</literal>
          object.</para>

          <para>Here's an example:</para>

          <screen>>>> import tables.netcdf3 as NetCDF
>>> import time
>>> history = 'Created ' + time.ctime(time.time())
>>> file = NetCDF.NetCDFFile('test.h5', 'w', history=history)
>>> file.close()</screen>
        </section>

        <section>
          <title>Dimensions in a <literal>tables.netcdf3</literal>
          file</title>

          <para>NetCDF defines the sizes of all variables in terms of
          dimensions, so before any variables can be created the dimensions
          they use must be created first. A dimension is created using the
          <literal>createDimension</literal> method of the
          <literal>NetCDFFile</literal> object. A Python string is used to set
          the name of the dimension, and an integer value is used to set the
          size. To create an <emphasis>unlimited</emphasis> dimension (a
          dimension that can be appended to), the size value is set to
          <literal>None</literal>.</para>

          <screen>>>> import tables.netcdf3 as NetCDF
>>> file = NetCDF.NetCDFFile('test.h5', 'a')
>>> file.NetCDFFile.createDimension('level', 12)
>>> file.NetCDFFile.createDimension('time', None)
>>> file.NetCDFFile.createDimension('lat', 90)</screen>

          <para>All of the dimension names and their associated sizes are
          stored in a Python dictionary.</para>

          <screen>>>> print file.dimensions
{'lat': 90, 'time': None, 'level': 12}</screen>
        </section>

        <section>
          <title>Variables in a <literal>tables.netcdf3</literal> file</title>

          <para>Most of the data in a <literal>tables.netcdf3</literal> file
          is stored in a netCDF variable (except for global attributes). To
          create a netCDF variable, use the <literal>createVariable</literal>
          method of the <literal>NetCDFFile</literal> object. The
          <literal>createVariable</literal> method has three mandatory
          arguments, the variable name (a Python string), the variable
          datatype described by a single character Numeric typecode string
          which can be one of <literal>f</literal> (Float32),
          <literal>d</literal> (Float64), <literal>i</literal> (Int32),
          <literal>l</literal> (Int32), <literal>s</literal> (Int16),
          <literal>c</literal> (CharType - length 1), <literal>F</literal>
          (Complex32), <literal>D</literal> (Complex64) or
          <literal>1</literal> (Int8), and a tuple containing the variable's
          dimension names (defined previously with
          <literal>createDimension</literal>). The dimensions themselves are
          usually defined as variables, called coordinate variables. The
          <literal>createVariable</literal> method returns an instance of the
          <literal>NetCDFVariable</literal> class whose methods can be used
          later to access and set variable data and attributes.</para>

          <screen>>>> times = file.createVariable('time','d',('time',))
>>> levels = file.createVariable('level','i',('level',))
>>> latitudes = file.createVariable('latitude','f',('lat',))
>>> temp = file.createVariable('temp','f',('time','level','lat',))
>>> pressure = file.createVariable('pressure','i',('level','lat',))</screen>

          <para>All of the variables in the file are stored in a Python
          dictionary, in the same way as the dimensions:</para>

          <screen>>>> print file.variables
{'latitude': &lt;tables.netcdf3.NetCDFVariable instance at 0x244f350&gt;,
'pressure': &lt;tables.netcdf3.NetCDFVariable instance at 0x244f508&gt;,
'level': &lt;tables.netcdf3.NetCDFVariable instance at 0x244f0d0&gt;,
'temp': &lt;tables.netcdf3.NetCDFVariable instance at 0x244f3a0&gt;,
'time': &lt;tables.netcdf3.NetCDFVariable instance at 0x2564c88&gt;}</screen>
        </section>

        <section>
          <title>Attributes in a <literal>tables.netcdf3</literal>
          file</title>

          <para>There are two types of attributes in a
          <literal>tables.netcdf3</literal> file, global (or file) and
          variable. Global attributes provide information about the dataset,
          or file, as a whole. Variable attributes provide information about
          one of the variables in the file. Global attributes are set by
          assigning values to <literal>NetCDFFile</literal> instance
          variables. Variable attributes are set by assigning values to
          <literal>NetCDFVariable</literal> instance variables.</para>

          <para>Attributes can be strings, numbers or sequences. Returning to
          our example,</para>

          <screen>>>> file.description = 'bogus example to illustrate the use of tables.netcdf3'
>>> file.source = 'PyTables Users Guide'
>>> latitudes.units = 'degrees north'
>>> pressure.units = 'hPa'
>>> temp.units = 'K'
>>> times.units = 'days since January 1, 2005'
>>> times.scale_factor = 1</screen>

          <para>The <literal>ncattrs</literal> method of the
          <literal>NetCDFFile</literal> object can be used to retrieve the
          names of all the global attributes. This method is provided as a
          convenience, since using the built-in <literal> dir</literal> Python
          function will return a bunch of private methods and attributes that
          cannot (or should not) be modified by the user. Similarly, the
          <literal>ncattrs</literal> method of a
          <literal>NetCDFVariable</literal> object returns all of the netCDF
          variable attribute names. These functions can be used to easily
          print all of the attributes currently defined, like this</para>

          <screen>>>> for name in file.ncattrs():
>>>     print 'Global attr', name, '=', getattr(file,name)
Global attr description = bogus example to illustrate the use of tables.netcdf3
Global attr history = Created Mon Nov  7 10:30:56 2005
Global attr source = PyTables Users Guide</screen>

          <para>Note that the <literal>ncattrs</literal> function is not part
          of the <literal>Scientific.IO.NetCDF</literal> interface.</para>
        </section>

        <section>
          <title>Writing data to and retrieving data from a
          <literal>tables.netcdf3</literal> variable</title>

          <para>Now that you have a netCDF variable object, how do you put
          data into it? If the variable has no <emphasis>unlimited</emphasis>
          dimension, you just treat it like a Numeric array object and assign
          data to a slice.</para>

          <screen>>>> import numpy
>>> levels[:] = numpy.arange(12)+1
>>> latitudes[:] = numpy.arange(-89,90,2)
>>> for lev in levels[:]:
>>>     pressure[:,:] = 1000.-100.*lev
>>> print 'levels = ',levels[:]
levels =  [ 1  2  3  4  5  6  7  8  9 10 11 12]
>>> print 'latitudes =\n',latitudes[:]
latitudes =
[-89. -87. -85. -83. -81. -79. -77. -75. -73. -71. -69. -67. -65. -63.
-61. -59. -57. -55. -53. -51. -49. -47. -45. -43. -41. -39. -37. -35.
-33. -31. -29. -27. -25. -23. -21. -19. -17. -15. -13. -11.  -9.  -7.
-5.  -3.  -1.   1.   3.   5.   7.   9.  11.  13.  15.  17.  19.  21.
23.  25.  27.  29.  31.  33.  35.  37.  39.  41.  43.  45.  47.  49.
51.  53.  55.  57.  59.  61.  63.  65.  67.  69.  71.  73.  75.  77.
79.  81.  83.  85.  87.  89.]</screen>

          <para>Note that retrieving data from the netCDF variable object
          works just like a Numeric array too. If the netCDF variable has an
          <emphasis>unlimited</emphasis> dimension, and there is not yet an
          entry for the data along that dimension, the
          <literal>append</literal> method must be used.</para>

          <screen>>>> for n in range(10):
>>>     times.append(n)
>>> print 'times = ',times[:]
times =  [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]</screen>

          <para>The data you append must have either the same number of
          dimensions as the <literal>NetCDFVariable</literal>, or one less.
          The shape of the data you append must be the same as the
          <literal>NetCDFVariable</literal> for all of the dimensions except
          the <emphasis>unlimited</emphasis> dimension. The length of the data
          long the <emphasis>unlimited</emphasis> dimension controls how may
          entries along the <emphasis>unlimited</emphasis> dimension are
          appended. If the data you append has one fewer number of dimensions
          than the <literal>NetCDFVariable</literal>, it is assumed that you
          are appending one entry along the <emphasis>unlimited</emphasis>
          dimension. For example, if the <literal>NetCDFVariable</literal> has
          shape <literal>(10,50,100)</literal> (where the dimension length of
          length <literal>10</literal> is the <emphasis>unlimited</emphasis>
          dimension), and you append an array of shape
          <literal>(50,100)</literal>, the <literal>NetCDFVariable</literal>
          will subsequently have a shape of <literal>(11,50,100)</literal>. If
          you append an array with shape <literal>(5,50,100)</literal>, the
          <literal>NetCDFVariable</literal> will have a new shape of
          <literal>(15,50,100)</literal>. Appending an array whose last two
          dimensions do not have a shape <literal>(50,100)</literal> will
          raise an exception. This <literal>append</literal> method does not
          exist in the <literal>Scientific.IO.NetCDF</literal> interface,
          instead entries are appended along the
          <emphasis>unlimited</emphasis> dimension one at a time by assigning
          to a slice. This is the biggest difference between the
          <literal>tables.netcdf3</literal> and
          <literal>Scientific.IO.NetCDF</literal> interfaces.</para>

          <para>Once data has been appended to any variable with an
          <emphasis>unlimited</emphasis> dimension, the
          <literal>sync</literal> method can be used to synchronize the sizes
          of all the other variables with an <emphasis>unlimited</emphasis>
          dimension. This is done by filling in missing values (given by the
          default netCDF <literal>_FillValue</literal>, which is intended to
          indicate that the data was never defined). The
          <literal>sync</literal> method is automatically invoked with a
          <literal>NetCDFFile</literal> object is closed. Once the
          <literal>sync</literal> method has been invoked, the filled-in
          values can be assigned real data with slices.</para>

          <screen>>>> print 'temp.shape before sync = ',temp.shape
temp.shape before sync =  (0, 12, 90)
>>> file.sync()
>>> print 'temp.shape after sync = ',temp.shape
temp.shape after sync =  (10, 12, 90)
>>> from numarray import random_array
>>> for n in range(10):
>>>     temp[n] = 10.*random_array.random(pressure.shape)
>>>     print 'time, min/max temp, temp[n,0,0] = ',\
             times[n],min(temp[n].flat),max(temp[n].flat),temp[n,0,0]
time, min/max temp, temp[n,0,0] = 0.0 0.0122650898993 9.99259281158 6.13053750992
time, min/max temp, temp[n,0,0] = 1.0 0.00115821603686 9.9915933609 6.68516159058
time, min/max temp, temp[n,0,0] = 2.0 0.0152112031356 9.98737239838 3.60537290573
time, min/max temp, temp[n,0,0] = 3.0 0.0112022599205 9.99535560608 6.24249696732
time, min/max temp, temp[n,0,0] = 4.0 0.00519315246493 9.99831295013 0.225010097027
time, min/max temp, temp[n,0,0] = 5.0 0.00978941563517 9.9843454361 4.56814193726
time, min/max temp, temp[n,0,0] = 6.0 0.0159023851156 9.99160385132 6.36837291718
time, min/max temp, temp[n,0,0] = 7.0 0.0019518379122 9.99939727783 1.42762875557
time, min/max temp, temp[n,0,0] = 8.0 0.00390585977584 9.9909954071 2.79601073265
time, min/max temp, temp[n,0,0] = 9.0 0.0106026884168 9.99195957184 8.18835449219</screen>

          <para>Note that appending data along an
          <emphasis>unlimited</emphasis> dimension always increases the length
          of the variable along that dimension. Assigning data to a variable
          with an <emphasis>unlimited</emphasis> dimension with a slice
          operation does not change its shape. Finally, before closing the
          file we can get a summary of its contents simply by printing the
          <literal>NetCDFFile</literal> object. This produces output very
          similar to running 'ncdump -h' on a netCDF file.</para>

          <screen>>>> print file
test.h5 {
dimensions:
  lat = 90 ;
  time = UNLIMITED ; // (10 currently)
  level = 12 ;
variables:
  float latitude('lat',) ;
      latitude:units = 'degrees north' ;
  int pressure('level', 'lat') ;
      pressure:units = 'hPa' ;
  int level('level',) ;
  float temp('time', 'level', 'lat') ;
      temp:units = 'K' ;
  double time('time',) ;
      time:scale_factor = 1 ;
      time:units = 'days since January 1, 2005' ;
// global attributes:
      :description = 'bogus example to illustrate the use of tables.netcdf3' ;
      :history = 'Created Wed Nov  9 12:29:13 2005' ;
      :source = 'PyTables Users Guide' ;
}</screen>
        </section>

        <section>
          <title>Efficient compression of <literal>tables.netcdf3</literal>
          variables</title>

          <para>Data stored in <literal>NetCDFVariable</literal> objects is
          compressed on disk by default. The parameters for the default
          compression are determined from a <literal>Filters</literal> class
          instance (see section <xref linkend="FiltersClassDescr"
          xrefstyle="select: label" />) with <literal>complevel=6,
          complib='zlib' and shuffle=True</literal>. To change the default
          compression, simply pass a <literal>Filters</literal> instance to
          <literal>createVariable</literal> with the
          <literal>filters</literal> keyword. If your data only has a certain
          number of digits of precision (say for example, it is temperature
          data that was measured with a precision of <literal>0.1</literal>
          degrees), you can dramatically improve compression by quantizing (or
          truncating) the data using the
          <literal>least_significant_digit</literal> keyword argument to
          <literal>createVariable</literal>. The <emphasis>least significant
          digit</emphasis> is the power of ten of the smallest decimal place
          in the data that is a reliable value. For example if the data has a
          precision of <literal>0.1</literal>, then setting
          <literal>least_significant_digit=1</literal> will cause data the
          data to be quantized using
          <literal>numpy.around(scale*data)/scale</literal>, where
          <literal>scale = 2**bits</literal>, and bits is determined so that a
          precision of <literal>0.1</literal> is retained (in this case
          <literal>bits=4</literal>).</para>

          <para>In our example, try replacing the line</para>

          <screen>>>> temp = file.createVariable('temp','f',('time','level','lat',))</screen>

          <para>with</para>

          <screen>>>> temp = file.createVariable('temp','f',('time','level','lat',),
                             least_significant_digit=1)</screen>

          <para>and see how much smaller the resulting file is.</para>

          <para>The <literal>least_significant_digit</literal> keyword
          argument is not allowed in <literal>Scientific.IO.NetCDF</literal>,
          since netCDF version 3 does not support compression. The flexible,
          fast and efficient compression available in HDF5 is the main reason
          I wrote the <literal>tables.netcdf3</literal> package - my netCDF
          files were just getting too big.</para>

          <para>The <literal>createVariable</literal> method has one other
          keyword argument not found in
          <literal>Scientific.IO.NetCDF</literal> -
          <literal>expectedsize</literal>. The <literal>expectedsize</literal>
          keyword can be used to set the expected number of entries along the
          <emphasis>unlimited</emphasis> dimension (default 10000). If you
          expect that your data with have an order of magnitude more or less
          than 10000 entries along the <emphasis>unlimited</emphasis>
          dimension, you may consider setting this keyword to improve
          efficiency (see <xref linkend="expectedRowsOptim" xrefstyle="select:
          label" /> for details).</para>
        </section>
      </section>

      <section>
        <title><literal>tables.netcdf3</literal> package reference</title>

        <section>
          <title>Global constants</title>

          <glosslist>
            <?dbfo glosslist-presentation="list" ?>

            <glossentry>
              <glossterm><emphasis
              role="bold">_fillvalue_dict</emphasis></glossterm>

              <glossdef>
                <para>Dictionary whose keys are
                <literal>NetCDFVariable</literal> single character typecodes
                and whose values are the netCDF _FillValue for that
                typecode.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><emphasis
              role="bold">ScientificIONetCDF_imported</emphasis></glossterm>

              <glossdef>
                <para><literal>True</literal> if
                <literal>Scientific.IO.NetCDF</literal> is installed and can
                be imported.</para>
              </glossdef>
            </glossentry>
          </glosslist>
        </section>

        <section>
          <title>The <literal>NetCDFFile</literal> class</title>

          <para><emphasis>NetCDFFile(filename, mode='r',
          history=None)</emphasis></para>

          <para>Opens an existing <literal>tables.netcdf3</literal> file (mode
          = <literal>'r'</literal> or <literal>'a'</literal>) or creates a new
          one (mode = <literal>'w'</literal>). The <literal>history</literal>
          keyword can be used to set the <literal>NetCDFFile.history</literal>
          global attribute (if mode = <literal>'a'</literal> or
          <literal>'w'</literal>).</para>

          <para>A <literal>NetCDFFile</literal> object has two standard
          attributes: <literal>dimensions</literal> and
          <literal>variables</literal>. The values of both are dictionaries,
          mapping dimension names to their associated lengths and variable
          names to variables. All other attributes correspond to global
          attributes defined in a netCDF file. Global file attributes are
          created by assigning to an attribute of the
          <literal>NetCDFFile</literal> object.</para>

          <section>
            <title><literal>NetCDFFile</literal> methods</title>

            <section>
              <title>close()</title>

              <para>Closes the file (after invoking the
              <literal>sync</literal> method).</para>
            </section>

            <section>
              <title>sync()</title>

              <para>Synchronizes the size of variables along the
              <emphasis>unlimited</emphasis> dimension, by filling in data
              with default netCDF _FillValue. Returns the length of the
              <emphasis>unlimited</emphasis> dimension. Invoked automatically
              when the <literal>NetCDFFile</literal> object is closed.</para>
            </section>

            <section>
              <title>ncattrs()</title>

              <para>Returns a list with the names of all currently defined
              netCDF global file attributes.</para>
            </section>

            <section>
              <title>createDimension(name, length)</title>

              <para>Creates a netCDF dimension with a name given by the Python
              string <literal>name</literal> and a size given by the integer
              <literal>size</literal>. If <literal>size = None</literal>, the
              dimension is <emphasis>unlimited</emphasis> (i.e. it can grow
              dynamically). There can be only one
              <emphasis>unlimited</emphasis> dimension in a file.</para>
            </section>

            <section>
              <title>createVariable(name, type, dimensions,
              least_significant_digit= None, expectedsize=10000,
              filters=None)</title>

              <para>Creates a new variable with the given <literal>name, type,
              and dimensions</literal>. The type is a one-letter Numeric
              typecode string which can be one of <literal>f</literal>
              (Float32), <literal>d</literal> (Float64), <literal>i</literal>
              (Int32), <literal>l</literal> (Int32), <literal>s</literal>
              (Int16), <literal>c</literal> (CharType - length 1),
              <literal>F</literal> (Complex32), <literal>D</literal>
              (Complex64) or <literal>1</literal> (Int8); the predefined type
              constants from Numeric can also be used. The
              <literal>F</literal> and <literal>D</literal> types are not
              supported in netCDF or Scientific.IO.NetCDF, if they are used in
              a <literal>tables.netcdf3</literal> file, that file cannot be
              converted to a true netCDF file nor can it be shared over the
              Internet with OPeNDAP. Dimensions must be a tuple containing
              dimension names (strings) that have been defined previously by
              <literal>createDimensions</literal>. The
              <literal>least_significant_digit</literal> is the power of ten
              of the smallest decimal place in the variable's data that is a
              reliable value. If this keyword is specified, the variable's
              data truncated to this precision to improve compression. The
              <literal>expectedsize</literal> keyword can be used to set the
              expected number of entries along the
              <emphasis>unlimited</emphasis> dimension (default 10000). If you
              expect that your data with have an order of magnitude more or
              less than 10000 entries along the <emphasis>unlimited</emphasis>
              dimension, you may consider setting this keyword to improve
              efficiency (see <xref linkend="expectedRowsOptim"
              xrefstyle="select: label" /> for details). The
              <literal>filters</literal> keyword is a PyTables
              <literal>Filters</literal> instance that describes how to store
              the data on disk. The default corresponds to
              <literal>complevel=6</literal>,
              <literal>complib='zlib'</literal>,
              <literal>shuffle=True</literal> and
              <literal>fletcher32=False</literal>.</para>
            </section>

            <section>
              <title>nctoh5(filename, unpackshort=True, filters=None)</title>

              <para>Imports the data in a netCDF version 3 file
              (<literal>filename</literal>) into a
              <literal>NetCDFFile</literal> object using
              <literal>Scientific.IO.NetCDF</literal>
              (<literal>ScientificIONetCDF_imported</literal> must be
              <literal>True</literal>). If
              <literal>unpackshort=True</literal>, data packed as short
              integers (type <literal>s</literal>) in the netCDF file will be
              unpacked to type <literal>f</literal> using the
              <literal>scale_factor</literal> and
              <literal>add_offset</literal> netCDF variable attributes. The
              <literal>filters</literal> keyword can be set to a PyTables
              <literal>Filters</literal> instance to change the default
              parameters used to compress the data in the
              <literal>tables.netcdf3</literal> file. The default corresponds
              to <literal>complevel=6</literal>,
              <literal>complib='zlib'</literal>,
              <literal>shuffle=True</literal> and
              <literal>fletcher32=False</literal>.</para>
            </section>

            <section>
              <title>h5tonc(filename, packshort=False, scale_factor=None,
              add_offset=None)</title>

              <para>Exports the data in a <literal>tables.netcdf3</literal>
              file defined by the <literal>NetCDFFile</literal> instance into
              a netCDF version 3 file using
              <literal>Scientific.IO.NetCDF</literal>
              (<literal>ScientificIONetCDF_imported</literal> must be
              <literal>True</literal>). If
              <literal>packshort=True&gt;</literal> the dictionaries
              <literal>scale_factor</literal> and
              <literal>add_offset</literal> are used to pack data of type
              <literal>f</literal> as short integers (of type
              <literal>s</literal>) in the netCDF file. Since netCDF version 3
              does not provide automatic compression, packing as short
              integers is a commonly used way of saving disk space (see this
              <ulink
              url="http://www.cdc.noaa.gov/cdc/conventions/cdc_netcdf_standard.shtml">page</ulink>
              for more details). The keys of these dictionaries are the
              variable names to pack, the values are the scale_factors and
              offsets to use in the packing. The data are packed so that the
              original Float32 values can be reconstructed by multiplying the
              <literal>scale_factor</literal> and adding
              <literal>add_offset</literal>. The resulting netCDF file will
              have the <literal>scale_factor</literal> and
              <literal>add_offset</literal> variable attributes set
              appropriately.</para>
            </section>
          </section>
        </section>

        <section>
          <title>The <literal>NetCDFVariable</literal> class</title>

          <para>The <literal>NetCDFVariable</literal> constructor is not
          called explicitly, rather an <literal>NetCDFVariable</literal>
          instance is returned by an invocation of
          <literal>NetCDFFile.createVariable</literal>.
          <literal>NetCDFVariable</literal> objects behave like arrays, and
          have the standard attributes of arrays (such as
          <literal>shape</literal>). Data can be assigned or extracted from
          <literal>NetCDFVariable</literal> objects via slices.</para>

          <section>
            <title><literal>NetCDFVariable</literal> methods</title>

            <section>
              <title>typecode()</title>

              <para>Returns a single character typecode describing the type of
              the variable, one of <literal>f</literal> (Float32),
              <literal>d</literal> (Float64), <literal>i</literal> (Int32),
              <literal>l</literal> (Int32), <literal>s</literal> (Int16),
              <literal>c</literal> (CharType - length 1), <literal>F</literal>
              (Complex32), <literal>D</literal> (Complex64) or
              <literal>1</literal> (Int8).</para>
            </section>

            <section>
              <title>append(data)</title>

              <para>Append data to a variable along its
              <emphasis>unlimited</emphasis> dimension. The data you append
              must have either the same number of dimensions as the
              <literal>NetCDFVariable</literal>, or one less. The shape of the
              data you append must be the same as the
              <literal>NetCDFVariable</literal> for all of the dimensions
              except the <emphasis>unlimited</emphasis> dimension. The length
              of the data long the <emphasis>unlimited</emphasis> dimension
              controls how may entries along the
              <emphasis>unlimited</emphasis> dimension are appended. If the
              data you append has one fewer number of dimensions than the
              <literal>NetCDFVariable</literal>, it is assumed that you are
              appending one entry along the <emphasis>unlimited</emphasis>
              dimension. For variables without an
              <emphasis>unlimited</emphasis> dimension, data can simply be
              assigned to a slice without using the <literal>append</literal>
              method.</para>
            </section>

            <section>
              <title>ncattrs()</title>

              <para>Returns a list with all the names of the currently defined
              netCDF variable attributes.</para>
            </section>

            <section>
              <title>assignValue(data)</title>

              <para>Provided for compatibility with
              <literal>Scientific.IO.NetCDF</literal>. Assigns data to the
              variable. If the variable has an <emphasis>unlimited</emphasis>
              dimension, it is equivalent to <literal>append(data)</literal>.
              If the variable has no <emphasis>unlimited</emphasis> dimension,
              it is equivalent to assigning data to the variable with the
              slice <literal>[:]</literal>.</para>
            </section>

            <section>
              <title>getValue()</title>

              <para>Provided for compatibility with
              <literal>Scientific.IO.NetCDF</literal>. Returns all the data in
              the variable. Equivalent to extracting the slice
              <literal>[:]</literal> from the variable.</para>
            </section>
          </section>
        </section>
      </section>

      <section>
        <title>Converting between true netCDF files and
        <literal>tables.netcdf3</literal> files</title>

        <para>If <literal>Scientific.IO.NetCDF</literal> is installed,
        <literal>tables.netcdf3</literal> provides facilities for converting
        between true netCDF version 3 files and
        <literal>tables.netcdf3</literal> hdf5 files via the
        <literal>NetCDFFile.h5tonc()</literal> and
        <literal>NetCDFFile.nctoh5()</literal> class methods. Also, the
        <literal>nctoh5</literal> command-line utility (see <xref
        linkend="nctoh5Descr" xrefstyle="select: label" />) uses the
        <literal>NetCDFFile.nctoh5()</literal> class method.</para>

        <para>As an example, look how to convert a
        <literal>tables.netcdf3</literal> hdf5 file to a true netCDF version 3
        file (named <literal>test.nc</literal>)</para>

        <screen>>>> scale_factor = {'temp': 1.75e-4}
>>> add_offset = {'temp': 5.}
>>> file.h5tonc('test.nc',packshort=True, \
               scale_factor=scale_factor,add_offset=add_offset)
packing temp as short integers ...
>>> file.close()</screen>

        <para>The dictionaries <literal>scale_factor</literal> and
        <literal>add_offset</literal> are used to optionally pack the data as
        short integers in the netCDF file. Since netCDF version 3 does not
        provide automatic compression, packing as short integers is a commonly
        used way of saving disk space (see this <ulink
        url="http://www.cdc.noaa.gov/cdc/conventions/cdc_netcdf_standard.shtml">page</ulink>
        for more details). The keys of these dictionaries are the variable
        names to pack, the values are the scale_factors and offsets to use in
        the packing. The resulting netCDF file will have the
        <literal>scale_factor</literal> and <literal>add_offset</literal>
        variable attributes set appropriately.</para>

        <para>To convert the netCDF file back to a
        <literal>tables.netcdf3</literal> hdf5 file:</para>

        <screen>>>> history = 'Convert from netCDF ' + time.ctime(time.time())
>>> file = NetCDF.NetCDFFile('test2.h5', 'w', history=history)
>>> nobjects, nbytes = file.nctoh5('test.nc',unpackshort=True)
>>> print nobjects,' objects converted from netCDF, totaling',nbytes,'bytes'
5  objects converted from netCDF, totaling 48008 bytes
>>> temp = file.variables['temp']
>>> times = file.variables['time']
>>> print 'temp.shape after h5 --&gt; netCDF --&gt; h5 conversion = ',temp.shape
temp.shape after h5 --&gt; netCDF --&gt; h5 conversion =  (10, 12, 90)
>>> for n in range(10):
>>>     print 'time, min/max temp, temp[n,0,0] = ',\
             times[n],min(temp[n].flat),max(temp[n].flat),temp[n,0,0]
time, min/max temp, temp[n,0,0] = 0.0 0.0123250000179 9.99257469177 6.13049983978
time, min/max temp, temp[n,0,0] = 1.0 0.00130000000354 9.99152469635 6.68507480621
time, min/max temp, temp[n,0,0] = 2.0 0.0153000000864 9.98732471466 3.60542488098
time, min/max temp, temp[n,0,0] = 3.0 0.0112749999389 9.99520015717 6.2423248291
time, min/max temp, temp[n,0,0] = 4.0 0.00532499980181 9.99817466736 0.225124999881
time, min/max temp, temp[n,0,0] = 5.0 0.00987500045449 9.98417472839 4.56827497482
time, min/max temp, temp[n,0,0] = 6.0 0.01600000076 9.99152469635 6.36832523346
time, min/max temp, temp[n,0,0] = 7.0 0.00200000009499 9.99922466278 1.42772495747
time, min/max temp, temp[n,0,0] = 8.0 0.00392499985173 9.9908246994 2.79605007172
time, min/max temp, temp[n,0,0] = 9.0 0.0107500003651 9.99187469482 8.18832492828
>>> file.close()</screen>

        <para>Setting <literal>unpackshort=True</literal> tells
        <literal>nctoh5</literal> to unpack all of the variables which have
        the <literal>scale_factor</literal> and <literal>add_offset</literal>
        attributes back to floating point arrays. Note that
        <literal>tables.netcdf3</literal> files have some features not
        supported in netCDF (such as Complex data types and the ability to
        make any dimension <emphasis>unlimited</emphasis>).
        <literal>tables.netcdf3</literal> files which utilize these features
        cannot be converted to netCDF using
        <literal>NetCDFFile.h5tonc</literal>.</para>
      </section>

      <section>
        <title><literal>tables.netcdf3</literal> file structure</title>

        <para>A <literal>tables.netcdf3</literal> file consists of array
        objects (either <literal> EArrays</literal> or
        <literal>CArrays</literal>) located in the root group of a pytables
        hdf5 file. Each of the array objects must have a
        <literal>dimensions</literal> attribute, consisting of a tuple of
        dimension names (the length of this tuple should be the same as the
        rank of the array object). Any array objects with one of the supported
        datatypes in a pytables file that conforms to this simple structure
        can be read with the <literal>tables.netcdf3</literal> package.</para>
      </section>

      <section>
        <title>Sharing data in <literal>tables.netcdf3</literal> files over
        the Internet with OPeNDAP</title>

        <para><literal>tables.netcdf3</literal> datasets can be shared over
        the Internet with the OPeNDAP protocol (<ulink
        url="http://opendap.org">http://opendap.org</ulink>), via the python
        OPeNDAP module (<ulink
        url="http://opendap.oceanografia.org">http://opendap.oceanografia.org</ulink>).
        A plugin for the python opendap server is included with the pytables
        distribution (<literal>contrib/h5_dap_plugin.py</literal>). Simply
        copy that file into the <literal>plugins</literal> directory of the
        opendap python module source distribution, run <literal>python
        setup.py install</literal>, point the opendap server to the directory
        containing your <literal>tables.netcdf3</literal> files, and away you
        go. Any OPeNDAP aware client (such as Matlab or IDL) will now be able
        to access your data over http as if it were a local disk file. The
        only restriction is that your <literal>tables.netcdf3</literal> files
        must have the extension <literal>.h5</literal> or
        <literal>.hdf5</literal>. Unfortunately,
        <literal>tables.netcdf3</literal> itself cannot act as an OPeNDAP
        client, although there is a client included in the opendap python
        module, and <literal>Scientific.IO.NetCDF</literal> can act as an
        OPeNDAP client if it is linked with the OPeNDAP netCDF client library.
        Either of these python modules can be used to remotely access
        <literal>tables.netcdf3</literal> datasets with OPeNDAP.</para>
      </section>

      <section>
        <title>Differences between the <literal>Scientific.IO.NetCDF</literal>
        API and the <literal>tables.netcdf3</literal> API</title>

        <orderedlist>
          <listitem>
            <para><literal>tables.netcdf3</literal> data is stored in an HDF5
            file instead of a netCDF file.</para>
          </listitem>

          <listitem>
            <para>Although each variable can have only one
            <emphasis>unlimited</emphasis> dimension in a
            <literal>tables.netcdf3</literal> file, it need not be the first
            as in a true NetCDF file. Complex data types <literal>F</literal>
            (Complex32) and <literal>D</literal> (Complex64) are supported in
            <literal>tables.netcdf3</literal>, but are not supported in netCDF
            (or <literal>Scientific.IO.NetCDF</literal>). Files with variables
            that have these datatypes, or an <emphasis>unlimited</emphasis>
            dimension other than the first, cannot be converted to netCDF
            using <literal>h5tonc</literal>.</para>
          </listitem>

          <listitem>
            <para>Variables in a <literal>tables.netcdf3</literal> file are
            compressed on disk by default using HDF5 zlib compression with the
            <emphasis>shuffle</emphasis> filter. If the
            <emphasis>least_significant_digit</emphasis> keyword is used when
            a variable is created with the <literal>createVariable
            method</literal>, data will be truncated (quantized) before being
            written to the file. This can significantly improve compression.
            For example, if <literal>least_significant_digit=1</literal>, data
            will be quantized using
            <literal>numpy.around(scale*data)/scale</literal>, where
            <literal>scale = 2**bits</literal>, and bits is determined so that
            a precision of 0.1 is retained (in this case
            <literal>bits=4</literal>). From <ulink
            url="http://www.cdc.noaa.gov/cdc/conventions/cdc_netcdf_standard.shtml">http://www.cdc.noaa.gov/cdc/conventions/cdc_netcdf_standard.shtml</ulink>:
            <quote>least_significant_digit -- power of ten of the smallest
            decimal place in unpacked data that is a reliable value.</quote>
            Automatic data compression is not available in netCDF version 3,
            and hence is not available in the
            <literal>Scientific.IO.NetCDF</literal> module.</para>
          </listitem>

          <listitem>
            <para>In <literal>tables.netcdf3</literal>, data must be appended
            to a variable with an <emphasis>unlimited</emphasis> dimension
            using the <literal>append</literal> method of the
            <literal>netCDF</literal> variable object. In
            <literal>Scientific.IO.NetCDF</literal>, data can be added along
            an <emphasis>unlimited</emphasis> dimension by assigning it to a
            slice (there is no append method). The <literal>sync</literal>
            method of a <literal>tables.netcdf3 NetCDFVariable</literal>
            object synchronizes the size of all variables with an
            <emphasis>unlimited</emphasis> dimension by filling in data using
            the default netCDF <literal>_FillValue</literal>. The
            <literal>sync</literal> method is automatically invoked with a
            <literal>NetCDFFile</literal> object is closed. In
            <literal>Scientific.IO.NetCDF</literal>, the
            <literal>sync()</literal> method flushes the data to disk.</para>
          </listitem>

          <listitem>
            <para>The <literal>tables.netcdf3 createVariable()</literal>
            method has three extra optional keyword arguments not found in the
            <literal>Scientific.IO.NetCDF</literal> interface,
            <emphasis>least_significant_digit</emphasis> (see item (2) above),
            <emphasis>expectedsize</emphasis> and
            <emphasis>filters</emphasis>. The
            <emphasis>expectedsize</emphasis> keyword applies only to
            variables with an <emphasis>unlimited</emphasis> dimension, and is
            an estimate of the number of entries that will be added along that
            dimension (default 1000). This estimate is used to optimize HDF5
            file access and memory usage. The <emphasis>filters</emphasis>
            keyword is a PyTables filters instance that describes how to store
            the data on disk. The default corresponds to
            <literal>complevel=6</literal>, <literal>complib='zlib'</literal>,
            <literal>shuffle=True</literal> and
            <literal>fletcher32=False</literal>.</para>
          </listitem>

          <listitem>
            <para><literal>tables.netcdf3</literal> data can be saved to a
            true netCDF file using the <literal>NetCDFFile</literal> class
            method <literal>h5tonc</literal> (if
            <literal>Scientific.IO.NetCDF</literal> is installed). The
            <emphasis>unlimited</emphasis> dimension must be the first (for
            all variables in the file) in order to use the
            <literal>h5tonc</literal> method. Data can also be imported from a
            true netCDF file and saved in an HDF5
            <literal>tables.netcdf3</literal> file using the
            <literal>nctoh5</literal> class method.</para>
          </listitem>

          <listitem>
            <para>In <literal>tables.netcdf3</literal> a list of attributes
            corresponding to global netCDF attributes defined in the file can
            be obtained with the <literal>NetCDFFile ncattrs </literal>method.
            Similarly, netCDF variable attributes can be obtained with the
            <literal>NetCDFVariable</literal> <literal>ncattrs</literal>
            method. These functions are not available in the
            <literal>Scientific.IO.NetCDF</literal> API.</para>
          </listitem>

          <listitem>
            <para>You should not define <literal>tables.netcdf3</literal>
            global or variable attributes that start with
            <literal>_NetCDF_</literal>. Those names are reserved for internal
            use.</para>
          </listitem>

          <listitem>
            <para>Output similar to 'ncdump -h' can be obtained by simply
            printing a <literal>tables.netcdf3</literal>
            <literal>NetCDFFile</literal> instance.</para>
          </listitem>
        </orderedlist>
      </section>
    </chapter>
  </part>

  <part label="III">
    <title>Appendixes</title>

    <appendix id="datatypesSupported">
      <title>Supported data types in PyTables</title>

      <para>All PyTables datasets can handle the complete set of data types
      supported by the NumPy (see <biblioref linkend="NumPy" />),
      <literal>numarray</literal> (see <biblioref linkend="Numarray" />) and
      Numeric (see <biblioref linkend="Numeric" />) packages in Python. The
      data types for table fields can be set via instances of the
      <literal>Col</literal> class and its descendants (see <xref
      linkend="ColClassDescr" xrefstyle="select: label" />), while the data
      type of array elements can be set through the use of the
      <literal>Atom</literal> class and its descendants (see <xref
      linkend="AtomClassDescr" xrefstyle="select: label" />).</para>

      <warning>
        <para>The use of <literal>numarray</literal> and
        <literal>Numeric</literal> in PyTables is now deprecated.
        Support for these packages will be removed in future versions.
        </para>
      </warning>

      <para>PyTables uses ordinary strings to represent its
      <emphasis>types</emphasis>, with most of them matching the names of
      NumPy scalar types. Usually, a PyTables type consists of two parts: a
      <emphasis>kind</emphasis> and a <emphasis>precision</emphasis> in bits.
      The precision may be omitted in types with just one supported precision
      (like <literal>bool</literal>) or with a non-fixed size (like
      <literal>string</literal>).</para>

      <para>There are eight kinds of types supported by PyTables:
      <itemizedlist>
          <listitem>
            <para><literal>bool</literal>: Boolean (true/false) types.
            Supported precisions: 8 (default) bits.</para>
          </listitem>

          <listitem>
            <para><literal>int</literal>: Signed integer types. Supported
            precisions: 8, 16, 32 (default) and 64 bits.</para>
          </listitem>

          <listitem>
            <para><literal>uint</literal>: Unsigned integer types. Supported
            precisions: 8, 16, 32 (default) and 64 bits.</para>
          </listitem>

          <listitem>
            <para><literal>float</literal>: Floating point types. Supported
            precisions: 32 and 64 (default) bits.</para>
          </listitem>

          <listitem>
            <para><literal>complex</literal>: Complex number types. Supported
            precisions: 64 (32+32) and 128 (64+64, default) bits.</para>
          </listitem>

          <listitem>
            <para><literal>string</literal>: Raw string types. Supported
            precisions: 8-bit positive multiples.</para>
          </listitem>

          <listitem>
            <para><literal>time</literal>: Data/time types. Supported
            precisions: 32 and 64 (default) bits.</para>
          </listitem>

          <listitem>
            <para><literal>enum</literal>: Enumerated types. Precision depends
            on base type.</para>
          </listitem>
        </itemizedlist></para>

      <para>The <literal>time</literal> and <literal>enum</literal> kinds are
      a little bit special, since they represent HDF5 types which have no
      direct Python counterpart, though atoms of these kinds have a
      more-or-less equivalent NumPy data type.</para>

      <para>There are two types of <literal>time</literal>: 4-byte signed
      integer (<literal>time32</literal>) and 8-byte double precision floating
      point (<literal>time64</literal>). Both of them reflect the number of
      seconds since the Unix epoch, i.e. Jan 1 00:00:00 UTC 1970. They are
      stored in memory as NumPy's <literal>int32</literal> and
      <literal>float64</literal>, respectively, and in the HDF5 file using the
      <literal>H5T_TIME</literal> class. Integer times are stored on disk as
      such, while floating point times are split into two signed integer
      values representing seconds and microseconds (beware: smaller decimals
      will be lost!).</para>

      <para>PyTables also supports HDF5 <literal>H5T_ENUM</literal>
      <emphasis>enumerations</emphasis> (restricted sets of unique name and
      unique value pairs). The NumPy representation of an enumerated value (an
      <literal>Enum</literal>, see <xref linkend="EnumClassDescr"
      xrefstyle="select: label" />) depends on the concrete <emphasis>base
      type</emphasis> used to store the enumeration in the HDF5
      file. Currently, only scalar integer values (both signed and unsigned)
      are supported in enumerations. This restriction may be lifted when HDF5
      supports other kinds on enumerated values.</para>

      <para>Here you have a quick reference to the complete set of supported
      data types:</para>

      <table align="center" id="datatypesSupportedTable">
        <title>Data types supported for array elements and tables columns in
        PyTables.</title>

        <tgroup cols="5">
          <colspec align="left" colname="c1" />

          <colspec align="left" colname="c2" />

          <colspec align="left" colname="c3" />

          <colspec align="center" colname="c4" />

          <colspec align="left" colname="c5" />

          <thead>
            <row>
              <entry align="left">Type Code</entry>

              <entry align="left">Description</entry>

              <entry align="left">C Type</entry>

              <entry align="center">Size (in bytes)</entry>

              <entry align="left">Python Counterpart</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry align="left">bool</entry>

              <entry align="left">boolean</entry>

              <entry align="left">unsigned char</entry>

              <entry align="center">1</entry>

              <entry align="left">bool</entry>
            </row>

            <row>
              <entry align="left">int8</entry>

              <entry align="left">8-bit integer</entry>

              <entry align="left">signed char</entry>

              <entry align="center">1</entry>

              <entry align="left">int</entry>
            </row>

            <row>
              <entry align="left">uint8</entry>

              <entry align="left">8-bit unsigned integer</entry>

              <entry align="left">unsigned char</entry>

              <entry align="center">1</entry>

              <entry align="left">int</entry>
            </row>

            <row>
              <entry align="left">int16</entry>

              <entry align="left">16-bit integer</entry>

              <entry align="left">short</entry>

              <entry align="center">2</entry>

              <entry align="left">int</entry>
            </row>

            <row>
              <entry align="left">uint16</entry>

              <entry align="left">16-bit unsigned integer</entry>

              <entry align="left">unsigned short</entry>

              <entry align="center">2</entry>

              <entry align="left">int</entry>
            </row>

            <row>
              <entry align="left">int32</entry>

              <entry align="left">integer</entry>

              <entry align="left">int</entry>

              <entry align="center">4</entry>

              <entry align="left">int</entry>
            </row>

            <row>
              <entry align="left">uint32</entry>

              <entry align="left">unsigned integer</entry>

              <entry align="left">unsigned int</entry>

              <entry align="center">4</entry>

              <entry align="left">long</entry>
            </row>

            <row>
              <entry align="left">int64</entry>

              <entry align="left">64-bit integer</entry>

              <entry align="left">long long</entry>

              <entry align="center">8</entry>

              <entry align="left">long</entry>
            </row>

            <row>
              <entry align="left">uint64</entry>

              <entry align="left">unsigned 64-bit integer</entry>

              <entry align="left">unsigned long long</entry>

              <entry align="center">8</entry>

              <entry align="left">long</entry>
            </row>

            <row>
              <entry align="left">float32</entry>

              <entry align="left">single-precision float</entry>

              <entry align="left">float</entry>

              <entry align="center">4</entry>

              <entry align="left">float</entry>
            </row>

            <row>
              <entry align="left">float64</entry>

              <entry align="left">double-precision float</entry>

              <entry align="left">double</entry>

              <entry align="center">8</entry>

              <entry align="left">float</entry>
            </row>

            <row>
              <entry align="left">complex64</entry>

              <entry align="left">single-precision complex</entry>

              <entry align="left">struct {float r, i;}</entry>

              <entry align="center">8</entry>

              <entry align="left">complex</entry>
            </row>

            <row>
              <entry align="left">complex128</entry>

              <entry align="left">double-precision complex</entry>

              <entry align="left">struct {double r, i;}</entry>

              <entry align="center">16</entry>

              <entry align="left">complex</entry>
            </row>

            <row>
              <entry align="left">string</entry>

              <entry align="left">arbitrary length string</entry>

              <entry align="left">char[]</entry>

              <entry align="center">*</entry>

              <entry align="left">str</entry>
            </row>

            <row>
              <entry align="left">time32</entry>

              <entry align="left">integer time</entry>

              <entry align="left">POSIX's time_t</entry>

              <entry align="center">4</entry>

              <entry align="left">int</entry>
            </row>

            <row>
              <entry align="left">time64</entry>

              <entry align="left">floating point time</entry>

              <entry align="left">POSIX's struct timeval</entry>

              <entry align="center">8</entry>

              <entry align="left">float</entry>
            </row>

            <row>
              <entry align="left">enum</entry>

              <entry align="left">enumerated value</entry>

              <entry align="left">enum</entry>

              <entry align="center">-</entry>

              <entry align="left">-</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </appendix>

    <appendix id="conditionSyntax">
      <title>Condition syntax</title>

      <para>Conditions in PyTables are used in methods related with in-kernel
      and indexed searches such as <literal>Table.where()</literal> (see
      <xref linkend="Table.where" />) or <literal>Table.readWhere()</literal>
      (see <xref linkend="Table.readWhere" />).  They are interpreted using
      Numexpr, a powerful package for achieving C-speed computation of array
      operations (see <biblioref linkend="Numexpr" />).</para>

      <para>A condition on a table is just a <emphasis>string</emphasis>
      containing a Python expression involving <emphasis>at least one
      column</emphasis>, and maybe some constants and external variables, all
      combined with algebraic operators and functions. The result of a valid
      condition is always a <emphasis>boolean array</emphasis> of the same
      length as the table, where the <emphasis>i</emphasis>-th element is true
      if the value of the expression on the <emphasis>i</emphasis>-th row of
      the table evaluates to true <footnote>
          <para>That is the reason why multidimensional fields in a table are
          not supported in conditions, since the truth value of each resulting
          multidimensional boolean value is not obvious.</para>
        </footnote>. Usually, a method using a condition will only consider
      the rows where the boolean result is true.</para>

      <para>For instance, the condition <literal>'sqrt(x*x + y*y) &lt;
      1'</literal> applied on a table with <literal>x</literal> and
      <literal>y</literal> columns consisting of floating point numbers
      results in a boolean array where the <emphasis>i</emphasis>-th element
      is true if (unsurprisingly) the value of the square root of the sum of
      squares of <literal>x</literal> and <literal>y</literal> is less than 1.
      The <literal>sqrt()</literal> function works element-wise, the 1
      constant is adequately broadcast to an array of ones of the length of
      the table for evaluation, and the <emphasis>less than</emphasis>
      operator makes the result a valid boolean array. A condition like
      <literal>'mycolumn'</literal> alone will not usually be valid, unless
      <literal>mycolumn</literal> is itself a column of scalar, boolean
      values.</para>

      <para>In the previous conditions, <literal>mycolumn</literal>,
      <literal>x</literal> and <literal>y</literal> are examples of
      <emphasis>variables</emphasis> which are associated with columns.
      Methods supporting conditions do usually provide their own ways of
      binding variable names to columns and other values. You can read the
      documentation of <literal>Table.where()</literal> (see <xref
      linkend="Table.where" />) for more information on that. Also, please
      note that the names <literal>None</literal>, <literal>True</literal> and
      <literal>False</literal>, besides the names of functions (see below)
      <emphasis>can not be overridden</emphasis>, but you can always define
      other new names for the objects you intend to use.</para>

      <para>Values in a condition may have the following types:</para>

      <itemizedlist>
        <listitem>
          <para>8-bit boolean (<literal>bool</literal>).</para>
        </listitem>

        <listitem>
          <para>32-bit signed integer (<literal>int</literal>).</para>
        </listitem>

        <listitem>
          <para>64-bit signed integer (<literal>long</literal>).</para>
        </listitem>

        <listitem>
          <para>32-bit, single-precision floating point number
            (<literal>float</literal> or <literal>float32</literal>).</para>
        </listitem>

        <listitem>
          <para>64-bit, double-precision floating point number
            (<literal>double</literal> or <literal>float64</literal>).</para>
        </listitem>

        <listitem>
          <para>2x64-bit, double-precision complex number
            (<literal>complex</literal>).</para>
        </listitem>

        <listitem>
          <para>Raw string of bytes (<literal>str</literal>).</para>
        </listitem>
      </itemizedlist>

      <para>Nevertheless, if the type passed is not among the above ones, it
      will be silently upcasted, so you don't need to worry too much about
      passing supported types: just pass whatever type you want and the
      interpreter will take care of it.</para>

      <para>However, the types in PyTables conditions are somewhat stricter
      than those of Python. For instance, the <emphasis>only</emphasis> valid
      constants for booleans are <literal>True</literal> and
      <literal>False</literal>, and they are <emphasis>never</emphasis>
      automatically cast to integers. The type strengthening also affects the
      availability of operators and functions. Beyond that, the usual type
      inference rules apply.</para>

      <para>Conditions support the set of operators listed below:
      <itemizedlist>
          <listitem>
            <para>Logical operators: &amp;, |, ~.</para>
          </listitem>

          <listitem>
            <para>Comparison operators: &lt;, &lt;=, ==, !=, &gt;=, &gt;.</para>
          </listitem>

          <listitem>
            <para>Unary arithmetic operators: -.</para>
          </listitem>

          <listitem>
            <para>Binary arithmetic operators: +, -, *, /, **, %.</para>
          </listitem>
        </itemizedlist> Types do not support all operators. Boolean values
      only support logical and strict (in)equality comparison operators, while
      strings only support comparisons, numbers do not work with logical
      operators, and complex comparisons can only check for strict
      (in)equality. Unsupported operations (including invalid castings) raise
      <literal>NotImplementedError</literal> exceptions.</para>

      <para>You may have noticed the special meaning of the usually bitwise
      operators <literal>&amp;</literal>, <literal>|</literal> and
      <literal>~</literal>. Because of the way Python handles the
      short-circuiting of logical operators and the truth values of their
      operands, conditions must use the bitwise operator equivalents instead.
      This is not difficult to remember, but you must be careful because
      bitwise operators have a <emphasis>higher precedence</emphasis> than
      logical operators. For instance, <literal>'a and b == c'</literal>
      (<emphasis><literal>a</literal> is true AND <literal>b</literal> is
      equal to <literal>c</literal></emphasis>) is <emphasis>not</emphasis>
      equivalent to <literal>'a &amp; b == c'</literal>
      (<emphasis><literal>a</literal> AND <literal>b</literal> is equal to
      <literal>c</literal>)</emphasis>. The safest way to avoid confusions is
      to <emphasis>use parentheses</emphasis> around logical operators, like
      this: <literal>'a &amp; (b == c)'</literal>. Another effect of
      short-circuiting is that expressions like <literal>'0 &lt; x &lt;
      1'</literal> will <emphasis>not</emphasis> work as expected; you should
      use <literal>'(0 &lt; x) &amp; (x &lt; 1)'</literal> <footnote>
          <para>All of this may be solved if Python supported overloadable
          boolean operators (see PEP 335) or some kind of non-shortcircuiting
          boolean operators (like C's <literal>&amp;&amp;</literal>,
          <literal>||</literal> and <literal>!</literal>).</para>
        </footnote></para>

      <para>You can also use the following functions in conditions:
      <itemizedlist>
          <listitem>
            <para><literal>where(bool, number1, number2): number</literal> —
            <literal>number1</literal> if the <literal>bool</literal>
            condition is true, <literal>number2</literal> otherwise.</para>
          </listitem>

          <listitem>
            <para><literal>{sin,cos,tan}(float|complex):
            float|complex</literal> — trigonometric sine, cosine or
            tangent.</para>
          </listitem>

          <listitem>
            <para><literal>{arcsin,arccos,arctan}(float|complex):
            float|complex</literal> — trigonometric inverse sine, cosine or
            tangent.</para>
          </listitem>

          <listitem>
            <para><literal>arctan2(float1, float2): float</literal> —
            trigonometric inverse tangent of
            <literal>float1/float2</literal>.</para>
          </listitem>

          <listitem>
            <para><literal>{sinh,cosh,tanh}(float|complex):
            float|complex</literal> — hyperbolic sine, cosine or
            tangent.</para>
          </listitem>

          <listitem>
            <para><literal>{arcsinh,arccosh,arctanh}(float|complex):
            float|complex</literal> — hyperbolic inverse sine, cosine or
            tangent.</para>
          </listitem>

          <listitem>
            <para><literal>{log,log10,log1p}(float|complex):
            float|complex</literal> — natural, base-10
            and <literal>log(1+x)</literal> logarithms.</para>
          </listitem>

          <listitem>
            <para><literal>{exp,expm1}(float|complex):
            float|complex</literal> — exponential and exponential minus
            one.</para>
          </listitem>

          <listitem>
            <para><literal>sqrt(float|complex): float|complex</literal> —
            square root.</para>
          </listitem>

          <listitem>
            <para><literal>{abs}(float|complex): float|complex</literal> —
            absolute value.</para>
          </listitem>

          <listitem>
            <para><literal>{real,imag}(complex): float</literal> — real or
            imaginary part of complex.</para>
          </listitem>

          <listitem>
            <para><literal>complex(float, float): complex</literal> — complex
            from real and imaginary parts.</para>
          </listitem>
      </itemizedlist></para>
    </appendix>

    <appendix id="parametersFiles">
      <title>PyTables' parameter files</title>

      <para>PyTables issues warnings when certain limits are exceeded.  Those
      limits are not intrinsic limitations of the underlying software, but
      rather are proactive measures to avoid large resource consumptions.  The
      default limits should be enough for most of cases, and users should try
      to respect them.  However, in some situations, it can be convenient to
      increase (or decrease) these limits.</para>

      <para>Also, and in order to get maximum performance, PyTables implements
      a series of sophisticated features, like I/O buffers or different kind
      of caches (for nodes, chunks and other internal metadata).  These
      features comes with a default set of parameters that ensures a decent
      performance in most of situations.  But, as there is always a need for
      every case, it is handy to have the possibility to fine-tune some of
      these parameters.</para>

      <para>Because of these reasons, PyTables implements a couple of ways to
      change the values of these parameters.  All
      the <emphasis>tunable</emphasis> parameters live in
      the <literal>tables/parameters.py</literal>.  The user can choose to
      change them in the parameter files themselves for a global and
      persistent change.  Moreover, if he wants a finer control, he can pass
      any of these parameters directly to the <literal>openFile()</literal>
      function (see <xref linkend="openFileDescr"/>), and the new parameters
      will only take effect in the corresponding file (the defaults will
      continue to be in the parameter files).</para>

      <para>A description of all of the tunable parameters follows.  As the
      defaults stated here may change from release to release, please check
      with your actual parameter files so as to know your actual default
      values.</para>

      <warning><para>Changing the next parameters may have a very bad effect
      in the resource consumption and performance of your PyTables scripts.
      Please be careful when touching these!</para></warning>

      <section>
        <title>Tunable parameters in <literal>parameters.py</literal>.
        </title>

        <section>
          <title>Recommended maximum values</title>

          <glosslist>

            <glossentry>
              <glossterm><literal>MAX_COLUMNS</literal></glossterm>
              <glossdef>
                <para>Maximum number of columns in <literal>Table</literal>
                objects before a <literal>PerformanceWarning</literal> is
                issued.  This limit is somewhat arbitrary and can be
                increased.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>MAX_NODE_ATTRS</literal></glossterm>
              <glossdef>
                <para>Maximum allowed number of attributes in a node</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>MAX_GROUP_WIDTH</literal></glossterm>
              <glossdef>
                <para>Maximum depth in object tree allowed.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>MAX_UNDO_PATH_LENGTH</literal></glossterm>
              <glossdef>
                <para>Maximum length of paths allowed in undo/redo
                operations.</para>
              </glossdef>
            </glossentry>

          </glosslist>
        </section>

        <section>
          <title>Cache limits</title>

          <glosslist>

            <glossentry>
              <glossterm><literal>CHUNK_CACHE_NELMTS</literal></glossterm>
              <glossdef>
                <para>Number of elements for HDF5 chunk cache.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>CHUNK_CACHE_PREEMPT</literal></glossterm>
              <glossdef>
                <para>Chunk preemption policy.  This value should be between 0
                  and 1 inclusive and indicates how much chunks that have been
                  fully read are favored for preemption. A value of zero means
                  fully read chunks are treated no differently than other
                  chunks (the preemption is strictly LRU) while a value of one
                  means fully read chunks are always preempted before other
                  chunks.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>CHUNK_CACHE_SIZE</literal></glossterm>
              <glossdef>
                <para>Size (in bytes) for HDF5 chunk cache.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>COND_CACHE_SLOTS</literal></glossterm>
              <glossdef>
                <para>Maximum number of conditions for table queries to be
                  kept in memory.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>METADATA_CACHE_SIZE</literal></glossterm>
              <glossdef>
                <para>Size (in bytes) of the HDF5 metadata cache.  This only
                  takes effect if using HDF5 1.8.x series.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>NODE_CACHE_SLOTS</literal></glossterm>
              <glossdef>
                <para>Maximum number of unreferenced nodes to be kept in
                  memory.</para>

                <para>If positive, this is the number
                  of <emphasis>unreferenced</emphasis> nodes to be kept in the
                  metadata cache.  Least recently used nodes are unloaded from
                  memory when this number of loaded nodes is reached. To load
                  a node again, simply access it as usual. Nodes referenced by
                  user variables are not taken into account nor
                  unloaded.</para>

                <para>Negative value means that all the touched nodes will be
                  kept in an internal dictionary.  This is the faster way to
                  load/retrieve nodes.  However, and in order to avoid a large
                  memory consumption, the user will be warned when the number
                  of loaded nodes will reach
                  the <literal>-NODE_CACHE_SLOTS</literal> value.</para>

                <para>Finally, a value of zero means that any cache mechanism
                  is disabled.</para>
              </glossdef>
            </glossentry>

          </glosslist>
        </section>

        <section>
          <title>Parameters for the different internal caches</title>

          <glosslist>
            <glossentry>
              <glossterm><literal>BOUNDS_MAX_SIZE</literal></glossterm>
              <glossdef>
                <para>The maximum size for bounds values cached during index
                lookups.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>BOUNDS_MAX_SLOTS</literal></glossterm>
              <glossdef>
                <para>The maximum number of slots for
                the <literal>BOUNDS</literal> cache.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>ITERSEQ_MAX_ELEMENTS</literal></glossterm>
              <glossdef>
                <para>The maximum number of iterator elements cached in data
                lookups.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>ITERSEQ_MAX_SIZE</literal></glossterm>
              <glossdef>
                <para>The maximum space that will
                take <literal>ITERSEQ</literal> cache (in bytes).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>ITERSEQ_MAX_SLOTS</literal></glossterm>
              <glossdef>
                <para>The maximum number of slots in
                <literal>ITERSEQ</literal> cache.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>LIMBOUNDS_MAX_SIZE</literal></glossterm>
              <glossdef>
                <para>The maximum size for the query limits (for example,
                <literal>(lim1, lim2)</literal> in conditions like
                <literal>lim1 &lt;= col &lt; lim2</literal>) cached during
                index lookups (in bytes).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>LIMBOUNDS_MAX_SLOTS</literal></glossterm>
              <glossdef>
                <para>The maximum number of slots for
                <literal>LIMBOUNDS</literal> cache.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>TABLE_MAX_SIZE</literal></glossterm>
              <glossdef>
                <para>The maximum size for table chunks cached during index
                queries.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>SORTED_MAX_SIZE</literal></glossterm>
              <glossdef>
                <para>The maximum size for sorted values cached during index
                lookups.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>SORTEDLR_MAX_SIZE</literal></glossterm>
              <glossdef>
                <para>The maximum size for chunks in last row cached in index
                lookups (in bytes).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>SORTEDLR_MAX_SLOTS</literal></glossterm>
              <glossdef>
                <para>The maximum number of chunks
                for <literal>SORTEDLR</literal> cache.</para>
              </glossdef>
            </glossentry>

          </glosslist>
        </section>

        <section>
          <title>Parameters for general cache behaviour</title>

          <warning>
            <para>The next parameters will not take any effect if passed to
            the <literal>openFile()</literal> function, so they can only be
            changed in a <emphasis>global</emphasis> way.  You can change
            them in the file, but this is strongly discouraged unless you know
            well what you are doing.</para>
          </warning>

          <glosslist>

            <glossentry>
              <glossterm><literal>DISABLE_EVERY_CYCLES</literal></glossterm>
              <glossdef>
                <para>The number of cycles in which a cache will be forced to
                be disabled if the hit ratio is lower than the
                <literal>LOWEST_HIT_RATIO</literal> (see below).  This value
                should provide time enough to check whether the cache is being
                efficient or not.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>ENABLE_EVERY_CYCLES</literal></glossterm>
              <glossdef>
                <para>The number of cycles in which a cache will be forced to
                be (re-)enabled, irregardless of the hit ratio. This will
                provide a chance for checking if we are in a better scenario
                for doing caching again.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>LOWEST_HIT_RATIO</literal></glossterm>
              <glossdef>
                <para>The minimum acceptable hit ratio for a cache to avoid
                disabling (and freeing) it.</para>
              </glossdef>
            </glossentry>

          </glosslist>

        </section>

        <section>
          <title>Parameters for the I/O buffer in <literal>Leaf</literal>
            objects</title>

          <glosslist>

            <glossentry>
              <glossterm><literal>IO_BUFFER_SIZE</literal></glossterm>
              <glossdef>
                <para>The PyTables internal buffer size for I/O purposes.
                  Should not exceed the amount of highest level cache size in
                  your CPU.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>BUFFER_TIMES</literal></glossterm>
              <glossdef>
                <para>The maximum buffersize/rowsize ratio before issuing a
                  <literal>PerformanceWarning</literal>.</para>
              </glossdef>
            </glossentry>

          </glosslist>
        </section>

        <section>
          <title>Miscellaneous</title>

          <glosslist>

            <glossentry>
              <glossterm><literal>EXPECTED_ROWS_EARRAY</literal></glossterm>
              <glossdef>
                <para>Default expected number of rows
                for <literal>EArray</literal> objects.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>EXPECTED_ROWS_TABLE</literal></glossterm>
              <glossdef>
                <para>Default expected number of rows
                for <literal>Table</literal> objects.</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>PYTABLES_SYS_ATTRS</literal></glossterm>
              <glossdef>
                <para>Set this to <literal>False</literal> if you don't want
                  to create PyTables system attributes in datasets.  Also, if
                  set to <literal>False</literal> the possible existing system
                  attributes are not considered for guessing the class of the
                  node during its loading from disk (this work is delegated to
                  the PyTables' class discoverer function for general HDF5
                  files).</para>
              </glossdef>
            </glossentry>

            <glossentry>
              <glossterm><literal>MAX_THREADS</literal></glossterm>
              <glossdef>
                <para>The maximum number of threads that PyTables should use
                  internally (mainly in Blosc and Numexpr currently).  If
                  <literal>None</literal>, it is automatically set to the
                  number of cores in your machine. In general, it is a good
                  idea to set this to the number of cores in your machine or,
                  when your machine has many of them (e.g. > 4), perhaps one
                  less than this.</para>
              </glossdef>
            </glossentry>

          </glosslist>
        </section>
      </section>

    </appendix>

    <appendix id="NestedRecArrayClassDescr">
      <title>Using nested record arrays (deprecated)</title>

      <section>
        <title>Introduction</title>

        <para>Nested record arrays are a generalization of the record array
        concept as it appears in the <literal>numarray</literal>
        package. Basically, a nested record array is a record array that
        supports nested datatypes. It means that columns can contain not only
        regular datatypes but also nested datatypes.</para>

        <warning>
          <para>PyTables nested record arrays were implemented to overcome a
          limitation of the record arrays in the <literal>numarray</literal>
          package.  However, as this functionality is already present in
          <literal>NumPy</literal>, current users should not need the package
          <literal>tables.nra</literal> anymore and it will be deprecated
          soon.</para>
        </warning>

        <para>Each nested record array is a <literal>NestedRecArray</literal>
        object in the <literal>tables.nra</literal> package. Nested record
        arrays are intended to be as compatible as possible with ordinary
        record arrays (in fact the <literal>NestedRecArray</literal> class
        inherits from <literal>RecArray</literal>). As a consequence, the user
        can deal with nested record arrays nearly in the same way that he does
        with ordinary record arrays.</para>

        <para>The easiest way to create a nested record array is to use the
        <literal>array()</literal> function in the
        <literal>tables.nra</literal> package. The only difference between
        this function and its non-nested capable analogous is that now, we
        <emphasis>must</emphasis> provide an structure for the buffer being
        stored. For instance:</para>

        <screen>>>> from tables.nra import array
>>> nra1 = array(
...     [(1, (0.5, 1.0), ('a1', 1j)), (2, (0, 0), ('a2', 1+.1j))],
...     formats=['Int64', '(2,)Float32', ['a2', 'Complex64']])</screen>

        <para>will create a two rows nested record array with two regular
        fields (columns), and one nested field with two sub-fields.</para>

        <para>The field structure of the nested record array is specified by
        the keyword argument <literal>formats</literal>. This argument only
        supports sequences of strings and other sequences. Each string defines
        the shape and type of a non-nested field. Each sequence contains the
        formats of the sub-fields of a nested field. Optionally, we can also
        pass an additional <literal>names</literal> keyword argument
        containing the names of fields and sub-fields:</para>

        <screen>>>> nra2 = array(
...     [(1, (0.5, 1.0), ('a1', 1j)), (2, (0, 0), ('a2', 1+.1j))],
...     names=['id', 'pos', ('info', ['name', 'value'])],
...     formats=['Int64', '(2,)Float32', ['a2', 'Complex64']])</screen>

        <para>The names argument only supports lists of strings and 2-tuples.
        Each string defines the name of a non-nested field. Each 2-tuple
        contains the name of a nested field and a list describing the names of
        its sub-fields. If the <literal>names</literal> argument is not passed
        then all fields are automatically named (<literal>c1</literal>,
        <literal>c2</literal> etc. on each nested field) so, in our first
        example, the fields will be named as <literal>['c1', 'c2', ('c3',
        ['c1', 'c2'])]</literal>.</para>

        <para>Another way to specify the nested record array structure is to
        use the <literal>descr</literal> keyword argument:</para>

        <screen>>>> nra3 = array(
...     [(1, (0.5, 1.0), ('a1', 1j)), (2, (0, 0), ('a2', 1+.1j))],
...     descr=[('id', 'Int64'), ('pos', '(2,)Float32'),
...            ('info', [('name', 'a2'), ('value', 'Complex64')])])
>>>
>>> nra3
array(
[(1L, array([ 0.5,  1. ], type=Float32), ('a1', 1j)),
(2L, array([ 0.,  0.], type=Float32), ('a2', (1+0.10000000000000001j)))],
descr=[('id', 'Int64'), ('pos', '(2,)Float32'), ('info', [('name', 'a2'),
('value', 'Complex64')])],
shape=2)
>>></screen>

        <para>The <literal>descr</literal> argument is a list of 2-tuples,
        each of them describing a field. The first value in a tuple is the
        name of the field, while the second one is a description of its
        structure. If the second value is a string, it defines the format
        (shape and type) of a non-nested field. Else, it is a list of 2-tuples
        describing the sub-fields of a nested field.</para>

        <para>As you can see, the <literal>descr</literal> list is a mix of
        the <literal>names</literal> and <literal>formats</literal> arguments.
        In fact, this argument is intended to replace
        <literal>formats</literal> and <literal>names</literal>, so they
        cannot be used at the same time.</para>

        <para>Of course the structure of all three keyword arguments must
        match that of the elements (rows) in the <literal>buffer</literal>
        being stored.</para>

        <para>Sometimes it is convenient to create nested arrays by processing
        a set of columns. In these cases the function
        <literal>fromarrays</literal> comes handy. This function works in a
        very similar way to the array function, but the passed buffer is a
        list of columns. For instance:</para>

        <screen>>>> from tables.nra import fromarrays
>>> nra = fromarrays([[1, 2], [4, 5]], descr=[('x', 'f8'),('y', 'f4')])
>>>
>>> nra
array(
[(1.0, 4.0),
(2.0, 5.0)],
descr=[('x', 'f8'), ('y', 'f4')],
shape=2)</screen>

        <para>Columns can be passed as nested arrays, what makes really
        straightforward to combine different nested arrays to get a new one,
        as you can see in the following examples:</para>

        <screen>>>> nra1 = fromarrays([nra, [7, 8]], descr=[('2D', [('x', 'f8'), ('y', 'f4')]),
>>> ... ('z', 'f4')])
>>>
>>> nra1
array(
[((1.0, 4.0), 7.0),
((2.0, 5.0), 8.0)],
descr=[('2D', [('x', 'f8'), ('y', 'f4')]), ('z', 'f4')],
shape=2)
>>>
>>> nra2 = fromarrays([nra1.field('2D/x'), nra1.field('z')], descr=[('x', 'f8'),
('z', 'f4')])
>>>
>>> nra2
array(
[(1.0, 7.0),
(2.0, 8.0)],
descr=[('x', 'f8'), ('z', 'f4')],
shape=2)</screen>

        <para>Finally it's worth to mention a small group of utility functions
        in the <literal>tables.nra.nestedrecords</literal> module,
        <literal>makeFormats</literal>, <literal>makeNames</literal> and
        <literal>makeDescr</literal>, that can be useful to obtain the
        structure specification to be used with the <literal>array</literal>
        and <literal>fromarrays</literal> functions. Given a description list,
        <literal>makeFormats</literal> gets the corresponding
        <literal>formats</literal> list. In the same way
        <literal>makeNames</literal> gets the <literal>names</literal> list.
        On the other hand the <literal>descr</literal> list can be obtained
        from <literal>formats</literal> and names lists using the
        <literal>makeDescr</literal> function. For example:</para>

        <screen>>>> from tables.nra.nestedrecords import makeDescr, makeFormats, makeNames
>>> descr =[('2D', [('x', 'f8'), ('y', 'f4')]),('z', 'f4')]
>>>
>>> formats = makeFormats(descr)
>>> formats
[['f8', 'f4'], 'f4']
>>> names = makeNames(descr)
>>> names
[('2D', ['x', 'y']), 'z']
>>> d1 = makeDescr(formats, names)
>>> d1
[('2D', [('x', 'f8'), ('y', 'f4')]), ('z', 'f4')]
>>> # If no names are passed then they are automatically generated
>>> d2 = makeDescr(formats)
>>> d2
[('c1', [('c1', 'f8'), ('c2', 'f4')]),('c2', 'f4')]</screen>
      </section>

      <section>
        <title><literal>NestedRecArray</literal> methods</title>

        <para>To access the fields in the nested record array use the
        <literal>field()</literal> method:</para>

        <screen>>>> print nra2.field('id')
[1, 2]
>>></screen>

        <para>The <literal>field()</literal> method accepts also names of
        sub-fields. It will consist of several field name components separated
        by the string <literal>'/'</literal> <footnote>
            <para>This way of specifying the names of sub-fields is
            <emphasis>very</emphasis> specific to the implementation of
            <literal>numarray</literal> nested arrays of PyTables.
            Particularly, if you are using NumPy arrays, keep in mind that
            sub-fields in such arrays must be accessed one at a time, like
            this: <literal>numpy_array['info']['name']</literal>, and not like
            this: <literal>numpy_array['info/name']</literal>.</para>
          </footnote>, for instance:</para>

        <screen>>>> print nra2.field('info/name')
['a1', 'a2']
>>></screen>

        <para>Finally, the top level fields of the nested recarray can be
        accessed passing an integer argument to the <literal>field()</literal>
        method:</para>

        <screen>>>> print nra2.field(1)
[[ 0.5 1. ] [ 0.  0. ]]
>>></screen>

        <para>An alternative to the <literal>field()</literal> method is the
        use of the <literal>fields</literal> attribute. It is intended mainly
        for interactive usage in the Python console. For example:</para>

        <screen>>>> nra2.fields.id
[1, 2]
>>> nra2.fields.info.fields.name
['a1', 'a2']
>>></screen>

        <para>Rows of nested recarrays can be read using the typical index
        syntax. The rows are retrieved as <literal>NestedRecord</literal>
        objects:</para>

        <screen>>>> print nra2[0]
(1L, array([ 0.5,  1. ], type=Float32), ('a1', 1j))
>>>
>>> nra2[0].__class__
&lt;class tables.nra.nestedrecords.NestedRecord at 0x413cbb9c&gt;</screen>

        <para>Slicing is also supported in the usual way:</para>

        <screen>>>> print nra2[0:2]
NestedRecArray[
(1L, array([ 0.5,  1. ], type=Float32), ('a1', 1j)),
(2L, array([ 0.,  0.], type=Float32), ('a2', (1+0.10000000000000001j)))
]
>>></screen>

        <para>Another useful method is <literal>asRecArray()</literal>. It
        converts a nested array to a non-nested equivalent array.</para>

        <para>This method creates a new vanilla <literal>RecArray</literal>
        instance equivalent to this one by flattening its fields. Only
        bottom-level fields included in the array. Sub-fields are named by
        pre-pending the names of their parent fields up to the top-level
        fields, using <literal>'/'</literal> as a separator. The data area of
        the array is copied into the new one. For example, calling
        <literal>nra3.asRecArray()</literal> would return the same array as
        calling:</para>

        <screen>>>> ra = numarray.records.array(
...     [(1, (0.5, 1.0), 'a1', 1j), (2, (0, 0), 'a2', 1+.1j)],
...     names=['id', 'pos', 'info/name', 'info/value'],
...     formats=['Int64', '(2,)Float32', 'a2', 'Complex64'])</screen>

        <para>Note that the shape of multidimensional fields is kept.</para>
      </section>

      <section>
        <title><literal>NestedRecord</literal> objects</title>

        <para>Each element of the nested record array is a
        <literal>NestedRecord</literal>, i.e. a <literal>Record</literal> with
        support for nested datatypes. As said before, we can do indexing as
        usual:</para>

        <screen>>>> print nra1[0]
(1, (0.5, 1.0), ('a1', 1j))
>>></screen>

        <para>Using <literal>NestedRecord</literal> objects is quite similar
        to using <literal>Record</literal> objects. To get the data of a field
        we use the <literal>field()</literal> method. As an argument to this
        method we pass a field name. Sub-field names can be passed in the way
        described for <literal>NestedRecArray.field()</literal>. The
        <literal>fields</literal> attribute is also present and works as it
        does in <literal>NestedRecArray</literal>.</para>

        <para>Field data can be set with the <literal>setField()</literal>
        method. It takes two arguments, the field name and its value.
        Sub-field names can be passed as usual. Finally, the
        <literal>asRecord()</literal> method converts a nested record into a
        non-nested equivalent record.</para>
      </section>
    </appendix>

    <appendix id="PTutilities">
      <title>Utilities</title>

      <para>PyTables comes with a couple of utilities that make the life
      easier to the user. One is called <literal>ptdump</literal> and lets you
      see the contents of a PyTables file (or generic HDF5 file, if
      supported). The other one is named <literal>ptrepack</literal> that
      allows to (recursively) copy sub-hierarchies of objects present in a
      file into another one, changing, if desired, some of the filters applied
      to the leaves during the copy process.</para>

      <para>Normally, these utilities will be installed somewhere in your PATH
      during the process of installation of the PyTables package, so that you
      can invoke them from any place in your file system after the
      installation has successfully finished.</para>

      <section id="ptdumpDescr">
        <title>ptdump</title>

        <para>As has been said before, <literal>ptdump</literal> utility
        allows you look into the contents of your PyTables files. It lets you
        see not only the data but also the metadata (that is, the
        <emphasis>structure</emphasis> and additional information in the form
        of <emphasis>attributes</emphasis>).</para>

        <section>
          <title>Usage</title>

          <para>For instructions on how to use it, just pass the
          <literal>-h</literal> flag to the command:

          <screen>$ ptdump -h</screen>

          to see the message usage:

          <screen>usage: ptdump [-d] [-v] [-a] [-c] [-i] [-R start,stop,step] [-h] file[:nodepath]
    -d -- Dump data information on leaves
    -v -- Dump more metainformation on nodes
    -a -- Show attributes in nodes (only useful when -v or -d are active)
    -c -- Show info of columns in tables (only useful when -v or -d are active)
    -i -- Show info of indexed columns (only useful when -v or -d are active)
    -R RANGE -- Select a RANGE of rows in the form "start,stop,step"
    -h -- Print help on usage
          </screen>

          Read on for a brief introduction to this utility.</para>
        </section>

        <section>
          <title>A small tutorial on <literal>ptdump</literal></title>

          <para>Let's suppose that we want to know only the
          <emphasis>structure</emphasis> of a file. In order to do that, just
          don't pass any flag, just the file as parameter: <screen>$ ptdump vlarray1.h5
/ (RootGroup) ''
/vlarray1 (VLArray(3,), shuffle, zlib(1)) 'ragged array of ints'
/vlarray2 (VLArray(3,), shuffle, zlib(1)) 'ragged array of strings'</screen>
          we can see that the file contains just a leaf object called
          <literal>vlarray1</literal>, that is an instance of
          <literal>VLArray</literal>, has 4 rows, and two filters has been
          used in order to create it: <literal>shuffle</literal> and
          <literal>zlib</literal> (with a compression level of 1).</para>

          <para>Let's say we want more meta-information. Just add the
          <literal>-v</literal> (verbose) flag: <screen>$ ptdump -v vlarray1.h5
/ (RootGroup) ''
/vlarray1 (VLArray(3,), shuffle, zlib(1)) 'ragged array of ints'
  atom = Int32Atom(shape=(), dflt=0)
  byteorder = 'little'
  nrows = 3
  flavor = 'numpy'
/vlarray2 (VLArray(3,), shuffle, zlib(1)) 'ragged array of strings'
  atom = StringAtom(itemsize=2, shape=(), dflt='')
  byteorder = 'irrelevant'
  nrows = 3
  flavor = 'python'</screen> so we can see more info about the atoms that are
          the components of the <literal>vlarray1</literal> dataset, i.e. they
          are scalars of type <literal>Int32</literal> and with
          <literal>NumPy</literal> <emphasis>flavor</emphasis>.</para>

          <para>If we want information about the attributes on the nodes, we
          must add the <literal>-a</literal> flag: <screen>$ ptdump -va vlarray1.h5
/ (RootGroup) ''
  /._v_attrs (AttributeSet), 5 attributes:
   [CLASS := 'GROUP',
    PYTABLES_FORMAT_VERSION := '2.0',
    TITLE := '',
    VERSION := '1.0']
/vlarray1 (VLArray(3,), shuffle, zlib(1)) 'ragged array of ints'
  atom = Int32Atom(shape=(), dflt=0)
  byteorder = 'little'
  nrows = 3
  flavor = 'numpy'
  /vlarray1._v_attrs (AttributeSet), 4 attributes:
   [CLASS := 'VLARRAY',
    FLAVOR := 'numpy',
    TITLE := 'ragged array of ints',
    VERSION := '1.2']
/vlarray2 (VLArray(3,), shuffle, zlib(1)) 'ragged array of strings'
  atom = StringAtom(itemsize=2, shape=(), dflt='')
  byteorder = 'irrelevant'
  nrows = 3
  flavor = 'python'
  /vlarray2._v_attrs (AttributeSet), 4 attributes:
   [CLASS := 'VLARRAY',
    FLAVOR := 'python',
    TITLE := 'ragged array of strings',
    VERSION := '1.2']</screen></para>

          <para>Let's have a look at the real data: <screen>$ ptdump -d vlarray1.h5
/ (RootGroup) ''
/vlarray1 (VLArray(3,), shuffle, zlib(1)) 'ragged array of ints'
  Data dump:
[0] [5 6]
[1] [5 6 7]
[2] [5 6 9 8]
/vlarray2 (VLArray(3,), shuffle, zlib(1)) 'ragged array of strings'
  Data dump:
[0] ['5', '66']
[1] ['5', '6', '77']
[2] ['5', '6', '9', '88']</screen> we see here a data dump of the 4 rows in
          <literal>vlarray1</literal> object, in the form of a list. Because
          the object is a VLA, we see a different number of integers on each
          row.</para>

          <para>Say that we are interested only on a specific <emphasis>row
          range</emphasis> of the <literal>/vlarray1</literal> object:
          <screen>ptdump -R2,3 -d vlarray1.h5:/vlarray1
/vlarray1 (VLArray(3,), shuffle, zlib(1)) 'ragged array of ints'
  Data dump:
[2] [5 6 9 8]</screen> Here, we have specified the range of rows between 2 and
          4 (the upper limit excluded, as usual in Python). See how we have
          selected only the <literal>/vlarray1</literal> object for doing the
          dump (<literal>vlarray1.h5:/vlarray1</literal>).</para>

          <para>Finally, you can mix several information at once: <screen>$ ptdump -R2,3 -vad vlarray1.h5:/vlarray1
/vlarray1 (VLArray(3,), shuffle, zlib(1)) 'ragged array of ints'
  atom = Int32Atom(shape=(), dflt=0)
  byteorder = 'little'
  nrows = 3
  flavor = 'numpy'
  /vlarray1._v_attrs (AttributeSet), 4 attributes:
   [CLASS := 'VLARRAY',
    FLAVOR := 'numpy',
    TITLE := 'ragged array of ints',
    VERSION := '1.2']
  Data dump:
[2] [5 6 9 8]</screen></para>
        </section>
      </section>

      <section id="ptrepackDescr">
        <title>ptrepack</title>

        <para>This utility is a very powerful one and lets you copy any leaf,
        group or complete subtree into another file. During the copy process
        you are allowed to change the filter properties if you want so. Also,
        in the case of duplicated pathnames, you can decide if you want to
        overwrite already existing nodes on the destination file. Generally
        speaking, <literal>ptrepack</literal> can be useful in may situations,
        like replicating a subtree in another file, change the filters in
        objects and see how affect this to the compression degree or I/O
        performance, consolidating specific data in repositories or even
        <emphasis>importing</emphasis> generic HDF5 files and create true
        PyTables counterparts.</para>

        <section>
          <title>Usage</title>

          <para>For instructions on how to use it, just pass the
          <literal>-h</literal> flag to the command:

          <screen>$ ptrepack -h</screen>

          to see the message usage:

          <screen>usage: ptrepack [-h] [-v] [-o] [-R start,stop,step] [--non-recursive] [--dest-title=title] [--dont-create-sysattrs] [--dont-copy-userattrs] [--overwrite-nodes] [--complevel=(0-9)] [--complib=lib] [--shuffle=(0|1)] [--fletcher32=(0|1)] [--keep-source-filters] [--chunkshape=value] [--upgrade-flavors] [--dont-regenerate-old-indexes] [--sortby=column] [--checkCSI] [--propindexes] sourcefile:sourcegroup destfile:destgroup
     -h -- Print usage message.
     -v -- Show more information.
     -o -- Overwrite destination file.
     -R RANGE -- Select a RANGE of rows (in the form "start,stop,step")
         during the copy of *all* the leaves.  Default values are
         "None,None,1", which means a copy of all the rows.
     --non-recursive -- Do not do a recursive copy. Default is to do it.
     --dest-title=title -- Title for the new file (if not specified,
         the source is copied).
     --dont-create-sysattrs -- Do not create sys attrs (default is to do it).
     --dont-copy-userattrs -- Do not copy the user attrs (default is to do it).
     --overwrite-nodes -- Overwrite destination nodes if they exist. Default is
         to not overwrite them.
     --complevel=(0-9) -- Set a compression level (0 for no compression, which
         is the default).
     --complib=lib -- Set the compression library to be used during the copy.
         lib can be set to "zlib", "lzo", "bzip2" or "blosc".  Defaults to
         "zlib".
     --shuffle=(0|1) -- Activate or not the shuffling filter (default is active
         if complevel>0).
     --fletcher32=(0|1) -- Whether to activate or not the fletcher32 filter
        (not active by default).
     --keep-source-filters -- Use the original filters in source files. The
         default is not doing that if any of --complevel, --complib, --shuffle
         or --fletcher32 option is specified.
     --chunkshape=("keep"|"auto"|int|tuple) -- Set a chunkshape.  A value
         of "auto" computes a sensible value for the chunkshape of the
         leaves copied.  The default is to "keep" the original value.
     --upgrade-flavors -- When repacking PyTables 1.x files, the flavor of
         leaves will be unset. With this, such a leaves will be serialized
         as objects with the internal flavor ('numpy' for 2.x series).
     --dont-regenerate-old-indexes -- Disable regenerating old indexes. The
         default is to regenerate old indexes as they are found.
     --sortby=column -- Do a table copy sorted by the index in "column".
         For reversing the order, use a negative value in the "step" part of
         "RANGE" (see "-R" flag).  Only applies to table objects.
     --checkCSI -- Force the check for a CSI index for the --sortby column.
     --propindexes -- Propagate the indexes existing in original tables.  The
         default is to not propagate them.  Only applies to table objects.
          </screen>


          Read on for a brief introduction to this utility.</para>
        </section>

        <section>
          <title>A small tutorial on <literal>ptrepack</literal></title>

          <para>Imagine that we have ended the tutorial 1 (see the output of
          <literal>examples/tutorial1-1.py</literal>), and we want to copy our
          reduced data (i.e. those datasets that hangs from the
          <literal>/column</literal> group) to another file. First, let's
          remember the content of the
          <literal>examples/tutorial1.h5</literal>: <screen>$ ptdump tutorial1.h5
/ (RootGroup) 'Test file'
/columns (Group) 'Pressure and Name'
/columns/name (Array(3,)) 'Name column selection'
/columns/pressure (Array(3,)) 'Pressure column selection'
/detector (Group) 'Detector information'
/detector/readout (Table(10,)) 'Readout example'</screen> Now, copy the
          <literal>/columns</literal> to other non-existing file. That's easy:
          <screen>$ ptrepack tutorial1.h5:/columns reduced.h5</screen> That's
          all. Let's see the contents of the newly created
          <literal>reduced.h5</literal> file: <screen>$ ptdump reduced.h5
/ (RootGroup) ''
/name (Array(3,)) 'Name column selection'
/pressure (Array(3,)) 'Pressure column selection'</screen> so, you have
          copied the children of <literal>/columns</literal> group into the
          <emphasis>root</emphasis> of the <literal>reduced.h5</literal>
          file.</para>

          <para>Now, you suddenly realized that what you intended to do was to
          copy all the hierarchy, the group <literal>/columns</literal> itself
          included. You can do that by just specifying the destination group:
          <screen>$ ptrepack tutorial1.h5:/columns reduced.h5:/columns
$ ptdump reduced.h5
/ (RootGroup) ''
/name (Array(3,)) 'Name column selection'
/pressure (Array(3,)) 'Pressure column selection'
/columns (Group) ''
/columns/name (Array(3,)) 'Name column selection'
/columns/pressure (Array(3,)) 'Pressure column selection'</screen> OK. Much
          better. But you want to get rid of the existing nodes on the new
          file. You can achieve this by adding the -o flag: <screen>$ ptrepack -o tutorial1.h5:/columns reduced.h5:/columns
$ ptdump reduced.h5
/ (RootGroup) ''
/columns (Group) ''
/columns/name (Array(3,)) 'Name column selection'
/columns/pressure (Array(3,)) 'Pressure column selection'</screen> where you
          can see how the old contents of the <literal>reduced.h5</literal>
          file has been overwritten.</para>

          <para>You can copy just one single node in the repacking operation
          and change its name in destination: <screen>$ ptrepack tutorial1.h5:/detector/readout reduced.h5:/rawdata
$ ptdump reduced.h5
/ (RootGroup) ''
/rawdata (Table(10,)) 'Readout example'
/columns (Group) ''
/columns/name (Array(3,)) 'Name column selection'
/columns/pressure (Array(3,)) 'Pressure column selection'</screen> where the
          <literal>/detector/readout</literal> has been copied to
          <literal>/rawdata</literal> in destination.</para>

          <para>We can change the filter properties as well: <screen>$ ptrepack --complevel=1 tutorial1.h5:/detector/readout reduced.h5:/rawdata
Problems doing the copy from 'tutorial1.h5:/detector/readout' to 'reduced.h5:/rawdata'
The error was --&gt; tables.exceptions.NodeError: destination group ``/`` already has a node named ``rawdata``; you may want to use the ``overwrite`` argument
The destination file looks like:
/ (RootGroup) ''
/rawdata (Table(10,)) 'Readout example'
/columns (Group) ''
/columns/name (Array(3,)) 'Name column selection'
/columns/pressure (Array(3,)) 'Pressure column selection'

Traceback (most recent call last):
  File "utils/ptrepack", line 3, in ?
    main()
  File ".../tables/scripts/ptrepack.py", line 349, in main
    stats = stats, start = start, stop = stop, step = step)
  File ".../tables/scripts/ptrepack.py", line 107, in copyLeaf
    raise RuntimeError, "Please check that the node names are not
    duplicated in destination, and if so, add the --overwrite-nodes flag
    if desired."
RuntimeError: Please check that the node names are not duplicated in
destination, and if so, add the --overwrite-nodes flag if desired.</screen>
          Ooops! We ran into problems: we forgot that the
          <literal>/rawdata</literal> pathname already existed in destination
          file. Let's add the <literal>--overwrite-nodes</literal>, as the
          verbose error suggested: <screen>$ ptrepack --overwrite-nodes --complevel=1 tutorial1.h5:/detector/readout
reduced.h5:/rawdata
$ ptdump reduced.h5
/ (RootGroup) ''
/rawdata (Table(10,), shuffle, zlib(1)) 'Readout example'
/columns (Group) ''
/columns/name (Array(3,)) 'Name column selection'
/columns/pressure (Array(3,)) 'Pressure column selection'</screen> you can
          check how the filter properties has been changed for the
          <literal>/rawdata</literal> table. Check as the other nodes still
          exists.</para>

          <para>Finally, let's copy a <emphasis>slice</emphasis> of the
          <literal>readout</literal> table in origin to destination, under a
          new group called <literal>/slices</literal> and with the name, for
          example, <literal>aslice</literal>: <screen>$ ptrepack -R1,8,3 tutorial1.h5:/detector/readout reduced.h5:/slices/aslice
$ ptdump reduced.h5
/ (RootGroup) ''
/rawdata (Table(10,), shuffle, zlib(1)) 'Readout example'
/columns (Group) ''
/columns/name (Array(3,)) 'Name column selection'
/columns/pressure (Array(3,)) 'Pressure column selection'
/slices (Group) ''
/slices/aslice (Table(3,)) 'Readout example'</screen> note how only 3 rows of
          the original <literal>readout</literal> table has been copied to the
          new <literal>aslice</literal> destination. Note as well how the
          previously nonexistent <literal>slices</literal> group has been
          created in the same operation.</para>
        </section>
      </section>

      <section id="nctoh5Descr">
        <title>nctoh5</title>

        <para>This tool is able to convert a file in <ulink
        url="http://www.unidata.ucar.edu/packages/netcdf/"><literal>NetCDF</literal></ulink>
        format to a PyTables file (and hence, to a HDF5 file). However, for
        this to work, you will need the NetCDF interface for Python that comes
        with the excellent <literal>Scientific Python</literal> (see
        <biblioref linkend="scientificpythonRef" />) package. This script was
        initially contributed by Jeff Whitaker. It has been updated to support
        selectable filters from the command line and some other small
        improvements.</para>

        <para>If you want other file formats to be converted to PyTables, have
        a look at the <literal>SciPy</literal> (see <biblioref
        linkend="scipyRef" />) project (subpackage <literal>io</literal>), and
        look for different methods to import them into
        <literal>NumPy/Numeric/numarray</literal> objects. Following the
        <literal>SciPy</literal> documentation, you can read, among other
        formats, ASCII files (<literal>read_array</literal>), binary files in
        C or Fortran (<literal>fopen</literal>) and <literal>MATLAB</literal>
        (version 4, 5 or 6) files (<literal>loadmat</literal>). Once you have
        the content of your files as <literal>NumPy/Numeric/numarray</literal>
        objects, you can save them as regular <literal>(E)Arrays</literal> in
        PyTables files. Remember, if you end with a nice converter, do not
        forget to contribute it back to the community. Thanks!</para>

        <section>
          <title>Usage</title>

          <para>For instructions on how to use it, just pass the
          <literal>-h</literal> flag to the command:

          <screen>$ nctoh5 -h</screen>

          to see the message usage:</para>

          <screen>usage: nctoh5 [-h] [-v] [-o] [--complevel=(0-9)] [--complib=lib] [--shuffle=(0|1)] [--fletcher32=(0|1)] netcdffilename hdf5filename
   -h -- Print usage message.
   -v -- Show more information.
   -o -- Overwrite destination file.
   --complevel=(0-9) -- Set a compression level (0 for no compression, which
       is the default).
   --complib=lib -- Set the compression library to be used during the copy.
       lib can be set to "zlib" or "lzo". Defaults to "zlib".
   --shuffle=(0|1) -- Activate or not the shuffling filter (default is active
       if complevel>0).
   --fletcher32=(0|1) -- Whether to activate or not the fletcher32 filter (not
       active by default).
          </screen>

        </section>
      </section>
    </appendix>

    <appendix id="PyTablesInternalFormat">
      <title>PyTables File Format</title>

      <para>PyTables has a powerful capability to deal with native HDF5 files
      created with another tools. However, there are situations were you may
      want to create truly native PyTables files with those tools while
      retaining fully compatibility with PyTables format. That is perfectly
      possible, and in this appendix is presented the format that you should
      endow to your own-generated files in order to get a fully PyTables
      compatible file.</para>

      <para>We are going to describe the <emphasis>2.0 version of PyTables
      file format</emphasis> (introduced in PyTables version 2.0). As time
      goes by, some changes might be introduced (and documented here) in order
      to cope with new necessities. However, the changes will be carefully
      pondered so as to ensure backward compatibility whenever is
      possible.</para>

      <para>A PyTables file is composed with arbitrarily large amounts of HDF5
      groups (<literal>Groups</literal> in PyTables naming scheme) and
      datasets (<literal>Leaves</literal> in PyTables naming scheme). For
      groups, the only requirements are that they must have some
      <emphasis>system attributes</emphasis> available. By convention, system
      attributes in PyTables are written in upper case, and user attributes in
      lower case but this is not enforced by the software. In the case of
      datasets, besides the mandatory system attributes, some conditions are
      further needed in their storage layout, as well as in the datatypes used
      in there, as we will see shortly.</para>

      <para>As a final remark, you can use any filter as you want to create a
      PyTables file, provided that the filter is a standard one in HDF5, like
      <emphasis>zlib</emphasis>, <emphasis>shuffle</emphasis> or
      <emphasis>szip</emphasis> (although the last one can not be used from
      within PyTables to create a new file, datasets compressed with szip can
      be read, because it is the HDF5 library which do the decompression
      transparently).</para>

      <section>
        <title>Mandatory attributes for a <literal>File</literal></title>

        <para>The <literal>File</literal> object is, in fact, an special HDF5
        <emphasis>group</emphasis> structure that is <emphasis>root</emphasis>
        for the rest of the objects on the object tree. The next attributes
        are mandatory for the HDF5 <emphasis>root group</emphasis> structure
        in PyTables files:</para>

        <glosslist>
          <glossentry>
            <glossterm><emphasis role="bold">CLASS</emphasis></glossterm>

            <glossdef>
              <para>This attribute should always be set to
              <literal>'GROUP'</literal> for group structures.</para>
            </glossdef>
          </glossentry>

          <glossentry>
            <glossterm><emphasis
            role="bold">PYTABLES_FORMAT_VERSION</emphasis></glossterm>

            <glossdef>
              <para>It represents the internal format version, and currently
              should be set to the <literal>'2.0'</literal> string.</para>
            </glossdef>
          </glossentry>

          <glossentry>
            <glossterm><emphasis role="bold">TITLE</emphasis></glossterm>

            <glossdef>
              <para>A string where the user can put some description on what
              is this group used for.</para>
            </glossdef>
          </glossentry>

          <glossentry>
            <glossterm><emphasis role="bold">VERSION</emphasis></glossterm>

            <glossdef>
              <para>Should contains the string
              <literal>'1.0'</literal>.</para>
            </glossdef>
          </glossentry>
        </glosslist>
      </section>

      <section>
        <title>Mandatory attributes for a <literal>Group</literal></title>

        <para>The next attributes are mandatory for <emphasis>group</emphasis>
        structures:</para>

        <glosslist>
          <glossentry>
            <glossterm><emphasis role="bold">CLASS</emphasis></glossterm>

            <glossdef>
              <para>This attribute should always be set to
              <literal>'GROUP'</literal> for group structures.</para>
            </glossdef>
          </glossentry>

          <glossentry>
            <glossterm><emphasis role="bold">TITLE</emphasis></glossterm>

            <glossdef>
              <para>A string where the user can put some description on what
              is this group used for.</para>
            </glossdef>
          </glossentry>

          <glossentry>
            <glossterm><emphasis role="bold">VERSION</emphasis></glossterm>

            <glossdef>
              <para>Should contains the string
              <literal>'1.0'</literal>.</para>
            </glossdef>
          </glossentry>
        </glosslist>
      </section>

      <section>
        <title>Optional attributes for a <literal>Group</literal></title>

        <para>The next attributes are optional for <emphasis>group</emphasis>
        structures:</para>

        <glosslist>
          <glossentry>
            <glossterm><emphasis role="bold">FILTERS</emphasis></glossterm>

            <glossdef>
              <para>When present, this attribute contains the filter
              properties (a <literal>Filters</literal> instance, see section
              <xref linkend="FiltersClassDescr" xrefstyle="select: label" />)
              that may be inherited by leaves or groups created immediately
              under this group. This is a packed 64-bit integer structure,
              where</para>

              <itemizedlist>
                <listitem>
                  <para><emphasis role="bold">byte 0</emphasis> (the
                  least-significant byte) is the compression level
                  (<literal>complevel</literal>).</para>
                </listitem>

                <listitem>
                  <para><emphasis role="bold">byte 1</emphasis> is the
                  compression library used (<literal>complib</literal>): 0
                  when irrelevant, 1 for Zlib, 2 for LZO and 3 for
                  Bzip2.</para>
                </listitem>

                <listitem>
                  <para><emphasis role="bold">byte 2</emphasis> indicates
                  which parameterless filters are enabled
                  (<literal>shuffle</literal> and
                  <literal>fletcher32</literal>): bit 0 is for
                  <emphasis>Shuffle</emphasis> while bit 1 is for
                  <emphasis>Fletcher32</emphasis>.</para>
                </listitem>

                <listitem>
                  <para>other bytes are reserved for future use.</para>
                </listitem>
              </itemizedlist>
            </glossdef>
          </glossentry>
        </glosslist>
      </section>

      <section>
        <title>Mandatory attributes, storage layout and supported data types
        for <literal>Leaves</literal></title>

        <para>This depends on the kind of <literal>Leaf</literal>. The format
        for each type follows.</para>

        <section id="TableFormatDescr">
          <title><literal>Table</literal> format</title>

          <section>
            <title>Mandatory attributes</title>

            <para>The next attributes are mandatory for
            <emphasis>table</emphasis> structures:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">CLASS</emphasis></glossterm>

                <glossdef>
                  <para>Must be set to <literal>'TABLE'</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">TITLE</emphasis></glossterm>

                <glossdef>
                  <para>A string where the user can put some description on
                  what is this dataset used for.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">VERSION</emphasis></glossterm>

                <glossdef>
                  <para>Should contain the string
                  <literal>'2.6'</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">FIELD_X_NAME</emphasis></glossterm>

                <glossdef>
                  <para>It contains the names of the different fields. The
                  <literal>X</literal> means the number of the field,
                  zero-based (beware, order do matter). You should add as many
                  attributes of this kind as fields you have in your
                  records.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">FIELD_X_FILL</emphasis></glossterm>

                <glossdef>
                  <para>It contains the default values of the different
                  fields. All the datatypes are supported natively, except for
                  complex types that are currently serialized using Pickle.
                  The <literal>X</literal> means the number of the field,
                  zero-based (beware, order do matter). You should add as many
                  attributes of this kind as fields you have in your records.
                  These fields are meant for saving the default values
                  persistently and their existence is optional.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">NROWS</emphasis></glossterm>

                <glossdef>
                  <para>This should contain the number of
                  <emphasis>compound</emphasis> data type entries in the
                  dataset. It must be an <emphasis>int</emphasis> data
                  type.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title>Storage Layout</title>

            <para>A <literal>Table</literal> has a
            <emphasis>dataspace</emphasis> with a <emphasis>1-dimensional
            chunked</emphasis> layout.</para>
          </section>

          <section>
            <title>Datatypes supported</title>

            <para>The datatype of the elements (rows) of
            <literal>Table</literal> must be the H5T_COMPOUND
            <emphasis>compound</emphasis> data type, and each of these
            compound components must be built with only the next HDF5 data
            types <emphasis>classes</emphasis>:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis
                role="bold">H5T_BITFIELD</emphasis></glossterm>

                <glossdef>
                  <para>This class is used to represent the
                  <literal>Bool</literal> type. Such a type must be build
                  using a H5T_NATIVE_B8 datatype, followed by a HDF5
                  <literal>H5Tset_precision</literal> call to set its
                  precision to be just 1 bit.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">H5T_INTEGER</emphasis></glossterm>

                <glossdef>
                  <para>This includes the next data types:</para>

                  <glosslist>
                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_SCHAR</emphasis></glossterm>

                      <glossdef>
                        <para>This represents a <emphasis>signed
                        char</emphasis> C type, but it is effectively used to
                        represent an <literal>Int8</literal> type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_UCHAR</emphasis></glossterm>

                      <glossdef>
                        <para>This represents an <emphasis>unsigned
                        char</emphasis> C type, but it is effectively used to
                        represent an <literal>UInt8</literal> type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_SHORT</emphasis></glossterm>

                      <glossdef>
                        <para>This represents a <emphasis>short</emphasis> C
                        type, and it is effectively used to represent an
                        <literal>Int16</literal> type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_USHORT</emphasis></glossterm>

                      <glossdef>
                        <para>This represents an <emphasis>unsigned
                        short</emphasis> C type, and it is effectively used to
                        represent an <literal>UInt16</literal> type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_INT</emphasis></glossterm>

                      <glossdef>
                        <para>This represents an <emphasis>int</emphasis> C
                        type, and it is effectively used to represent an
                        <literal>Int32</literal> type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_UINT</emphasis></glossterm>

                      <glossdef>
                        <para>This represents an <emphasis>unsigned
                        int</emphasis> C type, and it is effectively used to
                        represent an <literal>UInt32</literal> type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_LONG</emphasis></glossterm>

                      <glossdef>
                        <para>This represents a <emphasis>long</emphasis> C
                        type, and it is effectively used to represent an
                        <literal>Int32</literal> or an
                        <literal>Int64</literal>, depending on whether you are
                        running a 32-bit or 64-bit architecture.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_ULONG</emphasis></glossterm>

                      <glossdef>
                        <para>This represents an <emphasis>unsigned
                        long</emphasis> C type, and it is effectively used to
                        represent an <literal>UInt32</literal> or an
                        <literal>UInt64</literal>, depending on whether you
                        are running a 32-bit or 64-bit architecture.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_LLONG</emphasis></glossterm>

                      <glossdef>
                        <para>This represents a <emphasis>long long</emphasis>
                        C type (<literal>__int64</literal>, if you are using a
                        Windows system) and it is effectively used to
                        represent an <literal>Int64</literal> type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_ULLONG</emphasis></glossterm>

                      <glossdef>
                        <para>This represents an <emphasis>unsigned long
                        long</emphasis> C type (beware: this type does not
                        have a correspondence on Windows systems) and it is
                        effectively used to represent an
                        <literal>UInt64</literal> type.</para>
                      </glossdef>
                    </glossentry>
                  </glosslist>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">H5T_FLOAT</emphasis></glossterm>

                <glossdef>
                  <para>This includes the next datatypes:</para>

                  <glosslist>
                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_FLOAT</emphasis></glossterm>

                      <glossdef>
                        <para>This represents a <emphasis>float</emphasis> C
                        type and it is effectively used to represent an
                        <literal>Float32</literal> type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_NATIVE_DOUBLE</emphasis></glossterm>

                      <glossdef>
                        <para>This represents a <emphasis>double</emphasis> C
                        type and it is effectively used to represent an
                        <literal>Float64</literal> type.</para>
                      </glossdef>
                    </glossentry>
                  </glosslist>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">H5T_TIME</emphasis></glossterm>

                <glossdef>
                  <para>This includes the next datatypes:</para>

                  <glosslist>
                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_UNIX_D32</emphasis></glossterm>

                      <glossdef>
                        <para>This represents a POSIX
                        <emphasis>time_t</emphasis> C type and it is
                        effectively used to represent a
                        <literal>'Time32'</literal> aliasing type, which
                        corresponds to an <literal>Int32</literal>
                        type.</para>
                      </glossdef>
                    </glossentry>

                    <glossentry>
                      <glossterm><emphasis
                      role="bold">H5T_UNIX_D64</emphasis></glossterm>

                      <glossdef>
                        <para>This represents a POSIX <emphasis>struct
                        timeval</emphasis> C type and it is effectively used
                        to represent a <literal>'Time64'</literal> aliasing
                        type, which corresponds to a
                        <literal>Float64</literal> type.</para>
                      </glossdef>
                    </glossentry>
                  </glosslist>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">H5T_STRING</emphasis></glossterm>

                <glossdef>
                  <para>The datatype used to describe strings in PyTables is
                  H5T_C_S1 (i.e. a <emphasis>string</emphasis> C type)
                  followed with a call to the HDF5
                  <literal>H5Tset_size()</literal> function to set their
                  length.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">H5T_ARRAY</emphasis></glossterm>

                <glossdef>
                  <para>This allows the construction of homogeneous,
                  multidimensional arrays, so that you can include such
                  objects in compound records. The types supported as elements
                  of H5T_ARRAY data types are the ones described above.
                  Currently, PyTables does not support nested H5T_ARRAY
                  types.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">H5T_COMPOUND</emphasis></glossterm>

                <glossdef>

                  <para>This allows the support for datatypes that are
                  compounds of compounds (this is also known as
                  <emphasis>nested types</emphasis> along this manual).</para>

                  <para>This support can also be used for defining complex
                  numbers. Its format is described below:</para>

                  <para>The H5T_COMPOUND type class contains two members. Both
                  members must have the H5T_FLOAT atomic datatype class. The
                  name of the first member should be "r" and represents the
                  real part. The name of the second member should be "i" and
                  represents the imaginary part. The
                  <emphasis>precision</emphasis> property of both of the
                  H5T_FLOAT members must be either 32 significant bits (e.g.
                  H5T_NATIVE_FLOAT) or 64 significant bits (e.g.
                  H5T_NATIVE_DOUBLE). They represent Complex32 and Complex64
                  types respectively.</para>
                </glossdef>
              </glossentry>
            </glosslist>

          </section>
        </section>

        <section id="ArrayFormatDescr">
          <title><literal>Array</literal> format</title>

          <section>
            <title>Mandatory attributes</title>

            <para>The next attributes are mandatory for
            <emphasis>array</emphasis> structures:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">CLASS</emphasis></glossterm>

                <glossdef>
                  <para>Must be set to <literal>'ARRAY'</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">TITLE</emphasis></glossterm>

                <glossdef>
                  <para>A string where the user can put some description on
                  what is this dataset used for.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">VERSION</emphasis></glossterm>

                <glossdef>
                  <para>Should contain the string
                  <literal>'2.3'</literal>.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title>Storage Layout</title>

            <para>An <literal>Array</literal> has a
            <emphasis>dataspace</emphasis> with a <emphasis>N-dimensional
            contiguous</emphasis> layout (if you prefer a
            <emphasis>chunked</emphasis> layout see <literal>EArray</literal>
            below).</para>
          </section>

          <section>
            <title>Datatypes supported</title>

            <para>The elements of <literal>Array</literal> must have either
            HDF5 <emphasis>atomic</emphasis> data types or a
            <emphasis>compound</emphasis> data type representing a complex
            number. The atomic data types can currently be one of the next
            HDF5 data type <emphasis>classes</emphasis>: H5T_BITFIELD,
            H5T_INTEGER, H5T_FLOAT and H5T_STRING. The H5T_TIME class is also
            supported for reading existing <literal>Array</literal> objects,
            but not for creating them. See the <literal>Table</literal> format
            description in <xref linkend="TableFormatDescr"
            xrefstyle="select: label" /> for more info about these
            types.</para>

            <para>In addition to the HDF5 atomic data types, the Array format
            supports complex numbers with the H5T_COMPOUND data type class.
            See the <literal>Table</literal> format description in <xref
            linkend="TableFormatDescr" xrefstyle="select: label" /> for more
            info about this special type.</para>

            <para>You should note that H5T_ARRAY class datatypes are not
            allowed in <literal>Array</literal> objects.</para>
          </section>
        </section>

        <section id="CArrayFormatDescr">
          <title><literal>CArray</literal> format</title>

          <section>
            <title>Mandatory attributes</title>

            <para>The next attributes are mandatory for
            <emphasis>CArray</emphasis> structures:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">CLASS</emphasis></glossterm>

                <glossdef>
                  <para>Must be set to <literal>'CARRAY'</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">TITLE</emphasis></glossterm>

                <glossdef>
                  <para>A string where the user can put some description on
                  what is this dataset used for.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">VERSION</emphasis></glossterm>

                <glossdef>
                  <para>Should contain the string
                  <literal>'1.0'</literal>.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title>Storage Layout</title>

            <para>An <literal>CArray</literal> has a
            <emphasis>dataspace</emphasis> with a <emphasis>N-dimensional
            chunked</emphasis> layout.</para>
          </section>

          <section>
            <title>Datatypes supported</title>

            <para>The elements of <literal>CArray</literal> must have either
            HDF5 <emphasis>atomic</emphasis> data types or a
            <emphasis>compound</emphasis> data type representing a complex
            number. The atomic data types can currently be one of the next
            HDF5 data type <emphasis>classes</emphasis>: H5T_BITFIELD,
            H5T_INTEGER, H5T_FLOAT and H5T_STRING. The H5T_TIME class is also
            supported for reading existing <literal>CArray</literal> objects,
            but not for creating them. See the <literal>Table</literal> format
            description in <xref linkend="TableFormatDescr"
            xrefstyle="select: label" /> for more info about these
            types.</para>

            <para>In addition to the HDF5 atomic data types, the CArray format
            supports complex numbers with the H5T_COMPOUND data type class.
            See the <literal>Table</literal> format description in <xref
            linkend="TableFormatDescr" xrefstyle="select: label" /> for more
            info about this special type.</para>

            <para>You should note that H5T_ARRAY class datatypes are not
            allowed yet in <literal>Array</literal> objects.</para>
          </section>
        </section>

        <section id="EArrayFormatDescr">
          <title><literal>EArray</literal> format</title>

          <section>
            <title>Mandatory attributes</title>

            <para>The next attributes are mandatory for
            <emphasis>earray</emphasis> structures:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">CLASS</emphasis></glossterm>

                <glossdef>
                  <para>Must be set to <literal>'EARRAY'</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">EXTDIM</emphasis></glossterm>

                <glossdef>
                  <para>(<emphasis>Integer</emphasis>) Must be set to the
                  extendable dimension. Only one extendable dimension is
                  supported right now.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">TITLE</emphasis></glossterm>

                <glossdef>
                  <para>A string where the user can put some description on
                  what is this dataset used for.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">VERSION</emphasis></glossterm>

                <glossdef>
                  <para>Should contain the string
                  <literal>'1.3'</literal>.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title>Storage Layout</title>

            <para>An <literal>EArray</literal> has a
            <emphasis>dataspace</emphasis> with a <emphasis>N-dimensional
            chunked</emphasis> layout.</para>
          </section>

          <section>
            <title>Datatypes supported</title>

            <para>The elements of <literal>EArray</literal> are allowed to
            have the same data types as for the elements in the Array format.
            They can be one of the HDF5 <emphasis>atomic</emphasis> data type
            <emphasis>classes</emphasis>: H5T_BITFIELD, H5T_INTEGER,
            H5T_FLOAT, H5T_TIME or H5T_STRING, see the
            <literal>Table</literal> format description in <xref
            linkend="TableFormatDescr" xrefstyle="select: label" /> for more
            info about these types. They can also be a H5T_COMPOUND datatype
            representing a complex number, see the <literal>Table</literal>
            format description in <xref linkend="TableFormatDescr"
            xrefstyle="select: label" />.</para>

            <para>You should note that H5T_ARRAY class data types are not
            allowed in <literal>EArray</literal> objects.</para>
          </section>
        </section>

        <section id="VLArrayFormatDescr">
          <title><literal>VLArray</literal> format</title>

          <section>
            <title>Mandatory attributes</title>

            <para>The next attributes are mandatory for
            <emphasis>vlarray</emphasis> structures:</para>

            <glosslist>
              <glossentry>
                <glossterm><emphasis role="bold">CLASS</emphasis></glossterm>

                <glossdef>
                  <para>Must be set to <literal>'VLARRAY'</literal>.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">PSEUDOATOM</emphasis></glossterm>

                <glossdef>
                  <para>This is used so as to specify the kind of pseudo-atom
                  (see <xref linkend="VLArrayFormatDescr" xrefstyle="select:
                  label" />) for the <literal>VLArray</literal>. It can take
                  the values <literal>'vlstring'</literal>,
                  <literal>'vlunicode'</literal> or
                  <literal>'object'</literal>. If your atom is not a
                  pseudo-atom then you should not specify it.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis role="bold">TITLE</emphasis></glossterm>

                <glossdef>
                  <para>A string where the user can put some description on
                  what is this dataset used for.</para>
                </glossdef>
              </glossentry>

              <glossentry>
                <glossterm><emphasis
                role="bold">VERSION</emphasis></glossterm>

                <glossdef>
                  <para>Should contain the string
                  <literal>'1.3'</literal>.</para>
                </glossdef>
              </glossentry>
            </glosslist>
          </section>

          <section>
            <title>Storage Layout</title>

            <para>An <literal>VLArray</literal> has a
            <emphasis>dataspace</emphasis> with a <emphasis>1-dimensional
            chunked</emphasis> layout.</para>
          </section>

          <section>
            <title>Data types supported</title>

            <para>The data type of the elements (rows) of
            <literal>VLArray</literal> objects must be the H5T_VLEN
            <emphasis>variable-length</emphasis> (or VL for short) datatype,
            and the base datatype specified for the VL datatype can be of any
            <emphasis>atomic</emphasis> HDF5 datatype that is listed in the
            <literal>Table</literal> format description <xref
            linkend="TableFormatDescr" xrefstyle="select: label" />.  That
            includes the classes:</para>

            <itemizedlist>
              <listitem>
                <para>H5T_BITFIELD</para>
              </listitem>

              <listitem>
                <para>H5T_INTEGER</para>
              </listitem>

              <listitem>
                <para>H5T_FLOAT</para>
              </listitem>

              <listitem>
                <para>H5T_TIME</para>
              </listitem>

              <listitem>
                <para>H5T_STRING</para>
              </listitem>

              <listitem>
                <para>H5T_ARRAY</para>
              </listitem>
            </itemizedlist>

            <para>They can also be a H5T_COMPOUND data type representing a
            complex number, see the <literal>Table</literal> format
            description in <xref linkend="TableFormatDescr"
            xrefstyle="select: label" /> for a detailed description.</para>

            <para>You should note that this does not include another VL
            datatype, or a compound datatype that does not fit the description
            of a complex number. Note as well that, for
            <literal>object</literal> and <literal>vlstring</literal>
            pseudo-atoms, the base for the VL datatype is always a
            <literal>H5T_NATIVE_UCHAR</literal>
            (<literal>H5T_NATIVE_UINT</literal> for
            <literal>vlunicode</literal>). That means that the complete row
            entry in the dataset has to be used in order to fully serialize
            the object or the variable length string.</para>
          </section>
        </section>
      </section>

      <section>
        <title>Optional attributes for <literal>Leaves</literal></title>

        <para>The next attributes are optional for
        <emphasis>leaves</emphasis>:</para>

        <glosslist>
          <glossentry>
            <glossterm><emphasis role="bold">FLAVOR</emphasis></glossterm>

            <glossdef>
              <para>This is meant to provide the information about the kind of
              object kept in the <literal>Leaf</literal>, i.e. when the
              dataset is read, it will be converted to the indicated flavor.
              It can take one the next string values:</para>

              <glosslist>
                <glossentry>
                  <glossterm><emphasis
                  role="bold">"numpy"</emphasis></glossterm>

                  <glossdef>
                    <para>Read data (record arrays, arrays, records, scalars)
                    will be returned as <literal>NumPy</literal>
                    objects.</para>
                  </glossdef>
                </glossentry>

                <glossentry>
                  <glossterm><emphasis
                  role="bold">"numarray"</emphasis></glossterm>

                  <glossdef>
                    <para>Read data will be returned as
                    <literal>numarray</literal> objects (deprecated).</para>
                  </glossdef>
                </glossentry>

                <glossentry>
                  <glossterm><emphasis
                  role="bold">"numeric"</emphasis></glossterm>

                  <glossdef>
                    <para>Read data will be returned as
                    <literal>Numeric</literal> objects (deprecated).</para>
                  </glossdef>
                </glossentry>

                <glossentry>
                  <glossterm><emphasis
                  role="bold">"python"</emphasis></glossterm>

                  <glossdef>
                    <para>Read data will be returned as Python lists, tuples
                    or scalars.</para>
                  </glossdef>
                </glossentry>
              </glosslist>
            </glossdef>
          </glossentry>
        </glosslist>
      </section>
    </appendix>
  </part>

  <bibliography>
    <bibliomixed id="HDFWhatIs"></bibliomixed>

    <bibliomixed id="HDFIntr"></bibliomixed>

    <bibliomixed id="TableExamples"></bibliomixed>

    <bibliomixed id="Objectify"></bibliomixed>

    <bibliomixed id="Cython"></bibliomixed>

    <bibliomixed id="NetCDFRef"></bibliomixed>

    <bibliomixed id="NetCDF4Ref"></bibliomixed>

    <bibliomixed id="NumPy"></bibliomixed>

    <bibliomixed id="Numeric"></bibliomixed>

    <bibliomixed id="Numarray"></bibliomixed>

    <bibliomixed id="Numexpr"></bibliomixed>

    <bibliomixed id="zlibRef"></bibliomixed>

    <bibliomixed id="lzoRef"></bibliomixed>

    <bibliomixed id="bzip2Ref"></bibliomixed>

    <bibliomixed id="BloscRef"></bibliomixed>

    <bibliomixed id="GnuWin32"></bibliomixed>

    <bibliomixed id="psycoRef"></bibliomixed>

    <bibliomixed id="scientificpythonRef"></bibliomixed>

    <bibliomixed id="scipyRef"></bibliomixed>

    <bibliomixed id="NewObjectTreeCacheRef"></bibliomixed>

    <bibliomixed id="OPSI"></bibliomixed>

    <bibliomixed id="ViTablesRef"></bibliomixed>

  </bibliography>
</book>
<!-- Local Variables: -->
<!-- fill-column: 78 -->
<!-- indent-tabs-mode: nil -->
<!-- End: -->
